<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Samuel Huber">
<meta name="dcterms.date" content="2023-12-03">
<meta name="description" content="The information in this post is classified.">

<title>Blog - Classification</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-sidebar floating nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/hubersam/hubersam.github.io/" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Classification</li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Classification</h1>
                  <div>
        <div class="description">
          The information in this post is classified.
        </div>
      </div>
                </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Samuel Huber </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 3, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>In the last post, we discussed the basics of Linear Regression and showed an example of how it could be used to estimate a given value based on input parameters. In this section, we’ll talk about a similar concept known as <strong><em>Classification</em></strong>.</p>
<p>A model that is used for classification uses input features to predict a <em>class</em>. The class is a selection from a discrete set of classes. For example, a classification model could be trained for identifying whether an online store review is <em>positive</em> or <em>negative</em>. Another example is classifying a person’s facial expression as <em>happy</em>, <em>sad</em>, <em>bored</em>, <em>scared</em>, and so on. Classification can be used for many unique and useful purposes, and luckily, the metrics to determine the performance of a classification model are very simple.</p>
<p>However, there is one large difference between what we have seen so far for regression and what we will see for classification. In the last post, we looked at Linear Regression and Nonlinear Regression, and as it turns out, these are the main forms that are used. There are a few others (Ridge Regression, Lasso Regression, etc.), but they are very nuanced and have subtle differences. Classification, on the other hand, has many different algorithms that all have their own positives and negatives. Therefore, this post will look less at the mathematical theory and more at the applications and usage of each of the popular classification model types.</p>
<hr>
<section id="binary-classification" class="level1">
<h1>Binary Classification</h1>
<section id="the-dataset" class="level2">
<h2 class="anchored" data-anchor-id="the-dataset">The Dataset</h2>
<p>The simplest type of classification is, unsurprisingly, <strong><em>Binary Classification</em></strong>. However, as we will discover later, classification for multiple output classes is <em>very</em> similar to binary classification.</p>
<p>Before we get too deep into the theory though, let’s talk about our dataset. We will be using the “Breast Cancer Wisconsin” dataset, which contains measurements of breast cancer tumors and a classification based on if they’re malignant or benign. The data was obtained using digitized imagery from a process known as <em>fine needle aspiratation</em>. More details about the data collection can be found in the following paper:</p>
<blockquote class="blockquote">
<p>Street, W. Nick, William H. Wolberg, and Olvi L. Mangasarian. “Nuclear feature extraction for breast tumor diagnosis.” <em>Biomedical image processing and biomedical visualization.</em> Vol. 1905. SPIE, 1993.</p>
</blockquote>
<p>which can be accessed <a href="https://minds.wisconsin.edu/bitstream/handle/1793/59692/TR1131.pdf?sequence=1">here</a>.</p>
<p>Interestingly, the paper itself also contains a classification model that performs very well. What is even more impressive is that this was accomplished in 1992!</p>
<p>The dataset is obtained from <a href="https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data">Kaggle</a>, and contains information such as the average radius, area, smoothness, symmetry, and a few other unique values. The page also contains a link to the UCI Machine Learning Repository, which shows graphs of the performance of different classification models, all of which surpass <span class="math inline">\(90\%\)</span> accuracy.</p>
<p>Just like the previous et’s take a look at the data:</p>
<div class="cell" data-tags="[]" data-execution_count="185">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'cancer.csv'</span>, encoding<span class="op">=</span><span class="st">'windows-1252'</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="185">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">id</th>
<th data-quarto-table-cell-role="th">diagnosis</th>
<th data-quarto-table-cell-role="th">radius_mean</th>
<th data-quarto-table-cell-role="th">texture_mean</th>
<th data-quarto-table-cell-role="th">perimeter_mean</th>
<th data-quarto-table-cell-role="th">area_mean</th>
<th data-quarto-table-cell-role="th">smoothness_mean</th>
<th data-quarto-table-cell-role="th">compactness_mean</th>
<th data-quarto-table-cell-role="th">concavity_mean</th>
<th data-quarto-table-cell-role="th">concave points_mean</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">texture_worst</th>
<th data-quarto-table-cell-role="th">perimeter_worst</th>
<th data-quarto-table-cell-role="th">area_worst</th>
<th data-quarto-table-cell-role="th">smoothness_worst</th>
<th data-quarto-table-cell-role="th">compactness_worst</th>
<th data-quarto-table-cell-role="th">concavity_worst</th>
<th data-quarto-table-cell-role="th">concave points_worst</th>
<th data-quarto-table-cell-role="th">symmetry_worst</th>
<th data-quarto-table-cell-role="th">fractal_dimension_worst</th>
<th data-quarto-table-cell-role="th">Unnamed: 32</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>842302</td>
<td>M</td>
<td>17.99</td>
<td>10.38</td>
<td>122.80</td>
<td>1001.0</td>
<td>0.11840</td>
<td>0.27760</td>
<td>0.3001</td>
<td>0.14710</td>
<td>...</td>
<td>17.33</td>
<td>184.60</td>
<td>2019.0</td>
<td>0.1622</td>
<td>0.6656</td>
<td>0.7119</td>
<td>0.2654</td>
<td>0.4601</td>
<td>0.11890</td>
<td>NaN</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>842517</td>
<td>M</td>
<td>20.57</td>
<td>17.77</td>
<td>132.90</td>
<td>1326.0</td>
<td>0.08474</td>
<td>0.07864</td>
<td>0.0869</td>
<td>0.07017</td>
<td>...</td>
<td>23.41</td>
<td>158.80</td>
<td>1956.0</td>
<td>0.1238</td>
<td>0.1866</td>
<td>0.2416</td>
<td>0.1860</td>
<td>0.2750</td>
<td>0.08902</td>
<td>NaN</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>84300903</td>
<td>M</td>
<td>19.69</td>
<td>21.25</td>
<td>130.00</td>
<td>1203.0</td>
<td>0.10960</td>
<td>0.15990</td>
<td>0.1974</td>
<td>0.12790</td>
<td>...</td>
<td>25.53</td>
<td>152.50</td>
<td>1709.0</td>
<td>0.1444</td>
<td>0.4245</td>
<td>0.4504</td>
<td>0.2430</td>
<td>0.3613</td>
<td>0.08758</td>
<td>NaN</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>84348301</td>
<td>M</td>
<td>11.42</td>
<td>20.38</td>
<td>77.58</td>
<td>386.1</td>
<td>0.14250</td>
<td>0.28390</td>
<td>0.2414</td>
<td>0.10520</td>
<td>...</td>
<td>26.50</td>
<td>98.87</td>
<td>567.7</td>
<td>0.2098</td>
<td>0.8663</td>
<td>0.6869</td>
<td>0.2575</td>
<td>0.6638</td>
<td>0.17300</td>
<td>NaN</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>84358402</td>
<td>M</td>
<td>20.29</td>
<td>14.34</td>
<td>135.10</td>
<td>1297.0</td>
<td>0.10030</td>
<td>0.13280</td>
<td>0.1980</td>
<td>0.10430</td>
<td>...</td>
<td>16.67</td>
<td>152.20</td>
<td>1575.0</td>
<td>0.1374</td>
<td>0.2050</td>
<td>0.4000</td>
<td>0.1625</td>
<td>0.2364</td>
<td>0.07678</td>
<td>NaN</td>
</tr>
</tbody>
</table>

<p>5 rows × 33 columns</p>
</div>
</div>
</div>
<div class="cell" data-tags="[]" data-execution_count="186">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>df.info()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 569 entries, 0 to 568
Data columns (total 33 columns):
 #   Column                   Non-Null Count  Dtype  
---  ------                   --------------  -----  
 0   id                       569 non-null    int64  
 1   diagnosis                569 non-null    object 
 2   radius_mean              569 non-null    float64
 3   texture_mean             569 non-null    float64
 4   perimeter_mean           569 non-null    float64
 5   area_mean                569 non-null    float64
 6   smoothness_mean          569 non-null    float64
 7   compactness_mean         569 non-null    float64
 8   concavity_mean           569 non-null    float64
 9   concave points_mean      569 non-null    float64
 10  symmetry_mean            569 non-null    float64
 11  fractal_dimension_mean   569 non-null    float64
 12  radius_se                569 non-null    float64
 13  texture_se               569 non-null    float64
 14  perimeter_se             569 non-null    float64
 15  area_se                  569 non-null    float64
 16  smoothness_se            569 non-null    float64
 17  compactness_se           569 non-null    float64
 18  concavity_se             569 non-null    float64
 19  concave points_se        569 non-null    float64
 20  symmetry_se              569 non-null    float64
 21  fractal_dimension_se     569 non-null    float64
 22  radius_worst             569 non-null    float64
 23  texture_worst            569 non-null    float64
 24  perimeter_worst          569 non-null    float64
 25  area_worst               569 non-null    float64
 26  smoothness_worst         569 non-null    float64
 27  compactness_worst        569 non-null    float64
 28  concavity_worst          569 non-null    float64
 29  concave points_worst     569 non-null    float64
 30  symmetry_worst           569 non-null    float64
 31  fractal_dimension_worst  569 non-null    float64
 32  Unnamed: 32              0 non-null      float64
dtypes: float64(31), int64(1), object(1)
memory usage: 146.8+ KB</code></pre>
</div>
</div>
<div class="cell" data-tags="[]" data-execution_count="187">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>df.describe()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="187">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">id</th>
<th data-quarto-table-cell-role="th">radius_mean</th>
<th data-quarto-table-cell-role="th">texture_mean</th>
<th data-quarto-table-cell-role="th">perimeter_mean</th>
<th data-quarto-table-cell-role="th">area_mean</th>
<th data-quarto-table-cell-role="th">smoothness_mean</th>
<th data-quarto-table-cell-role="th">compactness_mean</th>
<th data-quarto-table-cell-role="th">concavity_mean</th>
<th data-quarto-table-cell-role="th">concave points_mean</th>
<th data-quarto-table-cell-role="th">symmetry_mean</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">texture_worst</th>
<th data-quarto-table-cell-role="th">perimeter_worst</th>
<th data-quarto-table-cell-role="th">area_worst</th>
<th data-quarto-table-cell-role="th">smoothness_worst</th>
<th data-quarto-table-cell-role="th">compactness_worst</th>
<th data-quarto-table-cell-role="th">concavity_worst</th>
<th data-quarto-table-cell-role="th">concave points_worst</th>
<th data-quarto-table-cell-role="th">symmetry_worst</th>
<th data-quarto-table-cell-role="th">fractal_dimension_worst</th>
<th data-quarto-table-cell-role="th">Unnamed: 32</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">count</td>
<td>5.690000e+02</td>
<td>569.000000</td>
<td>569.000000</td>
<td>569.000000</td>
<td>569.000000</td>
<td>569.000000</td>
<td>569.000000</td>
<td>569.000000</td>
<td>569.000000</td>
<td>569.000000</td>
<td>...</td>
<td>569.000000</td>
<td>569.000000</td>
<td>569.000000</td>
<td>569.000000</td>
<td>569.000000</td>
<td>569.000000</td>
<td>569.000000</td>
<td>569.000000</td>
<td>569.000000</td>
<td>0.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">mean</td>
<td>3.037183e+07</td>
<td>14.127292</td>
<td>19.289649</td>
<td>91.969033</td>
<td>654.889104</td>
<td>0.096360</td>
<td>0.104341</td>
<td>0.088799</td>
<td>0.048919</td>
<td>0.181162</td>
<td>...</td>
<td>25.677223</td>
<td>107.261213</td>
<td>880.583128</td>
<td>0.132369</td>
<td>0.254265</td>
<td>0.272188</td>
<td>0.114606</td>
<td>0.290076</td>
<td>0.083946</td>
<td>NaN</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">std</td>
<td>1.250206e+08</td>
<td>3.524049</td>
<td>4.301036</td>
<td>24.298981</td>
<td>351.914129</td>
<td>0.014064</td>
<td>0.052813</td>
<td>0.079720</td>
<td>0.038803</td>
<td>0.027414</td>
<td>...</td>
<td>6.146258</td>
<td>33.602542</td>
<td>569.356993</td>
<td>0.022832</td>
<td>0.157336</td>
<td>0.208624</td>
<td>0.065732</td>
<td>0.061867</td>
<td>0.018061</td>
<td>NaN</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">min</td>
<td>8.670000e+03</td>
<td>6.981000</td>
<td>9.710000</td>
<td>43.790000</td>
<td>143.500000</td>
<td>0.052630</td>
<td>0.019380</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.106000</td>
<td>...</td>
<td>12.020000</td>
<td>50.410000</td>
<td>185.200000</td>
<td>0.071170</td>
<td>0.027290</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.156500</td>
<td>0.055040</td>
<td>NaN</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">25%</td>
<td>8.692180e+05</td>
<td>11.700000</td>
<td>16.170000</td>
<td>75.170000</td>
<td>420.300000</td>
<td>0.086370</td>
<td>0.064920</td>
<td>0.029560</td>
<td>0.020310</td>
<td>0.161900</td>
<td>...</td>
<td>21.080000</td>
<td>84.110000</td>
<td>515.300000</td>
<td>0.116600</td>
<td>0.147200</td>
<td>0.114500</td>
<td>0.064930</td>
<td>0.250400</td>
<td>0.071460</td>
<td>NaN</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">50%</td>
<td>9.060240e+05</td>
<td>13.370000</td>
<td>18.840000</td>
<td>86.240000</td>
<td>551.100000</td>
<td>0.095870</td>
<td>0.092630</td>
<td>0.061540</td>
<td>0.033500</td>
<td>0.179200</td>
<td>...</td>
<td>25.410000</td>
<td>97.660000</td>
<td>686.500000</td>
<td>0.131300</td>
<td>0.211900</td>
<td>0.226700</td>
<td>0.099930</td>
<td>0.282200</td>
<td>0.080040</td>
<td>NaN</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">75%</td>
<td>8.813129e+06</td>
<td>15.780000</td>
<td>21.800000</td>
<td>104.100000</td>
<td>782.700000</td>
<td>0.105300</td>
<td>0.130400</td>
<td>0.130700</td>
<td>0.074000</td>
<td>0.195700</td>
<td>...</td>
<td>29.720000</td>
<td>125.400000</td>
<td>1084.000000</td>
<td>0.146000</td>
<td>0.339100</td>
<td>0.382900</td>
<td>0.161400</td>
<td>0.317900</td>
<td>0.092080</td>
<td>NaN</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">max</td>
<td>9.113205e+08</td>
<td>28.110000</td>
<td>39.280000</td>
<td>188.500000</td>
<td>2501.000000</td>
<td>0.163400</td>
<td>0.345400</td>
<td>0.426800</td>
<td>0.201200</td>
<td>0.304000</td>
<td>...</td>
<td>49.540000</td>
<td>251.200000</td>
<td>4254.000000</td>
<td>0.222600</td>
<td>1.058000</td>
<td>1.252000</td>
<td>0.291000</td>
<td>0.663800</td>
<td>0.207500</td>
<td>NaN</td>
</tr>
</tbody>
</table>

<p>8 rows × 32 columns</p>
</div>
</div>
</div>
<p>Now that we’ve seen some of the data, let’s move on to our first model.</p>
<hr>
</section>
<section id="logistic-regression" class="level2">
<h2 class="anchored" data-anchor-id="logistic-regression">Logistic Regression</h2>
<p>Logistic Regression has a very strong relationship to Linear Regression. In fact, it actually uses the Linear Regression equation in its own equation! To get a feel for how it works, let’s plot some data.</p>
<p>First, we have to convert the output labels into numbers so that we can actually plot them. We will assign a value of <code>1</code> to malignant tumors and <code>0</code> to benign tumors.</p>
<div class="cell" data-tags="[]" data-execution_count="188">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>arr <span class="op">=</span> [<span class="dv">1</span> <span class="cf">if</span> element <span class="op">==</span> <span class="st">"M"</span> <span class="cf">else</span> <span class="dv">0</span> <span class="cf">for</span> element <span class="kw">in</span> df[<span class="st">'diagnosis'</span>]]</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>df.insert(<span class="dv">33</span>, <span class="st">"diagnosis_number"</span>, arr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, let’s plot the diagnoses against the mean radius:</p>
<div class="cell" data-tags="[]" data-execution_count="190">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>scatter <span class="op">=</span> df.plot.scatter(x<span class="op">=</span><span class="st">'radius_mean'</span>, y<span class="op">=</span><span class="st">'diagnosis_number'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We can see that tumors below ~<span class="math inline">\(12\)</span> millimeters are guaranteed to be benign, while tumors above ~<span class="math inline">\(18\)</span> are guaranteed to be malignant. The gray area is in between these two numbers.</p>
<p>Now, imagine trying to fit a Linear Regression line to this. Of course, it doesn’t make much sense to do this, since our data clearly does not have a linear relationship. But just for fun, let’s imagine that we did. This line would clearly pass through both groupings of values, with each end of the line somewhere near the most dense areas of the data. However, since we’re not trying to make predictions on an output value but rather an output <em>class</em>, we need to modify this line to vary only between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. The best function to do this with is the <em>sigmoid</em> function:</p>
<p><span class="math display">\[f(x) = \frac{1}{1 + e^{-x}}\]</span></p>
<p>Since we’re modifying the line itself, our the variable in the exponential of our sigmoid is going to be replaced with the equation for the Linear Regression line. If you don’t recall, that was:</p>
<p><span class="math display">\[f(x) = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_3 + \ldots\]</span></p>
<p>Placing this into the exponential:</p>
<p><span class="math display">\[f(x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_3 + \ldots)}}\]</span></p>
<p>That’s it! It’s as simple as modifying a Linear Regression line. Of course, there is <em>a lot</em> more theory behind Logistic Regression, but this is essentially all it takes to understand what is happening.</p>
<p>Before we get into using it, though, we need to talk about one thing that is very important for classification. In order to measure how well our model is performing, we need to separate our data. The reason for this is that if you train you model on all of the data you have, then you can only test it on data that the model has already been trained on. This means you won’t be able to effectively measure how well your model generalizes to new data. To combot this, the dataset is almost always split into two different sets: the <strong><em>training set</em></strong>, and the <strong><em>test set</em></strong>. The most common ratio to use for split is <span class="math inline">\(80/20\)</span>, so we’ll try that out:</p>
<div class="cell" data-tags="[]" data-execution_count="211">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.asarray(df[<span class="st">'radius_mean'</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.asarray(df[<span class="st">'diagnosis_number'</span>])</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.20</span>, random_state<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> LogisticRegression(random_state<span class="op">=</span><span class="dv">15</span>).fit(X_train, y_train)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(clf.score(X_train, y_train))</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(clf.score(X_test, y_test))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.8879120879120879
0.8421052631578947</code></pre>
</div>
</div>
<p>Notice that we trained our model on the training set, and we get around <span class="math inline">\(88.8\%\)</span> accuracy when we compare it to the training set itself. However, when we compare it to the test set, we only get around <span class="math inline">\(84\%\)</span>. This is why it’s important to have different sets for training and testing. Either way, getting more than 4 out of 5 guesses right isn’t bad at all.</p>
<p>Now, let’s do a visualization of the Linear Regression line and Logistic Regression curve:</p>
<div class="cell" data-tags="[]" data-execution_count="222">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>b0 <span class="op">=</span> clf.intercept_[<span class="dv">0</span>]</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>b1 <span class="op">=</span> clf.coef_[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>plt.scatter(df[<span class="st">'radius_mean'</span>], df[<span class="st">'diagnosis_number'</span>], alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>xaxis <span class="op">=</span> np.linspace(<span class="fl">14.7</span>, <span class="fl">15.6</span>, <span class="dv">100</span>).reshape(<span class="dv">100</span>, <span class="dv">1</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>yaxis <span class="op">=</span> [(element <span class="op">*</span> b1) <span class="op">+</span> b0 <span class="cf">for</span> element <span class="kw">in</span> xaxis]</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>plt.plot(xaxis, yaxis, <span class="st">"red"</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>There’s the Linear Regression line. Let’s see what it looks like after transforming it:</p>
<div class="cell" data-tags="[]" data-execution_count="224">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>plt.scatter(df[<span class="st">'radius_mean'</span>], df[<span class="st">'diagnosis_number'</span>], alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>xaxis <span class="op">=</span> np.linspace(<span class="bu">min</span>(df[<span class="st">'radius_mean'</span>]), <span class="bu">max</span>(df[<span class="st">'radius_mean'</span>]), <span class="dv">100</span>).reshape(<span class="dv">100</span>, <span class="dv">1</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>yaxis <span class="op">=</span> [<span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span><span class="dv">1</span> <span class="op">*</span> ((element <span class="op">*</span> b1) <span class="op">+</span> b0))) <span class="cf">for</span> element <span class="kw">in</span> xaxis]</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>plt.plot(xaxis, yaxis, <span class="st">"green"</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-html/cell-10-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Very nice! We have a smooth variation from <span class="math inline">\(0\)</span> to <span class="math inline">\(1\)</span>, whereas the Linear Regression would continue forever in either direction.</p>
<p>Let’s try making a prediction!</p>
<div class="cell" data-tags="[]" data-execution_count="228">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>measurement_1 <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>measurement_2 <span class="op">=</span> <span class="dv">18</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>class1 <span class="op">=</span> clf.predict(np.array([measurement_1]).reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>))[<span class="dv">0</span>]</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>class2 <span class="op">=</span> clf.predict(np.array([measurement_2]).reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>))[<span class="dv">0</span>]</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"12 millimeters:"</span>, <span class="st">"Malignant"</span> <span class="cf">if</span> class1 <span class="op">==</span> <span class="dv">1</span> <span class="cf">else</span> <span class="st">"Benign"</span>)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"18 millimeters:"</span>, <span class="st">"Malignant"</span> <span class="cf">if</span> class2 <span class="op">==</span> <span class="dv">1</span> <span class="cf">else</span> <span class="st">"Benign"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>12 millimeters: Benign
18 millimeters: Malignant</code></pre>
</div>
</div>
<p>It’s definitely working. But that raises a question: “why not just use linear regression and have the decision be based on the midpoint of the line?” That is a very good question. You could, for example, find the <span class="math inline">\(x\)</span> value that corresponds to <span class="math inline">\(y = 0.5\)</span> on the line and classify any measurement that is greather than that <span class="math inline">\(x\)</span> value as a <span class="math inline">\(1\)</span>, and <span class="math inline">\(0\)</span> for any values less than that <span class="math inline">\(x\)</span> value. The thing this misses is the <em>probability</em>. Since Logistic Regression curves vary between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>, the probability of the measurement belonging to a class is given by the curve itself. Here’s an example:</p>
<div class="cell" data-tags="[]" data-execution_count="234">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Measurement 1:"</span>, measurement_1, <span class="st">"millimeters"</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>measurement1_prob <span class="op">=</span> clf.predict_proba(np.array([measurement_1]).reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>measurement1_class1_prob <span class="op">=</span> measurement1_prob[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>measurement1_class2_prob <span class="op">=</span> measurement1_prob[<span class="dv">0</span>][<span class="dv">1</span>]</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Probability of being benign:"</span>, <span class="bu">round</span>(measurement1_class1_prob <span class="op">*</span> <span class="dv">100</span>, <span class="dv">2</span>), <span class="st">"%"</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Probability of being malignant:"</span>, <span class="bu">round</span>(measurement1_class2_prob <span class="op">*</span> <span class="dv">100</span>, <span class="dv">2</span>), <span class="st">"%</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Measurement 2:"</span>, measurement_2, <span class="st">"millimeters"</span>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>measurement2_prob <span class="op">=</span> clf.predict_proba(np.array([measurement_2]).reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>measurement2_class1_prob <span class="op">=</span> measurement2_prob[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>measurement2_class2_prob <span class="op">=</span> measurement2_prob[<span class="dv">0</span>][<span class="dv">1</span>]</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Probability of being benign:"</span>, <span class="bu">round</span>(measurement2_class1_prob <span class="op">*</span> <span class="dv">100</span>, <span class="dv">2</span>), <span class="st">"%"</span>)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Probability of being malignant:"</span>, <span class="bu">round</span>(measurement2_class2_prob <span class="op">*</span> <span class="dv">100</span>, <span class="dv">2</span>), <span class="st">"%</span><span class="ch">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Measurement 1: 12 millimeters
Probability of being benign: 95.17 %
Probability of being malignant: 4.83 %

Measurement 2: 18 millimeters
Probability of being benign: 2.57 %
Probability of being malignant: 97.43 %
</code></pre>
</div>
</div>
<p>Super neat, right? Let’s try to find the point that is the hardest to classify:</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
\frac{1}{1 + e^{-(\beta_0 + \beta_1x)}} = 0.5 \longrightarrow 1 + e^{-(\beta_0 + \beta_1x)} = 2 \longrightarrow \ln(e^{-(\beta_0 + \beta_1x)}) &amp;= \ln(1)\\
-(\beta_0 + \beta_1x) &amp;= 0\\
x &amp;= -\frac{\beta_0}{\beta_1}
\end{split}
\end{equation}\]</span></p>
<div class="cell" data-tags="[]" data-execution_count="237">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>measurement_3 <span class="op">=</span> <span class="op">-</span>b0<span class="op">/</span>b1</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>measurement_3</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="237">
<pre><code>14.702869626376605</code></pre>
</div>
</div>
<p>Looking back up at the sigmoidal graph, this definitely seems to be close to the center point. Let’s check out the probabilities:</p>
<div class="cell" data-tags="[]" data-execution_count="239">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Measurement 3:"</span>, <span class="bu">round</span>(measurement_3, <span class="dv">2</span>), <span class="st">"millimeters"</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>measurement3_prob <span class="op">=</span> clf.predict_proba(np.array([measurement_3]).reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>measurement3_class1_prob <span class="op">=</span> measurement3_prob[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>measurement3_class2_prob <span class="op">=</span> measurement3_prob[<span class="dv">0</span>][<span class="dv">1</span>]</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Probability of being benign:"</span>, <span class="bu">round</span>(measurement3_class1_prob <span class="op">*</span> <span class="dv">100</span>, <span class="dv">2</span>), <span class="st">"%"</span>)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Probability of being malignant:"</span>, <span class="bu">round</span>(measurement3_class2_prob <span class="op">*</span> <span class="dv">100</span>, <span class="dv">2</span>), <span class="st">"%</span><span class="ch">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Measurement 3: 14.7 millimeters
Probability of being benign: 50.0 %
Probability of being malignant: 50.0 %
</code></pre>
</div>
</div>
<p>Sure enough, it’s a <span class="math inline">\(50/50\)</span> shot. Just for fun, let’s check the classification that <code>LogisticRegression</code> decides:</p>
<div class="cell" data-tags="[]" data-execution_count="245">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Malignant"</span> <span class="cf">if</span> clf.predict(np.array([measurement_3]).reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>))[<span class="dv">0</span>] <span class="op">==</span> <span class="dv">1</span> <span class="cf">else</span> <span class="st">"Benign"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Benign</code></pre>
</div>
</div>
<hr>
</section>
</section>
<section id="classification-metrics" class="level1">
<h1>Classification Metrics</h1>
<section id="confusion-matrix" class="level2">
<h2 class="anchored" data-anchor-id="confusion-matrix">Confusion Matrix</h2>
<p>Now, let’s move away from the guesswork and check what metrics we have to guage our model’s classification performance. If we had more insight than just raw percentages, we might be able to tune our model for our specific purpose.</p>
<p>We’re going to look at something called the confusion matrix:</p>
<div class="cell" data-tags="[]" data-execution_count="248">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_predict</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix, ConfusionMatrixDisplay</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>y_train_pred <span class="op">=</span> cross_val_predict(clf, X_train, y_train, cv<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>cm <span class="op">=</span> confusion_matrix(y_train, y_train_pred)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>plt.rc(<span class="st">'font'</span>, size<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>ConfusionMatrixDisplay.from_predictions(y_train, y_train_pred)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-html/cell-16-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The vertical axis represents actual benign or malignant classes, while the horizontal axis represents the classification that our model made. You can see that our model correctly classified <span class="math inline">\(263\)</span> benign tumors and <span class="math inline">\(141\)</span> malignant tumors, but accidentally flagged <span class="math inline">\(19\)</span> benign tumors as malignant, and let <span class="math inline">\(32\)</span> malignant tumors fly under the radar as benign.</p>
<p>The bottom right value represents <strong><em>True Positives (TP)</em></strong>, while the top left value represents <strong><em>True Negatives (TN)</em></strong>. Conversely, the top right value represents <strong><em>False Positives (FP)</em></strong> and the bottom left value represents <strong><em>False Negatives (FN)</em></strong>. Here’s another way to look at it:</p>
<p><span class="math display">\[ \begin{bmatrix}
\mathrm{True~Negatives} &amp; \mathrm{False~Positives}\\
\mathrm{False~Negatives} &amp; \mathrm{True~Positives}
\end{bmatrix} \]</span></p>
<p>This matrix gives us a good idea of what kind of mistakes our model is making. Now, let’s look at the two main numeric metrics that are used to rate the performance of our model.</p>
</section>
<section id="precision-and-recall" class="level2">
<h2 class="anchored" data-anchor-id="precision-and-recall">Precision and Recall</h2>
<p>We’ll start with <strong><em>Precision</em></strong> first. Precision is the ratio of true positives to total positive <em>guesses</em>. In other words, it is a metric that describes how many positive classifications that we were <em>actually</em> right about, relative to how many positives that we <em>said</em> there were. Mathematically:</p>
<p><span class="math display">\[\mathrm{Precision} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FP}}\]</span></p>
<p>We can calulate this manually using our confusion matrix:</p>
<div class="cell" data-tags="[]" data-execution_count="249">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>cm[<span class="dv">1</span>, <span class="dv">1</span>] <span class="op">/</span> (cm[<span class="dv">0</span>, <span class="dv">1</span>] <span class="op">+</span> cm[<span class="dv">1</span>, <span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="249">
<pre><code>0.88125</code></pre>
</div>
</div>
<p>Or take it from <code>scikit-learn</code> itself:</p>
<div class="cell" data-tags="[]" data-execution_count="251">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_score</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>precision_score(y_train, y_train_pred)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="251">
<pre><code>0.88125</code></pre>
</div>
</div>
<p>We can also just verify this mathematically:</p>
<p><span class="math display">\[\frac{141}{141 + 19} = \frac{141}{160} = 0.88125\]</span></p>
<p>This means that out of all tumors that our model <em>said</em> were malignant, roughly <span class="math inline">\(88\%\)</span> were <em>actually</em> malignant.</p>
<p>The other main metric is <strong><em>Recall</em></strong>. Recall is the ratio of true positives to <em>total positives</em>. In other words, it is a metric that describes how many positive classifications that we were <em>right</em> about, relative to how many positives there <em>actually</em> were. Mathematically:</p>
<p><span class="math display">\[\mathrm{Recall} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FN}}\]</span></p>
<p>We can calulate this manually using our confusion matrix:</p>
<div class="cell" data-tags="[]" data-execution_count="252">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>cm[<span class="dv">1</span>, <span class="dv">1</span>] <span class="op">/</span> (cm[<span class="dv">1</span>, <span class="dv">0</span>] <span class="op">+</span> cm[<span class="dv">1</span>, <span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="252">
<pre><code>0.815028901734104</code></pre>
</div>
</div>
<p>Or use <code>scikit-learn</code>’s built-in functions:</p>
<div class="cell" data-tags="[]" data-execution_count="254">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> recall_score</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>recall_score(y_train, y_train_pred)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="254">
<pre><code>0.815028901734104</code></pre>
</div>
</div>
<p>And of course we can verify this mathematically again:</p>
<p><span class="math display">\[\frac{141}{141 + 32} = \frac{141}{173} = 0.815028901734104\]</span></p>
<p>This tells us that our classifier only classified roughly <span class="math inline">\(81.5\%\)</span> of <strong><em>all</em></strong> malignant tumors correctly.</p>
<p>To see the difference between these two metrics, let’s imagine that these images of malignant tumors are known criminals, and we’re guards on the look out. Our precision and recall tell us that our classifier let nearly <span class="math inline">\(18.5\%\)</span> (<span class="math inline">\(1 - \mathrm{Recall} = 100\% - 81.5\%\)</span>) of all criminals sneak by, and out of all the people we stopped, around <span class="math inline">\(12\%\)</span> (<span class="math inline">\(1 - \mathrm{Precision} = 100\% - 88\%\)</span>) of them were innocent civilians!</p>
<p>As it turns out, for any model that has <em>some</em> error, the precision and recall values are inextricably linked. Looking back at our “civilian vs.&nbsp;criminal” example, let’s think about how we might try to classify civilians from criminals. Assuming we could see all the individuals at once, we may use some identifiable features or behaviors to set a “score” to each person. A person with a higher “score” is more likely to be a criminal, and a lower “score” means that person is probably a civilian. Once we’ve assigned each person a score, we need to set a threshold value above which we will assume all people are criminals, and below which we will assume all people are civilians.</p>
<p>Once we have a score for each person and all of those scores are placed in order along a horizontal number line, we can set a vertical dividing line with all scores on the right side containing scores belonging to criminals and the left side containing scores belonging to civilians. Now, assuming our model is imperfect, we will have some civilians on the right side of the line (false positives) and some criminals on the left side of the line (false negatives). If we shift our threshold to the right, this stops classifying so many civilians as criminals, but it also stops classifying so many <strong>criminals</strong> as criminals. In other words, the number of false positives will go down, but the number of false negatives will go up. Looking at our equations for precision and recall again:</p>
<p><span class="math display">\[\mathrm{Precision} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FP}}\]</span></p>
<p><span class="math display">\[\mathrm{Recall} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FN}}\]</span></p>
<p>We can see that the decrease of false positives will cause precision to increase, but the increase in false negatives will cause the recall to decrease. Theoretically, if we moved the threshold all the way to the right so that no one’s score is high enough to be classified as a criminal, we would theoretically have a precision of <span class="math inline">\(100\%\)</span>! This is great on paper, but it is only occurring because we’re not classying <strong>anyone</strong> as a criminal, so we have no room to fail. In other words, “you miss 100% of the shots you don’t take”.</p>
<p>Conversely, if we move the threshold to the left, our number of classified criminals will increase, but we will also be classifying more civilians as criminals. In other words, the number of false negatives will go down, but the number of false positives will go up. We can see from the equations above that this will result in the decrease of precision and the increase of recall. Again, if the threshold reaches the very edge, and we classify all people as criminals, then our recall looks good on paper (<span class="math inline">\(100\%\)</span>), but only because we’re accusing <em>everyone</em> of being a criminal. In other words, “you will hit 100% of your shots, if you take every shot possible”.</p>
<p>This conundrum is known as the <strong><em>Precision-Recall Tradeoff</em></strong>. We can visualize the changing of the precision and recall with the shifting of the threshold using a graph:</p>
<div class="cell" data-tags="[]" data-execution_count="267">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_recall_curve</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>threshold <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>y_scores <span class="op">=</span> cross_val_predict(clf, X_train, y_train, cv<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>                             method<span class="op">=</span><span class="st">"decision_function"</span>)</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>precisions, recalls, thresholds <span class="op">=</span> precision_recall_curve(y_train, y_scores)</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">4</span>))</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>plt.plot(thresholds, precisions[:<span class="op">-</span><span class="dv">1</span>], <span class="st">"b--"</span>, label<span class="op">=</span><span class="st">"Precision"</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>plt.plot(thresholds, recalls[:<span class="op">-</span><span class="dv">1</span>], <span class="st">"g-"</span>, label<span class="op">=</span><span class="st">"Recall"</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>plt.vlines(threshold, <span class="dv">0</span>, <span class="fl">1.0</span>, <span class="st">"k"</span>, <span class="st">"dotted"</span>, label<span class="op">=</span><span class="st">"threshold"</span>)</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>idx <span class="op">=</span> (thresholds <span class="op">&gt;=</span> threshold).argmax()  <span class="co"># first index ≥ threshold</span></span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>plt.plot(thresholds[idx], precisions[idx], <span class="st">"bo"</span>)</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>plt.plot(thresholds[idx], recalls[idx], <span class="st">"go"</span>)</span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>plt.axis([<span class="bu">min</span>(y_scores), <span class="bu">max</span>(y_scores), <span class="op">-</span><span class="fl">0.01</span>, <span class="fl">1.01</span>])</span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Threshold"</span>)</span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">"center right"</span>)</span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-html/cell-21-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>It is up to us as the model designers to create a model that has the properties we desire. In the case of breast cancer classification, is it better to have higher precision, higher recall, or a mix of both? Well, a false negative could be the difference between life and death, whereas a false positive is not as serious. Therefore, it is in our best interest to lower the amount of false negatives as much as possible, even if there’s an increase in false positives. This, in turn, will raise our recall and lower our precision. This means we need to lower our threshold.</p>
<p>Looking at the left side of the above figure, we can see that the recall settles at <span class="math inline">\(1\)</span> for quite a while, and the precision settles at roughly <span class="math inline">\(0.4\)</span>. We could choose a threshold as far to the left as possible, but we can see that shifting it to the right will slightly raise our precision without affecting our recall much. Let’s use a threshold of <span class="math inline">\(-5\)</span> and see what we get:</p>
<div class="cell" data-tags="[]" data-execution_count="274">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>threshold <span class="op">=</span> <span class="op">-</span><span class="dv">5</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>idx <span class="op">=</span> (thresholds <span class="op">&gt;=</span> threshold).argmax()</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Recall:"</span>, <span class="dv">100</span> <span class="op">*</span> recalls[idx], <span class="st">"%"</span>)</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Precision:"</span>, <span class="bu">round</span>(<span class="dv">100</span> <span class="op">*</span> precisions[idx], <span class="dv">2</span>), <span class="st">"%"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Recall: 100.0 %
Precision: 42.2 %</code></pre>
</div>
</div>
<p>Sure enough, we find every malignant tumor, but with the drawback that we also accidentally classify nearly <span class="math inline">\(60\%\)</span> of benign tumors as malignant as well.</p>
<p>When you think about it, this isn’t too bad. Usually, tumors of any size would be cause for concern, and most people would safely operate under the assumption that their tumor is malignant and get it removed as soon as possible. This is the same as shifting the threshold all the way to the left and classifying everyone as a criminal. In this situation, we can actually say that our model has improved things substantially by still catching all malignant tumors, but also allowing more than <span class="math inline">\(40\%\)</span> of people to not have to get an operation.</p>
<hr>
<p><em>This post will eventually be updated to include:</em></p>
<ul>
<li><em>Other metrics: <span class="math inline">\(F_1\)</span> Score, PR Curves, ROC curves, etc.</em></li>
<li><em>Multiclass Classification</em></li>
<li><em>Other classification methods: Naïve Bayes, K-Nearest Neighbors, Decision Trees, Support Vector Machines, etc.</em></li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>