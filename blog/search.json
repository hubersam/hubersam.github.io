[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Probability/index.html",
    "href": "posts/Probability/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "If you’re reading this, you have just begun your pocket-sized journey into machine learning topics. As with any field, you must start at the basics, and that is where you are now. Unfortunately, in this case, the basics aren’t so basic.\nFor most people (myself included), probability is one of the hardest concepts to grasp out of all of the mathematical disciplines. Most of the ideas are algebraically and computationally simple, but can be a conceptual nightmare. My only goal for this post is to form a basic layer of understanding with which you can use as a foundation for all of your future endeavours in machine learning.\nWe will start with basic probability.\n\n\nBasic Probability\nWe’ll start with some terminology.\n\nThe set of all possible outcomes is called the sample space, and is usually given the symbol \\(S\\).\nThe probability of a event, \\(A\\), occurring can be calculated by taking the ratio of the cardinalities of the event and the sample space, \\(P(A) = \\frac{\\left| A \\right|}{\\left| S \\right|}\\).\n\nThere were a lot of big words in there, so let’s take a look at an example. Imagine rolling a fair six-sided die. There are six equally likely possible outcomes: 1, 2, 3, 4, 5, 6. Therefore, since there are no other possible outcomes, then we have our sample space:\n\\[S = \\{ 1, 2, 3, 4, 5, 6 \\}\\]\nNow, let’s describe an “event”. We will call \\(A\\) the event where we roll an even number. Then we can write:\n\\[A = \\{ 2, 4, 6 \\}\\]\nNow look at the third rule again. The symbols and big words may look complicated, but “taking the ratio of the cardinalities of the event and the sample space” really just means counting up the number of elements in our event and dividing it by the total number of possible elements. In our case, there are three possible even numbers, and six possible total numbers, so:\n\\[P(A) = \\frac{\\left| A \\right|}{\\left| S \\right|} = \\frac{3}{6} = \\frac{1}{2} = 0.5 = 50\\%\\]\nThis should be making sense so far. There are a few other things we can deduce from this simple equation too. Let’s calculate the probability of the sample space:\n\\[P(S) = \\frac{\\left| S \\right|}{\\left| S \\right|} = \\frac{6}{6} = 1 = 100\\%\\]\nThis should also make sense. The probability of any of the possible values occurring should be guaranteed. What about a set with nothing in it? We’ll call that set \\(B\\):\n\\[B = \\{\\}\\]\nThen the probability of \\(B\\) occuring is:\n\\[P(B) = \\frac{\\left| B \\right|}{\\left| S \\right|} = \\frac{0}{6} = 0\\]\nWhich also makes sense. This gives us insight into the range of values that a probability should be able to take:\n\\[0 \\leq P(A) \\leq 1\\]\nThis goes for all scenarios; not just for the die example.\n\n\n\nProbability of This and That\nLet’s extend our example by imagining that we roll two dice, one by one. The first roll can take on any one of six values, and the second one can take on six different values for each of the six values that the first roll could have taken. This gives us \\(6 \\times 6 = 36\\) different possible outcomes. Let’s write our sample space like this:\n\\[S = \\{ (1, 1), (1, 2), (1, 3) \\ldots (6, 5), (6, 6) \\}\\]\nWhat is the probability of the first roll being a \\(2\\)? Let’s write the set for the event of all combinations of rolls where the first roll is a \\(2\\):\n\\[A = \\{ (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6) \\}\\]\nAnd finally:\n\\[P(A) = \\frac{\\left| A \\right|}{\\left| S \\right|} = \\frac{6}{36} = \\frac{1}{6} \\approx 16.67\\%\\]\nLet’s try another probability. This time we ask ourselves, “what’s the probability that our second roll is \\(5\\)”? Well, this set is very similar to the previous one:\n\\[B = \\{ (1, 5), (2, 5), (3, 5), (4, 5), (5, 5), (6, 5) \\}\\]\nAnd the probability is identical:\n\\[P(B) = \\frac{\\left| B \\right|}{\\left| S \\right|} = \\frac{6}{36} = \\frac{1}{6} \\approx 16.67\\%\\]\nLast example! What’s the probability that the first roll is \\(2\\), and that our second roll is \\(5\\)? Well, this set is pretty simple:\n\\[C = \\{ (2, 5) \\}\\]\nSo our probability is:\n\\[P(C) = \\frac{\\left| B \\right|}{\\left| S \\right|} = \\frac{1}{36} \\approx 2.78\\%\\]\nThis is correct, but we have a more powerful technique up our sleeve. In order to find it, let’s look at a visual representation of our example:\n\n\n\n\n\n\n\n\n\nThe red circle on the left represents all outcomes in the \\(A\\) event, and the green circle on the right represents all outcomes in the \\(B\\) event. Notice that the intersection of these two regions shows the outcome for event \\(C\\).\nThere are two main operations when it comes to sets that are very useful for determining more advanced probabilities.\n\nUnion\nIntersection\n\nThe union, given the symbol \\(\\cup\\), is defined as the set of all events that both \\(A\\) and \\(B\\) contain. In the case of our picture, this is what the union looks like:\n\n\n\n\n\n\n\n\n\nConversely, the intersection, given the symbol \\(\\cap\\), gives the set of outcomes that \\(A\\) and \\(B\\) share. Here is the diagram:\n\n\n\n\n\n\n\n\n\nYou can think of it this way:\n\n\\(A \\cup B = A~\\mathrm{or}~B\\)\n\\(A \\cap B = A~\\mathrm{and}~B\\)\n\nBut why is this useful? Well, there’s a really cool relationship that we can derive between the two of them just using the information so far. Here’s how we do it.\nImagine taking the individual probabilities of \\(A\\) and \\(B\\) and adding them together. In doing this, you’d have calculated the probability of the union of the two events, plus a little extra. This little extra comes from the fact that both sets contain their intersection, so you counted the intersection twice on accident. The first copy contributed to the union of the two sets, the second copy is the “little extra”. In other words:\n\\[P(A) + P(B) = P(A \\cup B) + P(A \\cap B)\\]\nWe can rearrange this equation to get our desired result:\n\\[P(A \\cap B) = P(A) + P(B) - P(A \\cup B)\\]\nLet’s use this equation to recalculate the probability of \\(C\\) again. That is, let’s calculate the probability that the first roll is \\(2\\) and that the second roll is \\(5\\).\nFirst, we need to find \\(A \\cup B\\). Remember, this is the same as asking for all outcomes where the first number is \\(2\\) or that the second number is \\(5\\). We can take all of the individual elements from \\(A\\) and be \\(B\\) independently (making sure not to double count any) and place them together:\n\\[A \\cup B = \\{ (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (1, 5), (3, 5), (4, 5), (5, 5), (6, 5) \\}\\]\nNotice that \\((2, 5)\\) appears in both sets, but we only count it once.\nLet’s get this probability:\n\\[P(A \\cup B) = \\frac{\\left| A \\cup B \\right|}{\\left| S \\right|} = \\frac{11}{36} \\approx 30.56\\%\\]\nNow we can get to calculating!\n\\[\\begin{equation}\n\\begin{split}\nP(C) = P(A \\cap B) &= P(A) + P(B) - P(A \\cup B)\\\\\n&= \\frac{1}{6} + \\frac{1}{6} - \\frac{11}{36}\\\\\n&= \\frac{1}{36} \\approx 2.78\\%\n\\end{split}\n\\end{equation}\\]\nExactly the same as last time!\nThis process may seem far more cumbersome than the original calculation, but as the sets of events start increasing in size, it is much easier to know how to calculate probabilities of intersections of events from other probabilities as opposed to actually counting it out.\n\n\n\nConditional Probability\nLet’s extend our example by imagining that we roll the same die five times. The die is still fair, and takes on one of six values every roll. We will then track the total sum of all the rolls. For example, if the values that appeared during the rolls were \\(1, 4, 3, 6, 2\\), the total would be \\(1 + 4 + 3 + 6 + 2 = 16\\).\nBefore we get started, let’s analyze what we know right away. We know that the total number of possible rolls is \\(6^5 = 7,776\\). We also know that the lowest sum possible is \\(1 + 1 + 1 + 1 + 1 = 5\\), and the highest is \\(6 + 6 + 6 + 6 + 6 = 30\\). These both occur only once, since they only have one set of numbers that can provide that sum. Conversely, other sums will occur far more often. For example, imagine a sum where all the rolls are different numbers: \\(1 + 2 + 3 + 4 + 5 = 15\\). Since these rolls could have happened in any order, and there are \\(5! = 5 \\times 4 \\times 3 \\times 2 \\times 1 = 120\\) different possible orderings, then the chances of getting a sum of \\(15\\) is much higher than getting a sum of \\(5\\) or \\(30\\).\nThis gives us insight that the distribution of outcomes is going to be most common near the average roll value (\\(\\frac{30 + 5}{2} = 17.5\\)), since there are far more ways of arranging the sums, than at the low or high extremes. Let’s look at the graph of the rolls:\n\n\nCode\nallRolls = []\n\ncurrRoll = [1, 1, 1, 1, 1]\nfor a in range(1, 7):\n    currRoll[0] = a\n    for b in range(1, 7):\n        currRoll[1] = b\n        for c in range(1, 7):\n            currRoll[2] = c\n            for d in range(1, 7):\n                currRoll[3] = d\n                for e in range(1, 7):\n                    currRoll[4] = e\n                    allRolls.append(currRoll.copy())\n                    \nallRollSums = [sum(roll) for roll in allRolls]\n\nsumValues = list(range(5, 30 + 1))\nsumCounts = [allRollSums.count(value) for value in sumValues]\n\nimport pandas as pd\nimport matplotlib.ticker as mtick\n\ndf = pd.DataFrame(sumCounts, sumValues)\n\ndf.plot.bar(rot=0, width=0.85, legend=False)\nplt.xlabel(\"Sum of Rolls\")\nplt.ylabel(\"Number of Occurrences\")\n\n\nText(0, 0.5, 'Number of Occurrences')\n\n\n\n\n\n\n\n\n\nJust as we predicted. This graph is a great example of a Gaussian Distribution. Let’s divide each of the individual “number of occurrences” values by the total number of possible rolls (\\(7,776\\)) to get the probability of getting each sum:\n\n\nCode\nnormalizedSumCounts = [count / sum(sumCounts) for count in sumCounts]\ndf = pd.DataFrame(normalizedSumCounts, sumValues)\nax = df.plot.bar(rot=0, width=0.85, legend=False)\nplt.xlabel(\"Sum of Rolls\")\nplt.ylabel(\"Chance of Occurrence\")\nax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n\n\n\n\n\n\n\n\n\nVery nice. We can see that getting a sum of around \\(17\\) or \\(18\\) will happen in about \\(1\\) in every \\(10\\) rolls, while getting a sum of \\(5\\) or \\(30\\) is not probable at all (\\(\\frac{1}{7776} \\approx 0.0129\\%\\)).\nWe can also find the probability of obtaining a range of values very easily. All we have to do is add the probability of obtaining each sum within the range. For example, the probability of getting a sum between \\(16\\) and \\(19\\) (inclusive) would be:\n\\[\\begin{equation}\n\\begin{split}\nP\\left(16 \\leq \\sum_{i=1}^{5}x_i \\leq 19\\right) &= P(16 \\leq x_1 + x_2 + x_3 + x_4 + x_5 \\leq 19)\\\\\n&= P(16) + P(17) + P(18) + P(19)\n\\end{split}\n\\end{equation}\\]\n\n\nCode\nsumRange = range(16, 19 + 1)\nsumRangeProbs = [normalizedSumCounts[sumValues.index(value)] for value in sumValues if value in sumRange]\nprint(\"{}%\".format(round(sum(sumRangeProbs * 100), 2)))\n\n\n38.97%\n\n\nNeat! This shows that more than a third of all of the sums of five rolls will be somewhere between \\(16\\) and \\(19\\).\nThis has led us to one of the most important parts of probability. Let’s ask a much more interesting and insightful question: what is the probability that the sum will be between \\(16\\) and \\(19\\) given that the first three rolls add up to \\(14\\)? At first glance, this question may seem to have the same answer as the previous question. After all, why should the knowledge of our first three rolls affect the outcome? Well, as it turns out, this new information essentially excludes sequences of rolls that we would have otherwise considered. To understand how to solve this, let’s look at a graphical representation again:\n\n\n\n\n\n\n\n\n\nThe probability for of an event occurring given another event is written as \\(P(\\mathrm{Event~1} | \\mathrm{Event~2})\\). For example, the probability of \\(A\\) given \\(B\\) is \\(P(A|B)\\).\nDetermining a relationship for this is not hard. We are essentially asking, “what is the probability of \\(A\\) being true, given that \\(B\\) is also true?” We know that a typical probability is the ratio of the number of occurrences of that event to the number of possible occurrences: \\(P(A) = \\frac{\\left| A \\right|}{\\left| S \\right|}\\). However, in this case, the event we’re concerned with is the intersection of two events (since both \\(A\\) and \\(B\\) are true), so the numerator should be the number of occurrences of the intersection, \\(\\left| A \\cap B \\right|\\). The other difference is, the number of possible occurrences is no longer the sample space. Instead, it is the number of occurrences of the second event, \\(\\left| B \\right|\\). Let’s visualize this:\n\n\n\n\n\n\n\n\n\nHere is a visual representation of the events where \\(B\\) is true, but \\(A\\) is not true. This can be written as \\(B \\cap \\neg A\\), where the \\(\\neg\\) symbol means “not”. Now, let’s reinclude the intersection of \\(A\\) and \\(B\\):\n\n\n\n\n\n\n\n\n\nNow we have visually represented the question, “what is the chance that \\(A\\) is true, given that \\(B\\) is also true?” We know that this probability should be the number of cases where both \\(A\\) and \\(B\\) are true, divided by all of the cases where \\(B\\) is true. Another way of thinking about this is that \\(B\\) has essentially become a “new sample space”, and \\(P(A \\cap B)\\) is what we’re trying to calculate. Visually:\n\n\n\n\n\n\n\n\n\nSo our equation is:\n\\[P(A|B) = \\frac{\\left| A \\cap B \\right|}{\\left|B \\right|}\\]\nLet’s do some algebraic magic:\n\\[P(A|B) = \\frac{\\left| A \\cap B \\right|}{\\left|B \\right|} \\times \\frac{\\frac{1}{\\left| S \\right|}}{\\frac{1}{\\left| S \\right|}} = \\frac{\\frac{\\left| A \\cap B \\right|}{\\left| S \\right|}}{\\frac{\\left| B \\right|}{\\left| S \\right|}} = \\frac{P(A \\cap B)}{P(B)}\\]\nSo now we can firmly state that the problem of \\(A\\) given \\(B\\) is equal to the probability of \\(A\\) and \\(B\\), divided by the probability of \\(B\\):\n\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\]\nLet’s apply this to the original question: “what is the probability that the sum will be between \\(16\\) and \\(19\\) given that the first three rolls add up to \\(14\\)?”:\n\\[P\\left(16 \\leq \\left.\\sum_{i=1}^{5}x_i \\leq 19 \\right| \\sum_{i=1}^{3}x_i = 14\\right) = \\frac{P\\left(16 \\leq \\sum\\limits_{i=1}^{5}x_i \\leq 19~\\mathrm{and}~\\sum\\limits_{i=1}^{3}x_i = 14\\right)}{P\\left(\\sum\\limits_{i=1}^{3}x_i = 14\\right)}\\]\n\n\n27.78%\n\n\nSo, as it turns out, if your first three rolls sum to a \\(14\\), you have slightly more than a one in four chance to have a total sum between \\(16\\) and \\(19\\). Just for fun, let’s plot the conditional probability of rolling between \\(16\\) and \\(19\\) given different sums for the first three rolls:\n\n\nCode\nfirstThreeSum = range(3, 18 + 1)\nchances = []\nfor i in firstThreeSum:\n    firstThreeRollSums = [sum(roll) for roll in allRolls if roll[0] + roll[1] + roll[2] == i]\n    firstThreeRollSumsInRange = [roll for roll in firstThreeRollSums if roll &gt;= 16 and roll &lt;= 19]\n    chances.append(len(firstThreeRollSumsInRange) / len(firstThreeRollSums))\n\ndf = pd.DataFrame(chances, firstThreeSum)\nax = df.plot.bar(rot=0, width=0.85, legend=False)\nplt.xlabel(\"Sum of First Three Rolls\")\nplt.ylabel(\"Chance of Total Sum Between 16 and 19\")\nax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n\n\n\n\n\n\n\n\n\nWe can see that if the first three rolls sum to \\(3\\), even the highest values for the other rolls can’t reach the range: \\(1 + 1 + 1 + 6 + 6 = 15 &lt; 16\\). This applies to \\(18\\) as well: \\(6 + 6 + 6 + 1 + 1 = 20 &gt; 19\\). As it turns out, the sums of the first three values that gives you the highest chances of rolling within the range are \\(10\\) and \\(11\\).\nFunnily enough, if your first three rolls sum to \\(9\\) or \\(12\\), your chances of landing in the range are exactly \\(50\\%\\). Another quirk is that if your first three rolls sum to \\(8\\) or \\(13\\), your chances of landing in the range is \\(38.89\\%\\), which is almost identical to the original chance of rolling within the range, \\(P(A)\\)!\n\n\n\nBayes’ Rule\nWe have now entered the final (and most important) section. All of the previous knowledge has been building up to this point.\n\nLet’s examine the conditional probability equation again:\n\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\]\nWe can rearrange it like so:\n\\[P(A \\cap B) = P(A|B)P(B)\\]\nAt this point, we can note that \\(P(A \\cap B) = P(B \\cap A)\\), since the intersection of the two sets remains the same regardless of order. Let’s examine the conditional probability using this order:\n\\[P(B \\cap A) = P(B|A)P(A)\\]\nSince \\(P(A \\cap B) = P(B \\cap A)\\), then the right sides of each of the equations should be equal to each other:\n\\[P(A|B)P(B) = P(B|A)P(A)\\]\nAnd we can rearrange once more to give:\n\\[P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\\]\nThis is known as Bayes’ Rule, and is one of the most important machine learning concepts. It shows that there is a fundamental relationship between conditional probabilities with different orderings. What’s amazing about this relationship is that no assumptions have been made; this is true for all probabilities.\nTo demonstrate this, we will use a real dataset. This dataset comes from U.S. census reports from 2011 to 2015, and gives information about household income along with location, state, zip code, etc. It was downloaded from Kaggle. Let’s visualize the first few data points:\n\ndf = pd.read_csv('kaggle_income.csv', encoding='windows-1252')\ndf.head()\n\n\n\n\n\n\n\n\nid\nState_Code\nState_Name\nState_ab\nCounty\nCity\nPlace\nType\nPrimary\nZip_Code\nArea_Code\nALand\nAWater\nLat\nLon\nMean\nMedian\nStdev\nsum_w\n\n\n\n\n0\n1011000\n1\nAlabama\nAL\nMobile County\nChickasaw\nChickasaw city\nCity\nplace\n36611\n251\n10894952\n909156\n30.771450\n-88.079697\n38773\n30506\n33101\n1638.260513\n\n\n1\n1011010\n1\nAlabama\nAL\nBarbour County\nLouisville\nClio city\nCity\nplace\n36048\n334\n26070325\n23254\n31.708516\n-85.611039\n37725\n19528\n43789\n258.017685\n\n\n2\n1011020\n1\nAlabama\nAL\nShelby County\nColumbiana\nColumbiana city\nCity\nplace\n35051\n205\n44835274\n261034\n33.191452\n-86.615618\n54606\n31930\n57348\n926.031000\n\n\n3\n1011030\n1\nAlabama\nAL\nMobile County\nSatsuma\nCreola city\nCity\nplace\n36572\n251\n36878729\n2374530\n30.874343\n-88.009442\n63919\n52814\n47707\n378.114619\n\n\n4\n1011040\n1\nAlabama\nAL\nMobile County\nDauphin Island\nDauphin Island\nTown\nplace\n36528\n251\n16204185\n413605152\n30.250913\n-88.171268\n77948\n67225\n54270\n282.320328\n\n\n\n\n\n\n\nThere’s quite a bit of data here to look at. Let’s see if we can plot this somehow:\n\ndf.plot(kind=\"scatter\", x=\"Lon\", y=\"Lat\", grid=True)\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.show()\n\n\n\n\n\n\n\n\nWe can clearly see that this dataset has data from Alaska, Hawaii, and even Puerto Rico. We can see all the different parameters and their data types here:\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 32526 entries, 0 to 32525\nData columns (total 19 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   id          32526 non-null  int64  \n 1   State_Code  32526 non-null  int64  \n 2   State_Name  32526 non-null  object \n 3   State_ab    32526 non-null  object \n 4   County      32526 non-null  object \n 5   City        32526 non-null  object \n 6   Place       32526 non-null  object \n 7   Type        32526 non-null  object \n 8   Primary     32526 non-null  object \n 9   Zip_Code    32526 non-null  int64  \n 10  Area_Code   32526 non-null  object \n 11  ALand       32526 non-null  int64  \n 12  AWater      32526 non-null  int64  \n 13  Lat         32526 non-null  float64\n 14  Lon         32526 non-null  float64\n 15  Mean        32526 non-null  int64  \n 16  Median      32526 non-null  int64  \n 17  Stdev       32526 non-null  int64  \n 18  sum_w       32526 non-null  float64\ndtypes: float64(3), int64(8), object(8)\nmemory usage: 4.7+ MB\n\n\nFor the object types, we can see what different values they can take like this:\n\ndf[\"State_Name\"].value_counts()\n\nState_Name\nCalifornia              3280\nTexas                   2300\nNew York                2160\nFlorida                 1661\nPennsylvania            1475\nIllinois                1431\nOhio                    1349\nMichigan                1174\nNorth Carolina           915\nNew Jersey               888\nGeorgia                  818\nVirginia                 785\nIndiana                  704\nMissouri                 700\nWashington               688\nWisconsin                674\nMassachusetts            670\nMinnesota                641\nArizona                  617\nTennessee                610\nMaryland                 586\nAlabama                  526\nColorado                 526\nLouisiana                511\nKentucky                 497\nOklahoma                 480\nSouth Carolina           459\nIowa                     454\nOregon                   387\nKansas                   381\nPuerto Rico              380\nConnecticut              355\nArkansas                 340\nMississippi              321\nNebraska                 275\nNevada                   265\nUtah                     261\nWest Virginia            248\nNew Mexico               240\nMaine                    157\nIdaho                    149\nMontana                  148\nHawaii                   138\nNew Hampshire            131\nSouth Dakota             128\nNorth Dakota             119\nRhode Island             109\nAlaska                   105\nDelaware                  88\nVermont                   83\nWyoming                   75\nDistrict of Columbia      64\nName: count, dtype: int64\n\n\nCalifornia has the highest number of data points (no surprise there), while D.C. has the fewest. Now that we’ve had a look around, let’s focus on Virginia:\n\nvirginia = df.copy()\nvirginia = virginia.drop([\"id\", \"State_Code\", \"Zip_Code\", \"State_ab\", \"Area_Code\", \"ALand\", \"AWater\"], axis=1)\nvirginia = virginia[virginia[\"State_Name\"] == \"Virginia\"]\nvirginia = virginia[virginia[\"Type\"] == \"Track\"]\nvirginia.describe()\n\n\n\n\n\n\n\n\nLat\nLon\nMean\nMedian\nStdev\nsum_w\n\n\n\n\ncount\n724.000000\n724.000000\n724.000000\n724.000000\n724.000000\n724.000000\n\n\nmean\n37.751752\n-77.849723\n79952.308011\n105208.250000\n51075.248619\n434.758474\n\n\nstd\n0.835956\n1.512712\n39089.677211\n95940.593647\n16825.048860\n358.204421\n\n\nmin\n36.581972\n-83.106093\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n36.983112\n-78.518816\n52039.500000\n41727.250000\n39309.500000\n201.326579\n\n\n50%\n37.519565\n-77.420987\n70964.500000\n63942.000000\n51219.500000\n327.972570\n\n\n75%\n38.736408\n-77.057345\n101269.000000\n119256.250000\n64484.250000\n511.870572\n\n\nmax\n39.345172\n-75.503313\n205835.000000\n300000.000000\n94395.000000\n2328.854201\n\n\n\n\n\n\n\nI took the liberty of removing some of the information that I’m not planning on using, such as the state abbreviations, zip codes, and others. I also narrowed the location types to “Track” since it is the most narrow result you can get in this dataset.\nWe can see some important values, like the maximum and minimum values for housing number and average household income. Let’s plot this to get a better look:\n\nvirginia.plot(kind=\"scatter\", x=\"Lon\", y=\"Lat\", grid=True, s=virginia[\"sum_w\"] / 15, c=\"Mean\", cmap=\"jet\", colorbar=True, figsize=(4.8 * (8 / 3), 4.8))\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.show()\n\n\n\n\nLooking at this graph, the number of houses is represented by the size of the data point, and the average household income is represented by the color. We can see that the average household income seems to increase the more Northeast that you go. Let’s plot all points that are above the average of the average household incomes, which is around \\(\\$80,000\\):\n\naboveMeans = virginia[virginia[\"Mean\"] &gt; virginia[\"Mean\"].mean()]\n\naboveMeans.plot(kind=\"scatter\", x=\"Lon\", y=\"Lat\", grid=True, s=aboveMeans[\"sum_w\"] / 15, c=\"Mean\", cmap=\"jet\", colorbar=True, figsize=(4.8 * (8 / 3), 4.8))\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.show()\n\n\n\n\nUnfortunately, this data set does not contain information about the population densities of these locations. However, a quick Google search shows that there are roughly \\(3\\) people per household in Virginia, so let’s estimate the population of Virginia:\n\nint(round(sum(virginia[\"sum_w\"] * 3), 0))\n\n944295\n\n\nThis is about \\(\\frac{1}{8}\\) of the actual population, but the probabilities that we will calculate using this will not be too far from the truth.\nLet’s do some calculations now. Let’s find the conditional probability of making more than the average household income given that you are living on the right side of the midline of Virginia:\n\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\]\n\nmidLong = (virginia[\"Lon\"].max() + virginia[\"Lon\"].min()) / 2\nmidLong\n\n-79.304703\n\n\n\nrightSideVirginia = virginia[virginia[\"Lon\"] &gt; midLong]\naboveMeanRightSideVirginia = rightSideVirginia[rightSideVirginia[\"Mean\"] &gt; rightSideVirginia[\"Mean\"].mean()]\nprobAGivenB = sum(aboveMeanRightSideVirginia[\"sum_w\"]) / sum(rightSideVirginia[\"sum_w\"])\n\nprint(\"{}%\".format(round(probAGivenB * 100, 2)))\n\n30.21%\n\n\nThis means that there are enough locations on the right side of Virginia making below average income to bring this probability down, even though the red-colored areas are making almost double the average household income.\nLet’s reverse our question now. What is the conditional probability that someone is living on the right side of the midline of Virginia given that their average household income is above the average? Let’s finally use Bayes’ Rule!\n\\[P(B|A) = \\frac{P(A|B)P(B)}{P(A)}\\]\n\nprobA = sum(aboveMeans[\"sum_w\"]) / sum(virginia[\"sum_w\"])\nprobB = sum(rightSideVirginia[\"sum_w\"]) / sum(virginia[\"sum_w\"])\nprobBGivenA = (probAGivenB * probB) / probA\n\nprint(\"{}%\".format(round(probBGivenA * 100, 2)))\n\n84.33%\n\n\nA much different answer came out. This shows that you’d have a much easier time guessing where someone lives based on their income, rather than guessing someone’s income based on where they live.\nLet’s verify our answer using regular conditional probability, just to be sure that Bayes didn’t lie to us:\n\\[P(B|A) = \\frac{P(B \\cap A)}{P(A)}\\]\n\notherProbBGivenA = sum(aboveMeanRightSideVirginia[\"sum_w\"]) / sum(aboveMeans[\"sum_w\"])\nprint(\"{}%\".format(round(otherProbBGivenA * 100, 2)))\n\n84.33%\n\n\nSure enough, Bayes was right!"
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Welcome to my fourth blog post!"
  },
  {
    "objectID": "posts/Anomalies/index.html",
    "href": "posts/Anomalies/index.html",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Welcome to my fifth blog post!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! My name is Samuel Huber. I am a graduate student attending Virginia Tech for my Master’s degree in Computer Science with a specificied track toward machine learning and artificial intelligence. I graduated summa cum laude with a Bachelor’s degree in Aerospace Engineering and I am interested in all things science and technology related.\nAs of now, this blog will contain five posts about different machine learning topics. These posts should be read in order, top to bottom, and are written as a sort of pocket-sized “learning journey” for introducing new topics to interested individuals (such as yourself). Hopefully these posts will aid in your understanding of machine learning, or at least pique your interest."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Samuel Huber’s Blog",
    "section": "",
    "text": "Probability Theory and Random Variables\n\n\n\n\n\nThings are about to get Bayesian…\n\n\n\n\n\n\nDec 3, 2023\n\n\nSamuel Huber\n\n\n\n\n\n\n  \n\n\n\n\nLinear and Nonlinear Regression\n\n\n\n\n\nRegressive: An insurance company you can’t rely on.\n\n\n\n\n\n\nDec 3, 2023\n\n\nSamuel Huber\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\nThe information in this post is classified.\n\n\n\n\n\n\nDec 3, 2023\n\n\nSamuel Huber\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\nYou might get a headache…\n\n\n\n\n\n\nDec 3, 2023\n\n\nSamuel Huber\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly Detection\n\n\n\n\n\nWhat’s an ‘omaly’, and why are we detecting it?\n\n\n\n\n\n\nDec 3, 2023\n\n\nSamuel Huber\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "Welcome to my third blog post!"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/Regression/index.html",
    "href": "posts/Regression/index.html",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "Welcome to my second blog post!"
  }
]