[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Probability/index.html",
    "href": "posts/Probability/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "If you’re reading this, you have just begun your pocket-sized journey into machine learning topics. As with any field, you must start at the basics, and that is where you are now. Unfortunately, in this case, the basics aren’t so basic.\nFor most people (myself included), probability is one of the hardest concepts to grasp out of all of the mathematical disciplines. Most of the ideas are algebraically and computationally simple, but can be a conceptual nightmare. My only goal for this post is to form a basic layer of understanding with which you can use as a foundation for all of your future endeavours in machine learning.\nWe will start with basic probability.\n\n\nBasic Probability\nWe’ll start with some terminology.\n\nThe set of all possible outcomes is called the sample space, and is usually given the symbol \\(S\\).\nThe probability of a event, \\(A\\), occurring can be calculated by taking the ratio of the cardinalities of the event and the sample space, \\(P(A) = \\frac{\\left| A \\right|}{\\left| S \\right|}\\).\n\nThere were a lot of big words in there, so let’s take a look at an example. Imagine rolling a fair six-sided die. There are six equally likely possible outcomes: 1, 2, 3, 4, 5, 6. Therefore, since there are no other possible outcomes, then we have our sample space:\n\\[S = \\{ 1, 2, 3, 4, 5, 6 \\}\\]\nNow, let’s describe an “event”. We will call \\(A\\) the event where we roll an even number. Then we can write:\n\\[A = \\{ 2, 4, 6 \\}\\]\nNow look at the third rule again. The symbols and big words may look complicated, but “taking the ratio of the cardinalities of the event and the sample space” really just means counting up the number of elements in our event and dividing it by the total number of possible elements. In our case, there are three possible even numbers, and six possible total numbers, so:\n\\[P(A) = \\frac{\\left| A \\right|}{\\left| S \\right|} = \\frac{3}{6} = \\frac{1}{2} = 0.5 = 50\\%\\]\nThis should be making sense so far. There are a few other things we can deduce from this simple equation too. Let’s calculate the probability of the sample space:\n\\[P(S) = \\frac{\\left| S \\right|}{\\left| S \\right|} = \\frac{6}{6} = 1 = 100\\%\\]\nThis should also make sense. The probability of any of the possible values occurring should be guaranteed. What about a set with nothing in it? We’ll call that set \\(B\\):\n\\[B = \\{\\}\\]\nThen the probability of \\(B\\) occuring is:\n\\[P(B) = \\frac{\\left| B \\right|}{\\left| S \\right|} = \\frac{0}{6} = 0\\]\nWhich also makes sense. This gives us insight into the range of values that a probability should be able to take:\n\\[0 \\leq P(A) \\leq 1\\]\nThis goes for all scenarios; not just for the die example.\n\n\n\nProbability of This and That\nLet’s extend our example by imagining that we roll two dice, one by one. The first roll can take on any one of six values, and the second one can take on six different values for each of the six values that the first roll could have taken. This gives us \\(6 \\times 6 = 36\\) different possible outcomes. Let’s write our sample space like this:\n\\[S = \\{ (1, 1), (1, 2), (1, 3) \\ldots (6, 5), (6, 6) \\}\\]\nWhat is the probability of the first roll being a \\(2\\)? Let’s write the set for the event of all combinations of rolls where the first roll is a \\(2\\):\n\\[A = \\{ (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6) \\}\\]\nAnd finally:\n\\[P(A) = \\frac{\\left| A \\right|}{\\left| S \\right|} = \\frac{6}{36} = \\frac{1}{6} \\approx 16.67\\%\\]\nLet’s try another probability. This time we ask ourselves, “what’s the probability that our second roll is \\(5\\)”? Well, this set is very similar to the previous one:\n\\[B = \\{ (1, 5), (2, 5), (3, 5), (4, 5), (5, 5), (6, 5) \\}\\]\nAnd the probability is identical:\n\\[P(B) = \\frac{\\left| B \\right|}{\\left| S \\right|} = \\frac{6}{36} = \\frac{1}{6} \\approx 16.67\\%\\]\nLast example! What’s the probability that the first roll is \\(2\\), and that our second roll is \\(5\\)? Well, this set is pretty simple:\n\\[C = \\{ (2, 5) \\}\\]\nSo our probability is:\n\\[P(C) = \\frac{\\left| B \\right|}{\\left| S \\right|} = \\frac{1}{36} \\approx 2.78\\%\\]\nThis is correct, but we have a more powerful technique up our sleeve. In order to find it, let’s look at a visual representation of our example:\n\n\n\n\n\n\n\n\n\nThe red circle on the left represents all outcomes in the \\(A\\) event, and the green circle on the right represents all outcomes in the \\(B\\) event. Notice that the intersection of these two regions shows the outcome for event \\(C\\).\nThere are two main operations when it comes to sets that are very useful for determining more advanced probabilities.\n\nUnion\nIntersection\n\nThe union, given the symbol \\(\\cup\\), is defined as the set of all events that both \\(A\\) and \\(B\\) contain. In the case of our picture, this is what the union looks like:\n\n\n\n\n\n\n\n\n\nConversely, the intersection, given the symbol \\(\\cap\\), gives the set of outcomes that \\(A\\) and \\(B\\) share. Here is the diagram:\n\n\n\n\n\n\n\n\n\nYou can think of it this way:\n\n\\(A \\cup B = A~\\mathrm{or}~B\\)\n\\(A \\cap B = A~\\mathrm{and}~B\\)\n\nBut why is this useful? Well, there’s a really cool relationship that we can derive between the two of them just using the information so far. Here’s how we do it.\nImagine taking the individual probabilities of \\(A\\) and \\(B\\) and adding them together. In doing this, you’d have calculated the probability of the union of the two events, plus a little extra. This little extra comes from the fact that both sets contain their intersection, so you counted the intersection twice on accident. The first copy contributed to the union of the two sets, the second copy is the “little extra”. In other words:\n\\[P(A) + P(B) = P(A \\cup B) + P(A \\cap B)\\]\nWe can rearrange this equation to get our desired result:\n\\[P(A \\cap B) = P(A) + P(B) - P(A \\cup B)\\]\nLet’s use this equation to recalculate the probability of \\(C\\) again. That is, let’s calculate the probability that the first roll is \\(2\\) and that the second roll is \\(5\\).\nFirst, we need to find \\(A \\cup B\\). Remember, this is the same as asking for all outcomes where the first number is \\(2\\) or that the second number is \\(5\\). We can take all of the individual elements from \\(A\\) and be \\(B\\) independently (making sure not to double count any) and place them together:\n\\[A \\cup B = \\{ (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (1, 5), (3, 5), (4, 5), (5, 5), (6, 5) \\}\\]\nNotice that \\((2, 5)\\) appears in both sets, but we only count it once.\nLet’s get this probability:\n\\[P(A \\cup B) = \\frac{\\left| A \\cup B \\right|}{\\left| S \\right|} = \\frac{11}{36} \\approx 30.56\\%\\]\nNow we can get to calculating!\n\\[\\begin{equation}\n\\begin{split}\nP(C) = P(A \\cap B) &= P(A) + P(B) - P(A \\cup B)\\\\\n&= \\frac{1}{6} + \\frac{1}{6} - \\frac{11}{36}\\\\\n&= \\frac{1}{36} \\approx 2.78\\%\n\\end{split}\n\\end{equation}\\]\nExactly the same as last time!\nThis process may seem far more cumbersome than the original calculation, but as the sets of events start increasing in size, it is much easier to know how to calculate probabilities of intersections of events from other probabilities as opposed to actually counting it out.\n\n\n\nConditional Probability\nLet’s extend our example by imagining that we roll the same die five times. The die is still fair, and takes on one of six values every roll. We will then track the total sum of all the rolls. For example, if the values that appeared during the rolls were \\(1, 4, 3, 6, 2\\), the total would be \\(1 + 4 + 3 + 6 + 2 = 16\\).\nBefore we get started, let’s analyze what we know right away. We know that the total number of possible rolls is \\(6^5 = 7,776\\). We also know that the lowest sum possible is \\(1 + 1 + 1 + 1 + 1 = 5\\), and the highest is \\(6 + 6 + 6 + 6 + 6 = 30\\). These both occur only once, since they only have one set of numbers that can provide that sum. Conversely, other sums will occur far more often. For example, imagine a sum where all the rolls are different numbers: \\(1 + 2 + 3 + 4 + 5 = 15\\). Since these rolls could have happened in any order, and there are \\(5! = 5 \\times 4 \\times 3 \\times 2 \\times 1 = 120\\) different possible orderings, then the chances of getting a sum of \\(15\\) is much higher than getting a sum of \\(5\\) or \\(30\\).\nThis gives us insight that the distribution of outcomes is going to be most common near the average roll value (\\(\\frac{30 + 5}{2} = 17.5\\)), since there are far more ways of arranging the sums, than at the low or high extremes. Let’s look at the graph of the rolls:\n\n\nCode\nallRolls = []\n\ncurrRoll = [1, 1, 1, 1, 1]\nfor a in range(1, 7):\n    currRoll[0] = a\n    for b in range(1, 7):\n        currRoll[1] = b\n        for c in range(1, 7):\n            currRoll[2] = c\n            for d in range(1, 7):\n                currRoll[3] = d\n                for e in range(1, 7):\n                    currRoll[4] = e\n                    allRolls.append(currRoll.copy())\n                    \nallRollSums = [sum(roll) for roll in allRolls]\n\nsumValues = list(range(5, 30 + 1))\nsumCounts = [allRollSums.count(value) for value in sumValues]\n\nimport pandas as pd\nimport matplotlib.ticker as mtick\n\ndf = pd.DataFrame(sumCounts, sumValues)\n\ndf.plot.bar(rot=0, width=0.85, legend=False)\nplt.xlabel(\"Sum of Rolls\")\nplt.ylabel(\"Number of Occurrences\")\n\n\nText(0, 0.5, 'Number of Occurrences')\n\n\n\n\n\n\n\n\n\nJust as we predicted. This graph is a great example of a Gaussian Distribution. Let’s divide each of the individual “number of occurrences” values by the total number of possible rolls (\\(7,776\\)) to get the probability of getting each sum:\n\n\nCode\nnormalizedSumCounts = [count / sum(sumCounts) for count in sumCounts]\ndf = pd.DataFrame(normalizedSumCounts, sumValues)\nax = df.plot.bar(rot=0, width=0.85, legend=False)\nplt.xlabel(\"Sum of Rolls\")\nplt.ylabel(\"Chance of Occurrence\")\nax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n\n\n\n\n\n\n\n\n\nVery nice. We can see that getting a sum of around \\(17\\) or \\(18\\) will happen in about \\(1\\) in every \\(10\\) rolls, while getting a sum of \\(5\\) or \\(30\\) is not probable at all (\\(\\frac{1}{7776} \\approx 0.0129\\%\\)).\nWe can also find the probability of obtaining a range of values very easily. All we have to do is add the probability of obtaining each sum within the range. For example, the probability of getting a sum between \\(16\\) and \\(19\\) (inclusive) would be:\n\\[\\begin{equation}\n\\begin{split}\nP\\left(16 \\leq \\sum_{i=1}^{5}x_i \\leq 19\\right) &= P(16 \\leq x_1 + x_2 + x_3 + x_4 + x_5 \\leq 19)\\\\\n&= P(16) + P(17) + P(18) + P(19)\n\\end{split}\n\\end{equation}\\]\n\n\nCode\nsumRange = range(16, 19 + 1)\nsumRangeProbs = [normalizedSumCounts[sumValues.index(value)] for value in sumValues if value in sumRange]\nprint(\"{}%\".format(round(sum(sumRangeProbs * 100), 2)))\n\n\n38.97%\n\n\nNeat! This shows that more than a third of all of the sums of five rolls will be somewhere between \\(16\\) and \\(19\\).\nThis has led us to one of the most important parts of probability. Let’s ask a much more interesting and insightful question: what is the probability that the sum will be between \\(16\\) and \\(19\\) given that the first three rolls add up to \\(14\\)? At first glance, this question may seem to have the same answer as the previous question. After all, why should the knowledge of our first three rolls affect the outcome? Well, as it turns out, this new information essentially excludes sequences of rolls that we would have otherwise considered. To understand how to solve this, let’s look at a graphical representation again:\n\n\n\n\n\n\n\n\n\nThe probability for of an event occurring given another event is written as \\(P(\\mathrm{Event~1} | \\mathrm{Event~2})\\). For example, the probability of \\(A\\) given \\(B\\) is \\(P(A|B)\\).\nDetermining a relationship for this is not hard. We are essentially asking, “what is the probability of \\(A\\) being true, given that \\(B\\) is also true?” We know that a typical probability is the ratio of the number of occurrences of that event to the number of possible occurrences: \\(P(A) = \\frac{\\left| A \\right|}{\\left| S \\right|}\\). However, in this case, the event we’re concerned with is the intersection of two events (since both \\(A\\) and \\(B\\) are true), so the numerator should be the number of occurrences of the intersection, \\(\\left| A \\cap B \\right|\\). The other difference is, the number of possible occurrences is no longer the sample space. Instead, it is the number of occurrences of the second event, \\(\\left| B \\right|\\). Let’s visualize this:\n\n\n\n\n\n\n\n\n\nHere is a visual representation of the events where \\(B\\) is true, but \\(A\\) is not true. This can be written as \\(B \\cap \\neg A\\), where the \\(\\neg\\) symbol means “not”. Now, let’s reinclude the intersection of \\(A\\) and \\(B\\):\n\n\n\n\n\n\n\n\n\nNow we have visually represented the question, “what is the chance that \\(A\\) is true, given that \\(B\\) is also true?” We know that this probability should be the number of cases where both \\(A\\) and \\(B\\) are true, divided by all of the cases where \\(B\\) is true. Another way of thinking about this is that \\(B\\) has essentially become a “new sample space”, and \\(P(A \\cap B)\\) is what we’re trying to calculate. Visually:\n\n\n\n\n\n\n\n\n\nSo our equation is:\n\\[P(A|B) = \\frac{\\left| A \\cap B \\right|}{\\left|B \\right|}\\]\nLet’s do some algebraic magic:\n\\[P(A|B) = \\frac{\\left| A \\cap B \\right|}{\\left|B \\right|} \\times \\frac{\\frac{1}{\\left| S \\right|}}{\\frac{1}{\\left| S \\right|}} = \\frac{\\frac{\\left| A \\cap B \\right|}{\\left| S \\right|}}{\\frac{\\left| B \\right|}{\\left| S \\right|}} = \\frac{P(A \\cap B)}{P(B)}\\]\nSo now we can firmly state that the problem of \\(A\\) given \\(B\\) is equal to the probability of \\(A\\) and \\(B\\), divided by the probability of \\(B\\):\n\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\]\nLet’s apply this to the original question: “what is the probability that the sum will be between \\(16\\) and \\(19\\) given that the first three rolls add up to \\(14\\)?”:\n\\[P\\left(16 \\leq \\left.\\sum_{i=1}^{5}x_i \\leq 19 \\right| \\sum_{i=1}^{3}x_i = 14\\right) = \\frac{P\\left(16 \\leq \\sum\\limits_{i=1}^{5}x_i \\leq 19~\\mathrm{and}~\\sum\\limits_{i=1}^{3}x_i = 14\\right)}{P\\left(\\sum\\limits_{i=1}^{3}x_i = 14\\right)}\\]\n\n\n27.78%\n\n\nSo, as it turns out, if your first three rolls sum to a \\(14\\), you have slightly more than a one in four chance to have a total sum between \\(16\\) and \\(19\\). Just for fun, let’s plot the conditional probability of rolling between \\(16\\) and \\(19\\) given different sums for the first three rolls:\n\n\nCode\nfirstThreeSum = range(3, 18 + 1)\nchances = []\nfor i in firstThreeSum:\n    firstThreeRollSums = [sum(roll) for roll in allRolls if roll[0] + roll[1] + roll[2] == i]\n    firstThreeRollSumsInRange = [roll for roll in firstThreeRollSums if roll &gt;= 16 and roll &lt;= 19]\n    chances.append(len(firstThreeRollSumsInRange) / len(firstThreeRollSums))\n\ndf = pd.DataFrame(chances, firstThreeSum)\nax = df.plot.bar(rot=0, width=0.85, legend=False)\nplt.xlabel(\"Sum of First Three Rolls\")\nplt.ylabel(\"Chance of Total Sum Between 16 and 19\")\nax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n\n\n\n\n\n\n\n\n\nWe can see that if the first three rolls sum to \\(3\\), even the highest values for the other rolls can’t reach the range: \\(1 + 1 + 1 + 6 + 6 = 15 &lt; 16\\). This applies to \\(18\\) as well: \\(6 + 6 + 6 + 1 + 1 = 20 &gt; 19\\). As it turns out, the sums of the first three values that gives you the highest chances of rolling within the range are \\(10\\) and \\(11\\).\nFunnily enough, if your first three rolls sum to \\(9\\) or \\(12\\), your chances of landing in the range are exactly \\(50\\%\\). Another quirk is that if your first three rolls sum to \\(8\\) or \\(13\\), your chances of landing in the range is \\(38.89\\%\\), which is almost identical to the original chance of rolling within the range, \\(P(A)\\)!\n\n\n\nBayes’ Rule\nWe have now entered the final (and most important) section. All of the previous knowledge has been building up to this point.\n\nLet’s examine the conditional probability equation again:\n\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\]\nWe can rearrange it like so:\n\\[P(A \\cap B) = P(A|B)P(B)\\]\nAt this point, we can note that \\(P(A \\cap B) = P(B \\cap A)\\), since the intersection of the two sets remains the same regardless of order. Let’s examine the conditional probability using this order:\n\\[P(B \\cap A) = P(B|A)P(A)\\]\nSince \\(P(A \\cap B) = P(B \\cap A)\\), then the right sides of each of the equations should be equal to each other:\n\\[P(A|B)P(B) = P(B|A)P(A)\\]\nAnd we can rearrange once more to give:\n\\[P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\\]\nThis is known as Bayes’ Rule, and is one of the most important machine learning concepts. It shows that there is a fundamental relationship between conditional probabilities with different orderings. What’s amazing about this relationship is that no assumptions have been made; this is true for all probabilities.\nTo demonstrate this, we will use a real dataset. This dataset comes from U.S. census reports from 2011 to 2015, and gives information about household income along with location, state, zip code, etc. It was downloaded from Kaggle. Let’s visualize the first few data points:\n\ndf = pd.read_csv('kaggle_income.csv', encoding='windows-1252')\ndf.head()\n\n\n\n\n\n\n\n\nid\nState_Code\nState_Name\nState_ab\nCounty\nCity\nPlace\nType\nPrimary\nZip_Code\nArea_Code\nALand\nAWater\nLat\nLon\nMean\nMedian\nStdev\nsum_w\n\n\n\n\n0\n1011000\n1\nAlabama\nAL\nMobile County\nChickasaw\nChickasaw city\nCity\nplace\n36611\n251\n10894952\n909156\n30.771450\n-88.079697\n38773\n30506\n33101\n1638.260513\n\n\n1\n1011010\n1\nAlabama\nAL\nBarbour County\nLouisville\nClio city\nCity\nplace\n36048\n334\n26070325\n23254\n31.708516\n-85.611039\n37725\n19528\n43789\n258.017685\n\n\n2\n1011020\n1\nAlabama\nAL\nShelby County\nColumbiana\nColumbiana city\nCity\nplace\n35051\n205\n44835274\n261034\n33.191452\n-86.615618\n54606\n31930\n57348\n926.031000\n\n\n3\n1011030\n1\nAlabama\nAL\nMobile County\nSatsuma\nCreola city\nCity\nplace\n36572\n251\n36878729\n2374530\n30.874343\n-88.009442\n63919\n52814\n47707\n378.114619\n\n\n4\n1011040\n1\nAlabama\nAL\nMobile County\nDauphin Island\nDauphin Island\nTown\nplace\n36528\n251\n16204185\n413605152\n30.250913\n-88.171268\n77948\n67225\n54270\n282.320328\n\n\n\n\n\n\n\nThere’s quite a bit of data here to look at. Let’s see if we can plot this somehow:\n\ndf.plot(kind=\"scatter\", x=\"Lon\", y=\"Lat\", grid=True)\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.show()\n\n\n\n\n\n\n\n\nWe can clearly see that this dataset has data from Alaska, Hawaii, and even Puerto Rico. We can see all the different parameters and their data types here:\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 32526 entries, 0 to 32525\nData columns (total 19 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   id          32526 non-null  int64  \n 1   State_Code  32526 non-null  int64  \n 2   State_Name  32526 non-null  object \n 3   State_ab    32526 non-null  object \n 4   County      32526 non-null  object \n 5   City        32526 non-null  object \n 6   Place       32526 non-null  object \n 7   Type        32526 non-null  object \n 8   Primary     32526 non-null  object \n 9   Zip_Code    32526 non-null  int64  \n 10  Area_Code   32526 non-null  object \n 11  ALand       32526 non-null  int64  \n 12  AWater      32526 non-null  int64  \n 13  Lat         32526 non-null  float64\n 14  Lon         32526 non-null  float64\n 15  Mean        32526 non-null  int64  \n 16  Median      32526 non-null  int64  \n 17  Stdev       32526 non-null  int64  \n 18  sum_w       32526 non-null  float64\ndtypes: float64(3), int64(8), object(8)\nmemory usage: 4.7+ MB\n\n\nFor the object types, we can see what different values they can take like this:\n\ndf[\"State_Name\"].value_counts()\n\nState_Name\nCalifornia              3280\nTexas                   2300\nNew York                2160\nFlorida                 1661\nPennsylvania            1475\nIllinois                1431\nOhio                    1349\nMichigan                1174\nNorth Carolina           915\nNew Jersey               888\nGeorgia                  818\nVirginia                 785\nIndiana                  704\nMissouri                 700\nWashington               688\nWisconsin                674\nMassachusetts            670\nMinnesota                641\nArizona                  617\nTennessee                610\nMaryland                 586\nAlabama                  526\nColorado                 526\nLouisiana                511\nKentucky                 497\nOklahoma                 480\nSouth Carolina           459\nIowa                     454\nOregon                   387\nKansas                   381\nPuerto Rico              380\nConnecticut              355\nArkansas                 340\nMississippi              321\nNebraska                 275\nNevada                   265\nUtah                     261\nWest Virginia            248\nNew Mexico               240\nMaine                    157\nIdaho                    149\nMontana                  148\nHawaii                   138\nNew Hampshire            131\nSouth Dakota             128\nNorth Dakota             119\nRhode Island             109\nAlaska                   105\nDelaware                  88\nVermont                   83\nWyoming                   75\nDistrict of Columbia      64\nName: count, dtype: int64\n\n\nCalifornia has the highest number of data points (no surprise there), while D.C. has the fewest. Now that we’ve had a look around, let’s focus on Virginia:\n\nvirginia = df.copy()\nvirginia = virginia.drop([\"id\", \"State_Code\", \"Zip_Code\", \"State_ab\", \"Area_Code\", \"ALand\", \"AWater\"], axis=1)\nvirginia = virginia[virginia[\"State_Name\"] == \"Virginia\"]\nvirginia = virginia[virginia[\"Type\"] == \"Track\"]\nvirginia.describe()\n\n\n\n\n\n\n\n\nLat\nLon\nMean\nMedian\nStdev\nsum_w\n\n\n\n\ncount\n724.000000\n724.000000\n724.000000\n724.000000\n724.000000\n724.000000\n\n\nmean\n37.751752\n-77.849723\n79952.308011\n105208.250000\n51075.248619\n434.758474\n\n\nstd\n0.835956\n1.512712\n39089.677211\n95940.593647\n16825.048860\n358.204421\n\n\nmin\n36.581972\n-83.106093\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n36.983112\n-78.518816\n52039.500000\n41727.250000\n39309.500000\n201.326579\n\n\n50%\n37.519565\n-77.420987\n70964.500000\n63942.000000\n51219.500000\n327.972570\n\n\n75%\n38.736408\n-77.057345\n101269.000000\n119256.250000\n64484.250000\n511.870572\n\n\nmax\n39.345172\n-75.503313\n205835.000000\n300000.000000\n94395.000000\n2328.854201\n\n\n\n\n\n\n\nI took the liberty of removing some of the information that I’m not planning on using, such as the state abbreviations, zip codes, and others. I also narrowed the location types to “Track” since it is the most narrow result you can get in this dataset.\nWe can see some important values, like the maximum and minimum values for housing number and average household income. Let’s plot this to get a better look:\n\nvirginia.plot(kind=\"scatter\", x=\"Lon\", y=\"Lat\", grid=True, s=virginia[\"sum_w\"] / 15, c=\"Mean\", cmap=\"jet\", colorbar=True, figsize=(4.8 * (8 / 3), 4.8))\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.show()\n\n\n\n\nLooking at this graph, the number of houses is represented by the size of the data point, and the average household income is represented by the color. We can see that the average household income seems to increase the more Northeast that you go. Let’s plot all points that are above the average of the average household incomes, which is around \\(\\$80,000\\):\n\naboveMeans = virginia[virginia[\"Mean\"] &gt; virginia[\"Mean\"].mean()]\n\naboveMeans.plot(kind=\"scatter\", x=\"Lon\", y=\"Lat\", grid=True, s=aboveMeans[\"sum_w\"] / 15, c=\"Mean\", cmap=\"jet\", colorbar=True, figsize=(4.8 * (8 / 3), 4.8))\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.show()\n\n\n\n\nUnfortunately, this data set does not contain information about the population densities of these locations. However, a quick Google search shows that there are roughly \\(3\\) people per household in Virginia, so let’s estimate the population of Virginia:\n\nint(round(sum(virginia[\"sum_w\"] * 3), 0))\n\n944295\n\n\nThis is about \\(\\frac{1}{8}\\) of the actual population, but the probabilities that we will calculate using this will not be too far from the truth.\nLet’s do some calculations now. Let’s find the conditional probability of making more than the average household income given that you are living on the right side of the midline of Virginia:\n\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\]\n\nmidLong = (virginia[\"Lon\"].max() + virginia[\"Lon\"].min()) / 2\nmidLong\n\n-79.304703\n\n\n\nrightSideVirginia = virginia[virginia[\"Lon\"] &gt; midLong]\naboveMeanRightSideVirginia = rightSideVirginia[rightSideVirginia[\"Mean\"] &gt; rightSideVirginia[\"Mean\"].mean()]\nprobAGivenB = sum(aboveMeanRightSideVirginia[\"sum_w\"]) / sum(rightSideVirginia[\"sum_w\"])\n\nprint(\"{}%\".format(round(probAGivenB * 100, 2)))\n\n30.21%\n\n\nThis means that there are enough locations on the right side of Virginia making below average income to bring this probability down, even though the red-colored areas are making almost double the average household income.\nLet’s reverse our question now. What is the conditional probability that someone is living on the right side of the midline of Virginia given that their average household income is above the average? Let’s finally use Bayes’ Rule!\n\\[P(B|A) = \\frac{P(A|B)P(B)}{P(A)}\\]\n\nprobA = sum(aboveMeans[\"sum_w\"]) / sum(virginia[\"sum_w\"])\nprobB = sum(rightSideVirginia[\"sum_w\"]) / sum(virginia[\"sum_w\"])\nprobBGivenA = (probAGivenB * probB) / probA\n\nprint(\"{}%\".format(round(probBGivenA * 100, 2)))\n\n84.33%\n\n\nA much different answer came out. This shows that you’d have a much easier time guessing where someone lives based on their income, rather than guessing someone’s income based on where they live.\nLet’s verify our answer using regular conditional probability, just to be sure that Bayes didn’t lie to us:\n\\[P(B|A) = \\frac{P(B \\cap A)}{P(A)}\\]\n\notherProbBGivenA = sum(aboveMeanRightSideVirginia[\"sum_w\"]) / sum(aboveMeans[\"sum_w\"])\nprint(\"{}%\".format(round(otherProbBGivenA * 100, 2)))\n\n84.33%\n\n\nSure enough, Bayes was right!"
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Welcome to my fourth blog post!"
  },
  {
    "objectID": "posts/Anomalies/index.html",
    "href": "posts/Anomalies/index.html",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Welcome to my fifth blog post!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! My name is Samuel Huber. I am a graduate student attending Virginia Tech for my Master’s degree in Computer Science with a specificied track toward machine learning and artificial intelligence. I graduated summa cum laude with a Bachelor’s degree in Aerospace Engineering and I am interested in all things science and technology related.\nAs of now, this blog will contain five posts about different machine learning topics. These posts should be read in order, top to bottom, and are written as a sort of pocket-sized “learning journey” for introducing new topics to interested individuals (such as yourself). Hopefully these posts will aid in your understanding of machine learning, or at least pique your interest."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Samuel Huber’s Blog",
    "section": "",
    "text": "Probability Theory and Random Variables\n\n\n\n\n\nThings are about to get Bayesian…\n\n\n\n\n\n\nDec 3, 2023\n\n\nSamuel Huber\n\n\n\n\n\n\n  \n\n\n\n\nLinear and Nonlinear Regression\n\n\n\n\n\nRegressive: An insurance company you can’t rely on.\n\n\n\n\n\n\nDec 3, 2023\n\n\nSamuel Huber\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\nThe information in this post is classified.\n\n\n\n\n\n\nDec 3, 2023\n\n\nSamuel Huber\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\nYou might get a headache…\n\n\n\n\n\n\nDec 3, 2023\n\n\nSamuel Huber\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly Detection\n\n\n\n\n\nWhat’s an ‘omaly’, and why are we detecting it?\n\n\n\n\n\n\nDec 3, 2023\n\n\nSamuel Huber\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "Welcome to my third blog post!"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/Regression/index.html",
    "href": "posts/Regression/index.html",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "If you’re reading this, you should have already finished the first blog post. Now, with Bayes’ Rule in your toolbox, you are ready to tackle a different set of problems.\nThis post will focus much more on actual data and machine learning concepts. Let’s get started!\n\n\nThe Dataset\nThis time we’ll start by introducing the dataset. This dataset contains information about different cars, such as their miles-per-gallon, number of cylinders, weight, and so on. It was downloaded from Kaggle and is simple enough to let us glide through the following concepts with ease. Let’s take a look:\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndf = pd.read_csv('auto-mpg.csv', encoding='windows-1252')\ndf.head()\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nmodel year\norigin\ncar name\n\n\n\n\n0\n18.0\n8\n307.0\n130\n3504\n12.0\n70\n1\nchevrolet chevelle malibu\n\n\n1\n15.0\n8\n350.0\n165\n3693\n11.5\n70\n1\nbuick skylark 320\n\n\n2\n18.0\n8\n318.0\n150\n3436\n11.0\n70\n1\nplymouth satellite\n\n\n3\n16.0\n8\n304.0\n150\n3433\n12.0\n70\n1\namc rebel sst\n\n\n4\n17.0\n8\n302.0\n140\n3449\n10.5\n70\n1\nford torino\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 398 entries, 0 to 397\nData columns (total 9 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   mpg           398 non-null    float64\n 1   cylinders     398 non-null    int64  \n 2   displacement  398 non-null    float64\n 3   horsepower    397 non-null    object \n 4   weight        398 non-null    int64  \n 5   acceleration  398 non-null    float64\n 6   model year    398 non-null    int64  \n 7   origin        398 non-null    int64  \n 8   car name      398 non-null    object \ndtypes: float64(3), int64(4), object(2)\nmemory usage: 28.1+ KB\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nweight\nacceleration\nmodel year\norigin\n\n\n\n\ncount\n398.000000\n398.000000\n398.000000\n398.000000\n398.000000\n398.000000\n398.000000\n\n\nmean\n23.514573\n5.454774\n193.425879\n2970.424623\n15.568090\n76.010050\n1.572864\n\n\nstd\n7.815984\n1.701004\n104.269838\n846.841774\n2.757689\n3.697627\n0.802055\n\n\nmin\n9.000000\n3.000000\n68.000000\n1613.000000\n8.000000\n70.000000\n1.000000\n\n\n25%\n17.500000\n4.000000\n104.250000\n2223.750000\n13.825000\n73.000000\n1.000000\n\n\n50%\n23.000000\n4.000000\n148.500000\n2803.500000\n15.500000\n76.000000\n1.000000\n\n\n75%\n29.000000\n8.000000\n262.000000\n3608.000000\n17.175000\n79.000000\n2.000000\n\n\nmax\n46.600000\n8.000000\n455.000000\n5140.000000\n24.800000\n82.000000\n3.000000\n\n\n\n\n\n\n\nThat covers the basic information again, but this time we’ll do something even cooler. Let’s display a histogram of each of the parameters:\n\ndf.hist(bins=50, figsize=(12, 8))\nplt.show()\n\n\n\n\n\n\n\n\nThis histogram shows the different values for each feature that every data point has. For example, we can see that a large portion of the cars have between \\(15\\) to \\(40\\) miles-per-gallon. This is cool and all, but for this blog post, we’re going to be more concerned with relationships between features. Let’s try something new:\n\nfrom pandas.plotting import scatter_matrix\n\nattributes = [\"mpg\", \"displacement\", \"weight\", \"acceleration\"]\nscatter_matrix(df[attributes], figsize=(12, 8))\nplt.show()\n\n\n\n\n\n\n\n\nNow this is cool, right? Each plot corresponds to a relationship between two features. For example, the bottom-left plot shows how miles-per-gallon might affect acceleration. From looking at that graph, we can see that there’s not much of a relationship. It’s somewhat of a big mess. But now look up and to the right one plot to see the relationship between weight and displacement. There’s clearly a strong relationship, and we are going to attempt to find it.\nLooking at the main diagonal, the plots look completely different. This is because they have the same \\(x\\) and \\(y\\) axes, so their plots really wouldn’t give us any important information. Therefore, the pandas library knows to replace these plots with histograms, although we’ve already seen them from the previous figure.\nLet’s now shift our focus to the graph of weight and displacement:\n\nscatter = df.plot.scatter(x='displacement', y='weight', alpha=0.5)\n\n\n\n\n\n\n\n\nAt this point, we’d like to find some sort of way to model this data. Before we do this, however, we need to look at some math.\n\n\n\nLinear Regression\nRelationships between two variables occur very frequently. For simplicity sake, we will assume that there is one independent variable (\\(x\\)) that affects the output variable (\\(y\\)), therefore called the dependent variable. Mathematically, we write:\n\\[y = f(x),\\]\nwhere \\(f\\) is a function that changes the input in some way to produce the output.\nWe can also use subscripts to represent having more than one input affecting the output, like so:\n\\[y = f(x_0, x_1, x_2, \\ldots)\\]\nEven with a large amount of total variables, determining the relationship between independent variables and dependent variables is as simple as finding \\(f\\). Or so you would think.\nWhen it comes to real-world relationships, there are very few cases where things are this simple. For example, think about a company that is using commercials to try to increase sales of their product. It would be very nice if doubling the amount of advertisement playing at any given time on television caused twice as many purchases, but that is simply not what happens. A small increase in ads may lead to a similar, deterministic increase in sales, but at a certain point people might get fed up with the ads and stop purchasing the product entirely.\nThis observation is even more exaggerated at the individual level. If you assume that all humans behave the same way, then you could easily reason that a change in the output variable will be consistent across all people, but this is also not true. Human beings are individuals, with their own thoughts and opinions and irrationality. At some point, someone may stop purchasing your product just because they don’t feel like it anymore.\nTo remedy this, we assume that there is still a function that can describe the relationship between our variables, but we also add on an error term:\n\\[Y = f(X) + \\epsilon\\]\nThis error term accounts for the other variables that are too complex to account for. It is true that if we knew every single variable, then we wouldn’t need the error term. But alas, this is the way things are.\nIn creating our model, we will try to find an approximate function that will give an approximate output for some input. In theory, this model will have reduced the error as much as possible:\n\\[\\hat{Y} = \\hat{f}(X)\\]\nThe little “hats” on top indicate that these are approximations.\nTo create our model, we need to train it on the available data, usually called the training set. If we could train forever, we would remove all of the error within our model, known as the reducible error. If our approximate function correctly represents the actual relationship between data, then we can come very close to this. However, since each data point is affected by variables that we can’t keep track of, there is some irreducible error that will always exist.\nSo, we have some understanding of the theory, but how do we do it in practice? There are quite a few methods, but the simplest is Linear Regression. Linear Regression essentially makes the assumption that the input(s) has a linear contribution to the output. In other words, doubling the input doubles the output, halving the input halves the output, and so on. A line has a very simple equation:\n\\[y = mx + b,\\]\nwhere \\(m\\) is the slope, which signifies the increase or decrease in output for a change in input, and b represents how vertically shifted the values are from the start. Let’s write it in a more appropriate form:\n\\[\\hat{Y} = \\hat{\\beta}_1X + \\hat{\\beta}_0\\]\nI’ve changed the \\(m\\) and \\(b\\) constants and used subscripts, but the idea is the same. We can also extend this idea to multiple independent variables:\n\\[\\hat{Y} = \\hat{\\beta}_0 + \\sum_{i = 1}^N \\hat{\\beta}_iX_i\\]\nThe hats on these constants also indicate that they are approximations of the true constants (if the data actually has a linear relationship).\nSo the next question is, how do we find the constants? This is where things begin to get a little complicated.\nComputers do this by minimizing some value that corresponds to the error of our model. There are a few good choices, but the most common is known as the Mean Squared Error (MSE):\n\\[\\mathrm{MSE} = \\frac{1}{N}\\sum_{i = 1}^N (Y_i - \\hat{f}(X_i))^2\\]\nThe inside of the parantheses show the difference between the actual measured output value of the data and the currne iteration our model’s output value for the same input. This difference is then squared. After getting this squared difference for every data point, they’re all summed up, and divided by the total number of data points.\nFor simple Linear Regression (one input variable), formulas for these constants can be found directly. First, we plug in the linear equation for \\(\\hat{f}(X)\\):\n\\[\\begin{equation}\n\\begin{split}\n\\frac{1}{N}\\sum_{i = 1}^N (Y_i - \\hat{f}(X_i))^2 &= \\frac{1}{N}\\sum_{i = 1}^N (Y_i - \\hat{\\beta}_1X_i - \\hat{\\beta}_0)^2\\\\\n&= \\frac{1}{N}\\sum_{i = 1}^N (Y_i^2 - 2\\hat{\\beta}_1X_iY_i - 2\\hat{\\beta}_0Y_i + \\hat{\\beta}_1^2X_i^2 + 2\\hat{\\beta}_0\\hat{\\beta}_1X_i + \\hat{\\beta}_0^2)\n\\end{split}\n\\end{equation}\\]\nPretty nasty. Now, we’d like to minimize this to find the coefficient values. Let’s imagine that the error for each coefficient looks something like this:\n\n\n\n\n\n\n\n\n\nOur error shouldn’t have a maximum, since our line could always be worse, but it should definitely have a minimum, so we know that this graph is somewhat representative of how the error will change as we change the coefficients. To mimimize this error, we need to find the coefficient values that will bring the error down into the very bottom of the valley. We can do this by taking the partial derivative with respect to the coefficient values. This is because the derivative represents the rate of change of a function with respect to a changing independent variable. If we find this derivative, and set it equal to zero, we should have reached an inflection point on the graph, since the derivative must switch between negative and positive values to flip the direction of the function.\nHere is a visual representation of the derivative of this curve:\n\n\n\n\n\n\n\n\n\nNotice how the line that represents the derivative of the curve passes through \\(y = 0\\) right at \\(x = 3\\), the exact same spot where the parabolic curve flips directions. Therefore, to minimize the error, we can take the derivative of our error with respect to each of the variables, set them equal to zero, and solve for the coefficients.\nWe’ll start with the easier one (\\(\\hat{\\beta}_0\\)) first:\n\\[\\frac{\\partial}{\\partial \\hat{\\beta}_0}\\mathrm{MSE} = \\frac{1}{N}\\sum_{i = 1}^N (-2Y_i + 2\\hat{\\beta}_1X_i + 2\\hat{\\beta}_0)\\]\nNow that we have the partial derivative, we can set it equal to zero to find the minimum:\n\\[\\newcommand{\\olsi}[1]{\\,\\overline{\\!{#1}}}\\]\n\\[\\begin{equation}\n\\begin{split}\n\\frac{1}{N}\\sum_{i = 1}^N (-2Y_i + 2\\hat{\\beta}_1X_i + 2\\hat{\\beta}_0) &= 0\\\\\n-\\frac{2}{N}\\sum_{i = 1}^NY_i + \\frac{2\\hat{\\beta}_1}{N}\\sum_{i = 1}^NX_i + \\frac{2\\hat{\\beta}_0}{N}\\sum_{i = 1}^N 1 &= 0\\\\\n-\\olsi{Y} + \\hat{\\beta}_1\\olsi{X} + \\hat{\\beta}_0 &= 0\n\\end{split}\n\\end{equation}\\]\nRearranging:\n\\[\\hat{\\beta}_0 = \\olsi{Y} - \\hat{\\beta}_1\\olsi{X},\\]\nwhere \\(\\olsi{X}\\) and \\(\\olsi{Y}\\) are the averages of \\(X\\) and \\(Y\\), respectively.\nNow, let’s do the same thing for \\(\\hat{\\beta}_1\\). Taking the partial derivative:\n\\[\\frac{\\partial}{\\partial \\hat{\\beta}_1}\\mathrm{MSE} = \\frac{1}{N}\\sum_{i = 1}^N (-2X_iY_i + 2\\hat{\\beta}_1X_i^2 + 2\\hat{\\beta}_0X_i)\\]\nAnd setting it equal to zero:\n\\[\\begin{equation}\n\\begin{split}\n\\frac{1}{N}\\sum_{i = 1}^N (-2X_iY_i + 2\\hat{\\beta}_1X_i^2 + 2\\hat{\\beta}_0X_i) &= 0\\\\\n-\\frac{2}{N}\\sum_{i = 1}^N X_iY_i + \\frac{2\\hat{\\beta}_1}{N}\\sum_{i = 1}^NX_i^2 + \\frac{2\\hat{\\beta}_0}{N}\\sum_{i = 1}^NX_i &= 0\\\\\n\\end{split}\n\\end{equation}\\]\nSubstituting our new equation in for \\(\\hat{\\beta}_1\\):\n\\[\\begin{equation}\n\\begin{split}\n-\\frac{2}{N}\\sum_{i = 1}^N X_iY_i + \\frac{2\\hat{\\beta}_1}{N}\\sum_{i = 1}^NX_i^2 + \\frac{2}{N}(\\olsi{Y} - \\hat{\\beta}_1\\olsi{X})\\sum_{i = 1}^NX_i &= 0\\\\\n-\\frac{2}{N}\\sum_{i = 1}^N X_iY_i + \\frac{2\\hat{\\beta}_1}{N}\\sum_{i = 1}^NX_i^2 + \\frac{2}{N}\\sum_{i = 1}^NX_i\\olsi{Y} - \\frac{2\\hat{\\beta}_1}{N}\\sum_{i = 1}^NX_i\\olsi{X} &= 0\\\\\n\\frac{2}{N}\\left(\\sum_{i = 1}^NX_i\\olsi{Y} - \\sum_{i = 1}^N X_iY_i\\right) + \\frac{2\\hat{\\beta}_1}{N}\\left(\\sum_{i = 1}^NX_i^2 - \\sum_{i = 1}^NX_i\\olsi{X}\\right) &= 0\\\\\n\\sum_{i = 1}^N(X_i\\olsi{Y} - X_iY_i) + \\hat{\\beta}_1\\sum_{i = 1}^N(X_i^2 - X_i\\olsi{X}) &= 0\\\\\n-\\sum_{i = 1}^NX_i(Y_i - \\olsi{Y}) + \\hat{\\beta}_1\\sum_{i = 1}^NX_i(X_i - \\olsi{X}) &= 0\n\\end{split}\n\\end{equation}\\]\nRearranging:\n\\[\\begin{equation}\n\\begin{split}\n\\hat{\\beta}_1 &= \\frac{\\sum\\limits_{i = 1}^NX_i(Y_i - \\olsi{Y})}{\\sum\\limits_{i = 1}^NX_i(X_i - \\olsi{X})}\n\\end{split}\n\\end{equation}\\]\nAt this point, we have an equation for \\(\\hat{\\beta}_1\\), but this may not be the form you’ve seen it in before. In order to get to that form, we need to do a unique manipulation. First, we need to note the following relationship:\n\\[\\begin{equation}\n\\begin{split}\n\\sum\\limits_{i = 1}^N (X_i - \\olsi{X}) &= \\left( X_0 -  \\frac{X_0 + X_1 + \\ldots}{N} \\right) + \\left( X_1 -  \\frac{X_0 + X_1 + \\ldots}{N} \\right) + \\ldots\\\\\n&= \\left( X_0 + X_1 + \\ldots \\right) - N\\left( \\frac{X_0 + X_1 + \\ldots}{N} \\right) = 0\n\\end{split}\n\\end{equation}\\]\nThe same exact process can be applied with \\(Y\\) and \\(\\olsi{Y}\\). So, our two new equations are:\n\\[\\begin{equation}\n\\begin{split}\n\\sum\\limits_{i = 1}^N (X_i - \\olsi{X}) &= 0\\\\\n\\sum\\limits_{i = 1}^N (Y_i - \\olsi{Y}) &= 0\n\\end{split}\n\\end{equation}\\]\nSince the left side is equal to zero, we can multiply it by whatever constant we’d like and the relationship would still be maintained. We will multiply both equations by \\(\\olsi{X}\\):\n\\[\\begin{equation}\n\\begin{split}\n\\olsi{X}\\sum\\limits_{i = 1}^N (X_i - \\olsi{X}) &= \\sum\\limits_{i = 1}^N \\olsi{X}(X_i - \\olsi{X}) = 0\\\\\n\\olsi{X}\\sum\\limits_{i = 1}^N (Y_i - \\olsi{Y}) &= \\sum\\limits_{i = 1}^N \\olsi{X}(Y_i - \\olsi{Y}) = 0\n\\end{split}\n\\end{equation}\\]\nThese equations will be used to perform some algebraic wizardry. Since both of these equations are equal to zero, we can subtract them from the numerator and denominator of our equation for \\(\\hat{\\beta}_1\\) without changing anything:\n\\[\\begin{equation}\n\\begin{split}\n\\hat{\\beta}_1 = \\frac{\\sum\\limits_{i = 1}^NX_i(Y_i - \\olsi{Y})}{\\sum\\limits_{i = 1}^NX_i(X_i - \\olsi{X})} &= \\frac{\\sum\\limits_{i = 1}^NX_i(Y_i - \\olsi{Y}) - \\sum\\limits_{i = 1}^N \\olsi{X}(Y_i - \\olsi{Y})}{\\sum\\limits_{i = 1}^NX_i(X_i - \\olsi{X}) - \\sum\\limits_{i = 1}^N \\olsi{X}(X_i - \\olsi{X})}\\\\\n&= \\frac{\\sum\\limits_{i = 1}^N\\left[X_i(Y_i - \\olsi{Y}) - \\olsi{X}(Y_i - \\olsi{Y})\\right]}{\\sum\\limits_{i = 1}^N\\left[X_i(X_i - \\olsi{X}) - \\olsi{X}(X_i - \\olsi{X})\\right]}\\\\\n&= \\frac{\\sum\\limits_{i = 1}^N\\left[(X_i - \\olsi{X})(Y_i - \\olsi{Y})\\right]}{\\sum\\limits_{i = 1}^N\\left[(X_i - \\olsi{X})(X_i - \\olsi{X})\\right]}\\\\\n&= \\frac{\\sum\\limits_{i = 1}^N(X_i - \\olsi{X})(Y_i - \\olsi{Y})}{\\sum\\limits_{i = 1}^N(X_i - \\olsi{X})^2}\n\\end{split}\n\\end{equation}\\]\nFinally! We got a (somewhat) simple looking equation for our second coefficient. There is another representation, but it is more complicated and doesn’t provide many benefits over this one.\nLet’s recap:\n\\[\\hat{\\beta}_0 = \\olsi{Y} - \\hat{\\beta}_1\\olsi{X},\\]\n\\[\\hat{\\beta}_1 = \\frac{\\sum\\limits_{i = 1}^N(X_i - \\olsi{X})(Y_i - \\olsi{Y})}{\\sum\\limits_{i = 1}^N(X_i - \\olsi{X})^2},\\]\nwhere\n\\[\\olsi{X} = \\frac{1}{N}\\sum\\limits_{i = 1}^N X_i\\]\n\\[\\mathrm{and}\\]\n\\[\\olsi{Y} = \\frac{1}{N}\\sum\\limits_{i = 1}^N Y_i\\]\nNow that we have our equations, let’s try applying them.\n\nimport numpy as np\n\nnp.random.seed(42)\nN = 100\nX = 2 * np.random.rand(m, 1)\nY = 4 + 3 * X + np.random.randn(m, 1)\n\nX = [element[0] for element in X]\nY = [element[0] for element in Y]\n\nplt.figure(figsize=(6, 4))\nplt.plot(X, Y, \"b.\")\nplt.xlabel(\"$X$\")\nplt.ylabel(\"$Y$\", rotation=0)\nplt.axis([0, 2, 0, 15])\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nThis is fake data that was generated with the sole purpose of using our new equations to fit a line to it. Let’s try it out!\n\nproductSum = 0\nsquareSum = 0\n\nX_Mean = sum(X) / N\nY_Mean = sum(Y) / N\n\nfor i in range(0, N):\n    productSum += (X[i] - X_Mean) * (Y[i] - Y_Mean)\n    squareSum += (X[i] - X_Mean) * (X[i] - X_Mean)\n    \nb1 = productSum / squareSum\nb0 = Y_Mean - (b1 * X_Mean)\n\nprint(b0)\nprint(b1)\n\n4.215096157546749\n2.770113386438484\n\n\nThese are close to the coefficients that were used to generate the randomized data in the first place (\\(4\\) and \\(3\\)), so we know we’re on the right track. Let’s plot it to see what the graph looks like:\n\nxAxis = np.asarray(range(0, 100))\nyAxis = (xAxis * b1) + b0\n\nplt.figure(figsize=(6, 4))\nplt.plot(X, Y, \"b.\")\nplt.plot(xAxis, yAxis, \"g\")\nplt.xlabel(\"$X$\")\nplt.ylabel(\"$Y$\", rotation=0)\nplt.axis([0, 2, 0, 15])\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nNice! We see that the line described by our coefficients seems to pass as close to all the points as it can.\nYou may wonder, “why did we try to minimize the Mean Squared Error?” Why not just the difference between the true value and estimated value? Well, some true values may be above our estimates, and some may be below. A worst-case scenario would occur when this occurs over and over, and all of the positive values cancel with the negative values, giving us no information. The squaring process removes all negatives.\nYour next question may be, “then why not use the absolute value instead of squaring?” And that is a very good point. In fact, this can be done. The reason it typically isn’t is that the Mean Squared Error has some nice properties with certain distributions and have a deeper connection to the data in general. However, the absolute value error is used enough to have its own name: the L-1 Norm. The square root of our Mean Squared Error, (also called the Root Mean Square (RMS)) is called the L-2 Norm. There are technically an infinite amount of other norms, but the L-1 and L-2 norms are the most common ones used in practice.\nThe L-1 Norm is preferred when there are a few outliers in the data. This is because the Mean Squared Error exacerbates the error that outliers have, since everything is squared. Let’s take a look at how much this affects our line by adding a couple outliers:\n\nrandomIndices = [i for (i, _) in enumerate(X) if X[i] &gt; 1.9]\n\nY[randomIndices[0]] = 0.5\nY[randomIndices[1]] = 0.25\n\nproductSum = 0\nsquareSum = 0\n\nX_Mean = sum(X) / N\nY_Mean = sum(Y) / N\n\nfor i in range(0, N):\n    productSum += (X[i] - X_Mean) * (Y[i] - Y_Mean)\n    squareSum += (X[i] - X_Mean) * (X[i] - X_Mean)\n    \nnewB1 = productSum / squareSum\nnewB0 = Y_Mean - (b1 * X_Mean)\n\nnewYAxis = (xAxis * newB1) + newB0\n\nplt.figure(figsize=(6, 4))\nplt.plot(X, Y, \"b.\")\nplt.plot(xAxis, yAxis, \"g\")\nplt.plot(xAxis, newYAxis, \"r\")\nplt.xlabel(\"$X$\")\nplt.ylabel(\"$Y$\", rotation=0)\nplt.axis([0, 2, 0, 15])\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nAs you can see, it took only two outliers to move our new line (red) a substantial amount. In this case, it might be better to use the L-1 norm.\nLet’s try to fit a line to our dataset this time. We’ll use scikit-learn’s LinearRegression class to do it for us:\n\nfrom sklearn.linear_model import LinearRegression\n\ndisp = np.asarray(df['displacement']).reshape(-1, 1)\nweight = np.asarray(df['weight']).reshape(-1, 1)\n\nlin_reg = LinearRegression()\nlin_reg.fit(disp, weight)\nlin_reg.intercept_, lin_reg.coef_\n\n(array([1505.01873083]), array([[7.57605909]]))\n\n\n\nxAxis = range(int(min(disp)), int(max(disp)))\nyAxis = (xAxis * lin_reg.coef_[0]) + lin_reg.intercept_[0]\n\nplt.scatter(df['displacement'], df['weight'], alpha=0.5)\nplt.plot(xAxis, yAxis, \"r\")\nplt.xlabel('displacement')\nplt.ylabel('weight')\nplt.show()\n\n\n\n\nLooks pretty solid! This model can now be used to estimate the weight of a car just by knowing the displacement. Let’s use a specific car as an example and measure how close our prediction was. We will use the Lexus LS 400, which has a \\(\\sim250~\\mathrm{in}^3\\) displacement and a curb weight of \\(\\sim3700~\\mathrm{lbs}\\):\n\ndispTest = 250\nweightTest = (lin_reg.coef_ * dispTest) + lin_reg.intercept_\nweightTest[0][0]\n\n3399.0335038967237\n\n\nNot too bad! With an error of about \\(\\frac{3700 - 3400}{3700} \\approx 8.1\\%\\), we’re doing pretty well, especially considering that our dataset doesn’t contain any Lexus cars:\nOf course, the variation of the weight for each displacement is due to other variables affecting the weight. In this case, if you want to determine how multiple variables are affecting the output, you can use Multiple Linear Regression. Since there is a coefficient for each independent variable, finding the formulas directly is incredibly difficult, and only gets harder as more variables are added. Of course, this is why we invented computers; to do our dirty work for us.\n\n\n\nNonlinear Regression\nLet’s look at another plot from our data:\n\nplt.scatter(df['mpg'], df['displacement'], alpha=0.5)\nplt.xlabel('mpg')\nplt.ylabel('displacement')\nplt.show()\n\n\n\n\nWe can see that fitting a line to this would be pretty difficult. We might have some accurrate approximations for miles-per-gallon values between \\(15\\) and \\(25\\), but the data seems to level out after this point. In this case, we need to find a non-linear model. Luckily, scikit-learn has our back:\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndisp = np.asarray(df['displacement']).reshape(-1, 1)\nmpg = np.asarray(df['mpg']).reshape(-1, 1)\n\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly_features.fit_transform(mpg)\n\nlin_reg = LinearRegression()\nlin_reg.fit(X_poly, disp)\nlin_reg.intercept_, lin_reg.coef_\n\nX_new = np.linspace(10, 45, 100).reshape(100, 1)\nX_new_poly = poly_features.transform(X_new)\ny_new = lin_reg.predict(X_new_poly)\n\nplt.figure(figsize=(6, 4))\nplt.plot(mpg, disp, \"b.\", alpha=0.5)\nplt.plot(X_new, y_new, \"r-\", linewidth=2)\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\", rotation=0)\nplt.grid()\nplt.show()\n\n\n\n\nNot too bad! We shaped this using a polynomial of degree \\(2\\), otherwise known as a parabola. Would it become more accurate if we increased the degree value? Let’s try:\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndisp = np.asarray(df['displacement']).reshape(-1, 1)\nmpg = np.asarray(df['mpg']).reshape(-1, 1)\n\npoly_features = PolynomialFeatures(degree=3, include_bias=False)\nX_poly = poly_features.fit_transform(mpg)\n\nlin_reg = LinearRegression()\nlin_reg.fit(X_poly, disp)\nlin_reg.intercept_, lin_reg.coef_\n\nX_new = np.linspace(10, 45, 100).reshape(100, 1)\nX_new_poly = poly_features.transform(X_new)\ny_new = lin_reg.predict(X_new_poly)\n\nplt.figure(figsize=(6, 4))\nplt.plot(mpg, disp, \"b.\", alpha=0.5)\nplt.plot(X_new, y_new, \"r-\", linewidth=2)\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\", rotation=0)\nplt.grid()\nplt.show()\n\n\n\n\nChanging the degree to \\(3\\) definitely seems to have lowered the curve at the end, which is nice! But why can’t we just keep increasing the degree? Well, as you increase the flexibility that the model has, you begin to do something called overfitting. In simple terms, your model is trying to match too closely to the data in your training set. This makes your training set error look really low, which appears to be a good thing, but the problem is that any predictions made will be less accurate. Essentially, the model is finding patterns that aren’t really there, and is trying to match them as closely as possible. Another way of saying the same thing is to say that the model has high variance.\nUnderfitting occurs when the model doesn’t fit the training data particulary well, but generalizes to new data well (only because it’s so simple). You can also say that this model has high bias.\nIn machine learning, bias and variance are inescapable. Raising your bias will lower your variance and result in underfitting. Raising your variance will lower your bias and result in overfitting. This is known as the bias-variance tradeoff. The question of finding a good model is the same as asking for a model that minimizes both bias and variance as much as possible."
  }
]