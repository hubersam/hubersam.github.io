[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Probability/index.html",
    "href": "posts/Probability/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "If you’re reading this, you have just begun your pocket-sized journey into machine learning topics. As with any field, you must start at the basics, and that is where you are now. Unfortunately, in this case, the basics aren’t so basic.\nFor most people (myself included), probability is one of the hardest concepts to grasp out of all of the mathematical disciplines. Most of the ideas are algebraically and computationally simple, but can be a conceptual nightmare. My only goal for this post is to form a basic layer of understanding with which you can use as a foundation for all of your future endeavours in machine learning.\nWe will start with basic probability.\n\n\nBasic Probability\nWe’ll start with some terminology.\n\nThe set of all possible outcomes is called the sample space, and is usually given the symbol \\(S\\).\nThe probability of a event, \\(A\\), occurring can be calculated by taking the ratio of the cardinalities of the event and the sample space, \\(P(A) = \\frac{\\left| A \\right|}{\\left| S \\right|}\\).\n\nThere were a lot of big words in there, so let’s take a look at an example. Imagine rolling a fair six-sided die. There are six equally likely possible outcomes: 1, 2, 3, 4, 5, 6. Therefore, since there are no other possible outcomes, then we have our sample space:\n\\[S = \\{ 1, 2, 3, 4, 5, 6 \\}\\]\nNow, let’s describe an “event”. We will call \\(A\\) the event where we roll an even number. Then we can write:\n\\[A = \\{ 2, 4, 6 \\}\\]\nNow look at the third rule again. The symbols and big words may look complicated, but “taking the ratio of the cardinalities of the event and the sample space” really just means counting up the number of elements in our event and dividing it by the total number of possible elements. In our case, there are three possible even numbers, and six possible total numbers, so:\n\\[P(A) = \\frac{\\left| A \\right|}{\\left| S \\right|} = \\frac{3}{6} = \\frac{1}{2} = 0.5 = 50\\%\\]\nThis should be making sense so far. There are a few other things we can deduce from this simple equation too. Let’s calculate the probability of the sample space:\n\\[P(S) = \\frac{\\left| S \\right|}{\\left| S \\right|} = \\frac{6}{6} = 1 = 100\\%\\]\nThis should also make sense. The probability of any of the possible values occurring should be guaranteed. What about a set with nothing in it? We’ll call that set \\(B\\):\n\\[B = \\{\\}\\]\nThen the probability of \\(B\\) occuring is:\n\\[P(B) = \\frac{\\left| B \\right|}{\\left| S \\right|} = \\frac{0}{6} = 0\\]\nWhich also makes sense. This gives us insight into the range of values that a probability should be able to take:\n\\[0 \\leq P(A) \\leq 1\\]\nThis goes for all scenarios; not just for the die example.\n\n\n\nProbability of This and That\nLet’s extend our example by imagining that we roll two dice, one by one. The first roll can take on any one of six values, and the second one can take on six different values for each of the six values that the first roll could have taken. This gives us \\(6 \\times 6 = 36\\) different possible outcomes. Let’s write our sample space like this:\n\\[S = \\{ (1, 1), (1, 2), (1, 3) \\ldots (6, 5), (6, 6) \\}\\]\nWhat is the probability of the first roll being a \\(2\\)? Let’s write the set for the event of all combinations of rolls where the first roll is a \\(2\\):\n\\[A = \\{ (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6) \\}\\]\nAnd finally:\n\\[P(A) = \\frac{\\left| A \\right|}{\\left| S \\right|} = \\frac{6}{36} = \\frac{1}{6} \\approx 16.67\\%\\]\nLet’s try another probability. This time we ask ourselves, “what’s the probability that our second roll is \\(5\\)”? Well, this set is very similar to the previous one:\n\\[B = \\{ (1, 5), (2, 5), (3, 5), (4, 5), (5, 5), (6, 5) \\}\\]\nAnd the probability is identical:\n\\[P(B) = \\frac{\\left| B \\right|}{\\left| S \\right|} = \\frac{6}{36} = \\frac{1}{6} \\approx 16.67\\%\\]\nLast example! What’s the probability that the first roll is \\(2\\), and that our second roll is \\(5\\)? Well, this set is pretty simple:\n\\[C = \\{ (2, 5) \\}\\]\nSo our probability is:\n\\[P(C) = \\frac{\\left| B \\right|}{\\left| S \\right|} = \\frac{1}{36} \\approx 2.78\\%\\]\nThis is correct, but we have a more powerful technique up our sleeve. In order to find it, let’s look at a visual representation of our example:\n\n\n\n\n\n\n\n\n\nThe red circle on the left represents all outcomes in the \\(A\\) event, and the green circle on the right represents all outcomes in the \\(B\\) event. Notice that the intersection of these two regions shows the outcome for event \\(C\\).\nThere are two main operations when it comes to sets that are very useful for determining more advanced probabilities.\n\nUnion\nIntersection\n\nThe union, given the symbol \\(\\cup\\), is defined as the set of all events that both \\(A\\) and \\(B\\) contain. In the case of our picture, this is what the union looks like:\n\n\n\n\n\n\n\n\n\nConversely, the intersection, given the symbol \\(\\cap\\), gives the set of outcomes that \\(A\\) and \\(B\\) share. Here is the diagram:\n\n\n\n\n\n\n\n\n\nYou can think of it this way:\n\n\\(A \\cup B = A~\\mathrm{or}~B\\)\n\\(A \\cap B = A~\\mathrm{and}~B\\)\n\nBut why is this useful? Well, there’s a really cool relationship that we can derive between the two of them just using the information so far. Here’s how we do it.\nImagine taking the individual probabilities of \\(A\\) and \\(B\\) and adding them together. In doing this, you’d have calculated the probability of the union of the two events, plus a little extra. This little extra comes from the fact that both sets contain their intersection, so you counted the intersection twice on accident. The first copy contributed to the union of the two sets, the second copy is the “little extra”. In other words:\n\\[P(A) + P(B) = P(A \\cup B) + P(A \\cap B)\\]\nWe can rearrange this equation to get our desired result:\n\\[P(A \\cap B) = P(A) + P(B) - P(A \\cup B)\\]\nLet’s use this equation to recalculate the probability of \\(C\\) again. That is, let’s calculate the probability that the first roll is \\(2\\) and that the second roll is \\(5\\).\nFirst, we need to find \\(A \\cup B\\). Remember, this is the same as asking for all outcomes where the first number is \\(2\\) or that the second number is \\(5\\). We can take all of the individual elements from \\(A\\) and be \\(B\\) independently (making sure not to double count any) and place them together:\n\\[A \\cup B = \\{ (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (1, 5), (3, 5), (4, 5), (5, 5), (6, 5) \\}\\]\nNotice that \\((2, 5)\\) appears in both sets, but we only count it once.\nLet’s get this probability:\n\\[P(A \\cup B) = \\frac{\\left| A \\cup B \\right|}{\\left| S \\right|} = \\frac{11}{36} \\approx 30.56\\%\\]\nNow we can get to calculating!\n\\[\\begin{equation}\n\\begin{split}\nP(C) = P(A \\cap B) &= P(A) + P(B) - P(A \\cup B)\\\\\n&= \\frac{1}{6} + \\frac{1}{6} - \\frac{11}{36}\\\\\n&= \\frac{1}{36} \\approx 2.78\\%\n\\end{split}\n\\end{equation}\\]\nExactly the same as last time!\nThis process may seem far more cumbersome than the original calculation, but as the sets of events start increasing in size, it is much easier to know how to calculate probabilities of intersections of events from other probabilities as opposed to actually counting it out.\n\n\n\nConditional Probability\nLet’s extend our example by imagining that we roll the same die five times. The die is still fair, and takes on one of six values every roll. We will then track the total sum of all the rolls. For example, if the values that appeared during the rolls were \\(1, 4, 3, 6, 2\\), the total would be \\(1 + 4 + 3 + 6 + 2 = 16\\).\nBefore we get started, let’s analyze what we know right away. We know that the total number of possible rolls is \\(6^5 = 7,776\\). We also know that the lowest sum possible is \\(1 + 1 + 1 + 1 + 1 = 5\\), and the highest is \\(6 + 6 + 6 + 6 + 6 = 30\\). These both occur only once, since they only have one set of numbers that can provide that sum. Conversely, other sums will occur far more often. For example, imagine a sum where all the rolls are different numbers: \\(1 + 2 + 3 + 4 + 5 = 15\\). Since these rolls could have happened in any order, and there are \\(5! = 5 \\times 4 \\times 3 \\times 2 \\times 1 = 120\\) different possible orderings, then the chances of getting a sum of \\(15\\) is much higher than getting a sum of \\(5\\) or \\(30\\).\nThis gives us insight that the distribution of outcomes is going to be most common near the average roll value (\\(\\frac{30 + 5}{2} = 17.5\\)), since there are far more ways of arranging the sums, than at the low or high extremes. Let’s look at the graph of the rolls:\n\n\nCode\nallRolls = []\n\ncurrRoll = [1, 1, 1, 1, 1]\nfor a in range(1, 7):\n    currRoll[0] = a\n    for b in range(1, 7):\n        currRoll[1] = b\n        for c in range(1, 7):\n            currRoll[2] = c\n            for d in range(1, 7):\n                currRoll[3] = d\n                for e in range(1, 7):\n                    currRoll[4] = e\n                    allRolls.append(currRoll.copy())\n                    \nallRollSums = [sum(roll) for roll in allRolls]\n\nsumValues = list(range(5, 30 + 1))\nsumCounts = [allRollSums.count(value) for value in sumValues]\n\nimport pandas as pd\nimport matplotlib.ticker as mtick\n\ndf = pd.DataFrame(sumCounts, sumValues)\n\ndf.plot.bar(rot=0, width=0.85, legend=False)\nplt.xlabel(\"Sum of Rolls\")\nplt.ylabel(\"Number of Occurrences\")\n\n\nText(0, 0.5, 'Number of Occurrences')\n\n\n\n\n\n\n\n\n\nJust as we predicted. This graph is a great example of a Gaussian Distribution. Let’s divide each of the individual “number of occurrences” values by the total number of possible rolls (\\(7,776\\)) to get the probability of getting each sum:\n\n\nCode\nnormalizedSumCounts = [count / sum(sumCounts) for count in sumCounts]\ndf = pd.DataFrame(normalizedSumCounts, sumValues)\nax = df.plot.bar(rot=0, width=0.85, legend=False)\nplt.xlabel(\"Sum of Rolls\")\nplt.ylabel(\"Chance of Occurrence\")\nax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n\n\n\n\n\n\n\n\n\nVery nice. We can see that getting a sum of around \\(17\\) or \\(18\\) will happen in about \\(1\\) in every \\(10\\) rolls, while getting a sum of \\(5\\) or \\(30\\) is not probable at all (\\(\\frac{1}{7776} \\approx 0.0129\\%\\)).\nWe can also find the probability of obtaining a range of values very easily. All we have to do is add the probability of obtaining each sum within the range. For example, the probability of getting a sum between \\(16\\) and \\(19\\) (inclusive) would be:\n\\[\\begin{equation}\n\\begin{split}\nP\\left(16 \\leq \\sum_{i=1}^{5}x_i \\leq 19\\right) &= P(16 \\leq x_1 + x_2 + x_3 + x_4 + x_5 \\leq 19)\\\\\n&= P(16) + P(17) + P(18) + P(19)\n\\end{split}\n\\end{equation}\\]\n\n\nCode\nsumRange = range(16, 19 + 1)\nsumRangeProbs = [normalizedSumCounts[sumValues.index(value)] for value in sumValues if value in sumRange]\nprint(\"{}%\".format(round(sum(sumRangeProbs * 100), 2)))\n\n\n38.97%\n\n\nNeat! This shows that more than a third of all of the sums of five rolls will be somewhere between \\(16\\) and \\(19\\).\nThis has led us to one of the most important parts of probability. Let’s ask a much more interesting and insightful question: what is the probability that the sum will be between \\(16\\) and \\(19\\) given that the first three rolls add up to \\(14\\)? At first glance, this question may seem to have the same answer as the previous question. After all, why should the knowledge of our first three rolls affect the outcome? Well, as it turns out, this new information essentially excludes sequences of rolls that we would have otherwise considered. To understand how to solve this, let’s look at a graphical representation again:\n\n\n\n\n\n\n\n\n\nThe probability for of an event occurring given another event is written as \\(P(\\mathrm{Event~1} | \\mathrm{Event~2})\\). For example, the probability of \\(A\\) given \\(B\\) is \\(P(A|B)\\).\nDetermining a relationship for this is not hard. We are essentially asking, “what is the probability of \\(A\\) being true, given that \\(B\\) is also true?” We know that a typical probability is the ratio of the number of occurrences of that event to the number of possible occurrences: \\(P(A) = \\frac{\\left| A \\right|}{\\left| S \\right|}\\). However, in this case, the event we’re concerned with is the intersection of two events (since both \\(A\\) and \\(B\\) are true), so the numerator should be the number of occurrences of the intersection, \\(\\left| A \\cap B \\right|\\). The other difference is, the number of possible occurrences is no longer the sample space. Instead, it is the number of occurrences of the second event, \\(\\left| B \\right|\\). Let’s visualize this:\n\n\n\n\n\n\n\n\n\nHere is a visual representation of the events where \\(B\\) is true, but \\(A\\) is not true. This can be written as \\(B \\cap \\neg A\\), where the \\(\\neg\\) symbol means “not”. Now, let’s reinclude the intersection of \\(A\\) and \\(B\\):\n\n\n\n\n\n\n\n\n\nNow we have visually represented the question, “what is the chance that \\(A\\) is true, given that \\(B\\) is also true?” We know that this probability should be the number of cases where both \\(A\\) and \\(B\\) are true, divided by all of the cases where \\(B\\) is true. Another way of thinking about this is that \\(B\\) has essentially become a “new sample space”, and \\(P(A \\cap B)\\) is what we’re trying to calculate. Visually:\n\n\n\n\n\n\n\n\n\nSo our equation is:\n\\[P(A|B) = \\frac{\\left| A \\cap B \\right|}{\\left|B \\right|}\\]\nLet’s do some algebraic magic:\n\\[P(A|B) = \\frac{\\left| A \\cap B \\right|}{\\left|B \\right|} \\times \\frac{\\frac{1}{\\left| S \\right|}}{\\frac{1}{\\left| S \\right|}} = \\frac{\\frac{\\left| A \\cap B \\right|}{\\left| S \\right|}}{\\frac{\\left| B \\right|}{\\left| S \\right|}} = \\frac{P(A \\cap B)}{P(B)}\\]\nSo now we can firmly state that the problem of \\(A\\) given \\(B\\) is equal to the probability of \\(A\\) and \\(B\\), divided by the probability of \\(B\\):\n\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\]\nLet’s apply this to the original question: “what is the probability that the sum will be between \\(16\\) and \\(19\\) given that the first three rolls add up to \\(14\\)?”:\n\\[P\\left(16 \\leq \\left.\\sum_{i=1}^{5}x_i \\leq 19 \\right| \\sum_{i=1}^{3}x_i = 14\\right) = \\frac{P\\left(16 \\leq \\sum\\limits_{i=1}^{5}x_i \\leq 19~\\mathrm{and}~\\sum\\limits_{i=1}^{3}x_i = 14\\right)}{P\\left(\\sum\\limits_{i=1}^{3}x_i = 14\\right)}\\]\n\n\n27.78%\n\n\nSo, as it turns out, if your first three rolls sum to a \\(14\\), you have slightly more than a one in four chance to have a total sum between \\(16\\) and \\(19\\). Just for fun, let’s plot the conditional probability of rolling between \\(16\\) and \\(19\\) given different sums for the first three rolls:\n\n\nCode\nfirstThreeSum = range(3, 18 + 1)\nchances = []\nfor i in firstThreeSum:\n    firstThreeRollSums = [sum(roll) for roll in allRolls if roll[0] + roll[1] + roll[2] == i]\n    firstThreeRollSumsInRange = [roll for roll in firstThreeRollSums if roll &gt;= 16 and roll &lt;= 19]\n    chances.append(len(firstThreeRollSumsInRange) / len(firstThreeRollSums))\n\ndf = pd.DataFrame(chances, firstThreeSum)\nax = df.plot.bar(rot=0, width=0.85, legend=False)\nplt.xlabel(\"Sum of First Three Rolls\")\nplt.ylabel(\"Chance of Total Sum Between 16 and 19\")\nax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n\n\n\n\n\n\n\n\n\nWe can see that if the first three rolls sum to \\(3\\), even the highest values for the other rolls can’t reach the range: \\(1 + 1 + 1 + 6 + 6 = 15 &lt; 16\\). This applies to \\(18\\) as well: \\(6 + 6 + 6 + 1 + 1 = 20 &gt; 19\\). As it turns out, the sums of the first three values that gives you the highest chances of rolling within the range are \\(10\\) and \\(11\\).\nFunnily enough, if your first three rolls sum to \\(9\\) or \\(12\\), your chances of landing in the range are exactly \\(50\\%\\). Another quirk is that if your first three rolls sum to \\(8\\) or \\(13\\), your chances of landing in the range is \\(38.89\\%\\), which is almost identical to the original chance of rolling within the range, \\(P(A)\\)!\n\n\n\nBayes’ Rule\nWe have now entered the final (and most important) section. All of the previous knowledge has been building up to this point.\n\nLet’s examine the conditional probability equation again:\n\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\]\nWe can rearrange it like so:\n\\[P(A \\cap B) = P(A|B)P(B)\\]\nAt this point, we can note that \\(P(A \\cap B) = P(B \\cap A)\\), since the intersection of the two sets remains the same regardless of order. Let’s examine the conditional probability using this order:\n\\[P(B \\cap A) = P(B|A)P(A)\\]\nSince \\(P(A \\cap B) = P(B \\cap A)\\), then the right sides of each of the equations should be equal to each other:\n\\[P(A|B)P(B) = P(B|A)P(A)\\]\nAnd we can rearrange once more to give:\n\\[P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\\]\nThis is known as Bayes’ Rule, and is one of the most important machine learning concepts. It shows that there is a fundamental relationship between conditional probabilities with different orderings. What’s amazing about this relationship is that no assumptions have been made; this is true for all probabilities.\nTo demonstrate this, we will use a real dataset. This dataset comes from U.S. census reports from 2011 to 2015, and gives information about household income along with location, state, zip code, etc. It was downloaded from Kaggle. Let’s visualize the first few data points:\n\ndf = pd.read_csv('kaggle_income.csv', encoding='windows-1252')\ndf.head()\n\n\n\n\n\n\n\n\nid\nState_Code\nState_Name\nState_ab\nCounty\nCity\nPlace\nType\nPrimary\nZip_Code\nArea_Code\nALand\nAWater\nLat\nLon\nMean\nMedian\nStdev\nsum_w\n\n\n\n\n0\n1011000\n1\nAlabama\nAL\nMobile County\nChickasaw\nChickasaw city\nCity\nplace\n36611\n251\n10894952\n909156\n30.771450\n-88.079697\n38773\n30506\n33101\n1638.260513\n\n\n1\n1011010\n1\nAlabama\nAL\nBarbour County\nLouisville\nClio city\nCity\nplace\n36048\n334\n26070325\n23254\n31.708516\n-85.611039\n37725\n19528\n43789\n258.017685\n\n\n2\n1011020\n1\nAlabama\nAL\nShelby County\nColumbiana\nColumbiana city\nCity\nplace\n35051\n205\n44835274\n261034\n33.191452\n-86.615618\n54606\n31930\n57348\n926.031000\n\n\n3\n1011030\n1\nAlabama\nAL\nMobile County\nSatsuma\nCreola city\nCity\nplace\n36572\n251\n36878729\n2374530\n30.874343\n-88.009442\n63919\n52814\n47707\n378.114619\n\n\n4\n1011040\n1\nAlabama\nAL\nMobile County\nDauphin Island\nDauphin Island\nTown\nplace\n36528\n251\n16204185\n413605152\n30.250913\n-88.171268\n77948\n67225\n54270\n282.320328\n\n\n\n\n\n\n\nThere’s quite a bit of data here to look at. Let’s see if we can plot this somehow:\n\ndf.plot(kind=\"scatter\", x=\"Lon\", y=\"Lat\", grid=True)\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.show()\n\n\n\n\n\n\n\n\nWe can clearly see that this dataset has data from Alaska, Hawaii, and even Puerto Rico. We can see all the different parameters and their data types here:\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 32526 entries, 0 to 32525\nData columns (total 19 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   id          32526 non-null  int64  \n 1   State_Code  32526 non-null  int64  \n 2   State_Name  32526 non-null  object \n 3   State_ab    32526 non-null  object \n 4   County      32526 non-null  object \n 5   City        32526 non-null  object \n 6   Place       32526 non-null  object \n 7   Type        32526 non-null  object \n 8   Primary     32526 non-null  object \n 9   Zip_Code    32526 non-null  int64  \n 10  Area_Code   32526 non-null  object \n 11  ALand       32526 non-null  int64  \n 12  AWater      32526 non-null  int64  \n 13  Lat         32526 non-null  float64\n 14  Lon         32526 non-null  float64\n 15  Mean        32526 non-null  int64  \n 16  Median      32526 non-null  int64  \n 17  Stdev       32526 non-null  int64  \n 18  sum_w       32526 non-null  float64\ndtypes: float64(3), int64(8), object(8)\nmemory usage: 4.7+ MB\n\n\nFor the object types, we can see what different values they can take like this:\n\ndf[\"State_Name\"].value_counts()\n\nState_Name\nCalifornia              3280\nTexas                   2300\nNew York                2160\nFlorida                 1661\nPennsylvania            1475\nIllinois                1431\nOhio                    1349\nMichigan                1174\nNorth Carolina           915\nNew Jersey               888\nGeorgia                  818\nVirginia                 785\nIndiana                  704\nMissouri                 700\nWashington               688\nWisconsin                674\nMassachusetts            670\nMinnesota                641\nArizona                  617\nTennessee                610\nMaryland                 586\nAlabama                  526\nColorado                 526\nLouisiana                511\nKentucky                 497\nOklahoma                 480\nSouth Carolina           459\nIowa                     454\nOregon                   387\nKansas                   381\nPuerto Rico              380\nConnecticut              355\nArkansas                 340\nMississippi              321\nNebraska                 275\nNevada                   265\nUtah                     261\nWest Virginia            248\nNew Mexico               240\nMaine                    157\nIdaho                    149\nMontana                  148\nHawaii                   138\nNew Hampshire            131\nSouth Dakota             128\nNorth Dakota             119\nRhode Island             109\nAlaska                   105\nDelaware                  88\nVermont                   83\nWyoming                   75\nDistrict of Columbia      64\nName: count, dtype: int64\n\n\nCalifornia has the highest number of data points (no surprise there), while D.C. has the fewest. Now that we’ve had a look around, let’s focus on Virginia:\n\nvirginia = df.copy()\nvirginia = virginia.drop([\"id\", \"State_Code\", \"Zip_Code\", \"State_ab\", \"Area_Code\", \"ALand\", \"AWater\"], axis=1)\nvirginia = virginia[virginia[\"State_Name\"] == \"Virginia\"]\nvirginia = virginia[virginia[\"Type\"] == \"Track\"]\nvirginia.describe()\n\n\n\n\n\n\n\n\nLat\nLon\nMean\nMedian\nStdev\nsum_w\n\n\n\n\ncount\n724.000000\n724.000000\n724.000000\n724.000000\n724.000000\n724.000000\n\n\nmean\n37.751752\n-77.849723\n79952.308011\n105208.250000\n51075.248619\n434.758474\n\n\nstd\n0.835956\n1.512712\n39089.677211\n95940.593647\n16825.048860\n358.204421\n\n\nmin\n36.581972\n-83.106093\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n36.983112\n-78.518816\n52039.500000\n41727.250000\n39309.500000\n201.326579\n\n\n50%\n37.519565\n-77.420987\n70964.500000\n63942.000000\n51219.500000\n327.972570\n\n\n75%\n38.736408\n-77.057345\n101269.000000\n119256.250000\n64484.250000\n511.870572\n\n\nmax\n39.345172\n-75.503313\n205835.000000\n300000.000000\n94395.000000\n2328.854201\n\n\n\n\n\n\n\nI took the liberty of removing some of the information that I’m not planning on using, such as the state abbreviations, zip codes, and others. I also narrowed the location types to “Track” since it is the most narrow result you can get in this dataset.\nWe can see some important values, like the maximum and minimum values for housing number and average household income. Let’s plot this to get a better look:\n\nvirginia.plot(kind=\"scatter\", x=\"Lon\", y=\"Lat\", grid=True, s=virginia[\"sum_w\"] / 15, c=\"Mean\", cmap=\"jet\", colorbar=True, figsize=(4.8 * (8 / 3), 4.8))\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.show()\n\n\n\n\nLooking at this graph, the number of houses is represented by the size of the data point, and the average household income is represented by the color. We can see that the average household income seems to increase the more Northeast that you go. Let’s plot all points that are above the average of the average household incomes, which is around \\(\\$80,000\\):\n\naboveMeans = virginia[virginia[\"Mean\"] &gt; virginia[\"Mean\"].mean()]\n\naboveMeans.plot(kind=\"scatter\", x=\"Lon\", y=\"Lat\", grid=True, s=aboveMeans[\"sum_w\"] / 15, c=\"Mean\", cmap=\"jet\", colorbar=True, figsize=(4.8 * (8 / 3), 4.8))\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.show()\n\n\n\n\nUnfortunately, this data set does not contain information about the population densities of these locations. However, a quick Google search shows that there are roughly \\(3\\) people per household in Virginia, so let’s estimate the population of Virginia:\n\nint(round(sum(virginia[\"sum_w\"] * 3), 0))\n\n944295\n\n\nThis is about \\(\\frac{1}{8}\\) of the actual population, but the probabilities that we will calculate using this will not be too far from the truth.\nLet’s do some calculations now. Let’s find the conditional probability of making more than the average household income given that you are living on the right side of the midline of Virginia:\n\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\]\n\nmidLong = (virginia[\"Lon\"].max() + virginia[\"Lon\"].min()) / 2\nmidLong\n\n-79.304703\n\n\n\nrightSideVirginia = virginia[virginia[\"Lon\"] &gt; midLong]\naboveMeanRightSideVirginia = rightSideVirginia[rightSideVirginia[\"Mean\"] &gt; rightSideVirginia[\"Mean\"].mean()]\nprobAGivenB = sum(aboveMeanRightSideVirginia[\"sum_w\"]) / sum(rightSideVirginia[\"sum_w\"])\n\nprint(\"{}%\".format(round(probAGivenB * 100, 2)))\n\n30.21%\n\n\nThis means that there are enough locations on the right side of Virginia making below average income to bring this probability down, even though the red-colored areas are making almost double the average household income.\nLet’s reverse our question now. What is the conditional probability that someone is living on the right side of the midline of Virginia given that their average household income is above the average? Let’s finally use Bayes’ Rule!\n\\[P(B|A) = \\frac{P(A|B)P(B)}{P(A)}\\]\n\nprobA = sum(aboveMeans[\"sum_w\"]) / sum(virginia[\"sum_w\"])\nprobB = sum(rightSideVirginia[\"sum_w\"]) / sum(virginia[\"sum_w\"])\nprobBGivenA = (probAGivenB * probB) / probA\n\nprint(\"{}%\".format(round(probBGivenA * 100, 2)))\n\n84.33%\n\n\nA much different answer came out. This shows that you’d have a much easier time guessing where someone lives based on their income, rather than guessing someone’s income based on where they live.\nLet’s verify our answer using regular conditional probability, just to be sure that Bayes didn’t lie to us:\n\\[P(B|A) = \\frac{P(B \\cap A)}{P(A)}\\]\n\notherProbBGivenA = sum(aboveMeanRightSideVirginia[\"sum_w\"]) / sum(aboveMeans[\"sum_w\"])\nprint(\"{}%\".format(round(otherProbBGivenA * 100, 2)))\n\n84.33%\n\n\nSure enough, Bayes was right!"
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Welcome to my fourth blog post!\nIn the last post, we covered a topic known as “Classification”, which involves determining which class (from a set of discrete classes) that a given set of features belongs to. In this post, we will cover a similar topic, known as Clustering. Before we get into it, though, we need to cover some terminology, so let’s get into it.\n\n\nTypes of Machine Learning\nThere are two main types of machine learning: supervised learning and unsupervised learning. Let’s talk about supervised learning first.\nSupervised learning occurs when a there are input features and output responses. We’ve already covered two different types of machine learning concepts: regression, and classification. For regression, there are input features, which are usually numbers, and output responses, which are also numbers. The model that we generate for regression purposes determines the relationship between the training data’s features and output responses and creates a continuous function that can later be used to estimate an output response for a set of sample input features. Classification also has input features and output responses, but the difference is that the output responses are discrete classes.\nUnsupervised learning occurs when there are input features, but no output responses. In this case, the model has no way of relating input features to output responses (since they don’t exist) and therefore cannot perform regression or classification. However, that does not mean that nothing can be discovered from the input features. Clustering is the process of finding different input features that are related to one another. Unsurprisingly, these related features usually form “clusters”, and clustering models can determine this pattern. This gives insight into the data without having any output responses at all. Even more useful is the fact that once the model has determined the clustering pattern between the input features, a data scientist can give labels to the clusters, and the model knows to apply those labels to any new or old data that fits into a cluster.\nClustering models are also much different from the supervised learning models that we’ve looked at so far regarding the performance of different algorithms. For example, depending on the dataset, Logistic Regression will not produce results that are very different from, say, Decision Trees. When it comes to clustering, different algorithms may perform very differently, so this post will focus more on trying different models and seeing how they perform, rather than the theory behind them.\n\n\nThe Dataset\nThis time we’ll be using a cute little dataset known as the “Palmer Penguin Dataset”, which can be downloaded from Kaggle. This dataset contains information about the island location, body mass, flipper length, culmen (beak) length and depth, and sex of different penguins as features. The output responses correspond to the species of penguin, and they can be one of three: Chinstrap, Gentoo, and Adélie. To motivate us, here are some photos of each of the species:\n\n\n\n                              Chinstrap                                                              Gentoo                                                                        Adélie\n\n\nNow that we’ve seen them, let’s take a look at the data:\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndf = pd.read_csv('penguins.csv', encoding='windows-1252')\ndf.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nculmen_length_mm\nculmen_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMALE\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n\n\n\n\n\n\n\nThis dataset is nice and simple! Now, using clustering on this data isn’t going to help too much (since we already know that there are three classes), but it will still be nice to use this data to learn the basics.\nLet’s use the fact that we know that there are three classes to plot them and immediately distinguish the clusters! First, let’s convert the output classes into numbers. We’ll set Chinstrap = 0, Gentoo = 1, and Adélie = 2:\n\narr = [0 if element == \"Chinstrap\" else 1 if element == \"Gentoo\" else 2 for element in df['species']]\ndf.insert(7, \"species_number\", arr)\n\nNow, let’s assume that the two best determining factors of the penguin species is body mass and flipper length. Let’s try plotting these against each other and use different colors for each species:\n\n# Removing NaN values\ndf.dropna(subset=[\"body_mass_g\"], inplace=True)\ndf.dropna(subset=[\"flipper_length_mm\"], inplace=True)\n\nX1 = df['body_mass_g']\nX2 = df['flipper_length_mm']\ny = df['species_number']\n\nplt.figure(figsize=(9, 3.5))\n\nplt.subplot(121)\nplt.plot(X1[y==0], X2[y==0], \"yo\", label=\"Chinstrap\")\nplt.plot(X1[y==1], X2[y==1], \"bs\", label=\"Gentoo\")\nplt.plot(X1[y==2], X2[y==2], \"g^\", label=\"Adélie\")\nplt.xlabel(\"Body Mass\")\nplt.ylabel(\"Flipper Length (mm)\")\nplt.grid()\nplt.legend()\n\nplt.subplot(122)\nplt.scatter(X1, X2, c=\"k\", marker=\".\")\nplt.xlabel(\"Body Mass\")\nplt.tick_params(labelleft=False)\nplt.gca().set_axisbelow(True)\nplt.grid()\n\nplt.show()\n\n\n\n\n\n\n\n\nWhat a nice plot! The left plot shows us that Gentoo penguins are very unique in their combination of flipper length and body mass, while Chinstrap and Adélie are harder to tell apart. The right graph shows us what the data points would look like if we didn’t know the classes ahead of time. Notice that it is substantially harder to determine the groupings without the colors.\nLet’s see if we can find a nicer separation between Chinstrap and Adélie penguins by including one more feature and plotting in 3D:\n\n# Removing NaN values\ndf.dropna(subset=[\"culmen_length_mm\"], inplace=True)\n\nX1 = df['body_mass_g']\nX2 = df['flipper_length_mm']\nX3 = df['culmen_length_mm']\ny = df['species_number']\n\nfig = plt.figure(figsize=(12, 12))\nax = fig.add_subplot(projection='3d')\n\nplt.plot(X1[y==0], X2[y==0], X3[y==0], \"yo\", label=\"Chinstrap\")\nplt.plot(X1[y==1], X2[y==1], X3[y==1], \"bs\", label=\"Gentoo\")\nplt.plot(X1[y==2], X2[y==2], X3[y==2], \"g^\", label=\"Adélie\")\nplt.xlabel(\"Body Mass\")\nplt.ylabel(\"Flipper Length (mm)\")\nax.set_zlabel('Culmen Length (mm)')\nplt.grid()\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\nThis gives us a hint that the culmen length might be very important in discerning the different species. However, since these algorithms are best displayed in 2D, it would be best to try different combinations of features and see which ones creates the best clusters.\nAfter some testing, the best features to use are flipper_length_mm and culmen_length-mm:\n\nX1 = df['culmen_length_mm']\nX2 = df['flipper_length_mm']\ny = df['species_number']\n\nplt.figure(figsize=(9, 3.5))\n\nplt.subplot(121)\nplt.plot(X1[y==0], X2[y==0], \"yo\", label=\"Chinstrap\")\nplt.plot(X1[y==1], X2[y==1], \"bs\", label=\"Gentoo\")\nplt.plot(X1[y==2], X2[y==2], \"g^\", label=\"Adélie\")\nplt.xlabel(\"Culmen Length (mm)\")\nplt.ylabel(\"Flipper Length (mm)\")\nplt.grid()\nplt.legend()\n\nplt.subplot(122)\nplt.scatter(X1, X2, c=\"k\", marker=\".\")\nplt.xlabel(\"Culmen Length (mm)\")\nplt.tick_params(labelleft=False)\nplt.gca().set_axisbelow(True)\nplt.grid()\n\nplt.show()\n\n\n\n\n\n\n\n\nLooking at the plot on the right, it is much easier to discern the three clusters than before. Now that we’ve chosen which features separate the data the best, we can get into the different algorithms.\n\n\nK-Means Clustering\nK-Means Clustering works a lot like regression with a Mean Square Error. You supply it with the number of clusters, and it seeks to minimize the squared Euclidean distances between the centroids of the clusters and the rest of the points inside of the clusters. It is known as a Centroid-based Clustering algorithm. Let’s try it out:\n\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\nX = np.empty((342, 2))\nX[:, 0] = df['culmen_length_mm']\nX[:, 1] = df['flipper_length_mm']\n\nkmeans = KMeans(n_clusters=3, random_state=0, n_init=\"auto\").fit(X)\n\ndef plot_decision_boundaries(clusterer, X, resolution=500):\n    \n    mins = X.min(axis=0) - 0.1\n    maxs = X.max(axis=0) + 0.1\n    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n                         np.linspace(mins[1], maxs[1], resolution))\n    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n                cmap=\"Pastel2\")\n    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n                linewidths=1, colors='k')\n    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n\n    plt.xlabel(\"Culmen Length (mm)\")\n    plt.ylabel(\"Flipper Length (mm)\", rotation=90)\n\nplt.figure(figsize=(4.5, 3.5))\nplot_decision_boundaries(kmeans, X)\nplt.show()\n\n\n\n\n\n\n\n\nK-Means didn’t do terribly, but it didn’t do well either. It is clearly able to see the distinction between Gentoo and the other species, but it has a hard time finding the best way to separate Chinstrap from Adélie. Let’s look at another clustering algorithm.\n\n\nDBSCAN\nDBSCAN stands for Density-Based Spatial Clustering of Applications with Noise and is a very popular clustering algorithm. DBSCAN is a Density-based Clustering algorithm, which means it focuses more on the density of collections of points rather than their distances from some centroid. Let’s take a look at how it performs:\n\nfrom sklearn.cluster import DBSCAN\n\ndbscan = DBSCAN(eps=3, min_samples=4)\ndbscan.fit(X)\n\ndef plot_dbscan(dbscan, X, size):\n    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n    core_mask[dbscan.core_sample_indices_] = True\n    anomalies_mask = dbscan.labels_ == -1\n    non_core_mask = ~(core_mask | anomalies_mask)\n\n    cores = dbscan.components_\n    anomalies = X[anomalies_mask]\n    non_cores = X[non_core_mask]\n    \n    plt.scatter(cores[:, 0], cores[:, 1],\n                c=dbscan.labels_[core_mask], marker='o', s=size, cmap=\"Paired\")\n    plt.scatter(cores[:, 0], cores[:, 1], marker='*', s=20,\n                c=dbscan.labels_[core_mask])\n    plt.scatter(anomalies[:, 0], anomalies[:, 1],\n                c=\"r\", marker=\"x\", s=100)\n    plt.scatter(non_cores[:, 0], non_cores[:, 1],\n                c=dbscan.labels_[non_core_mask], marker=\".\")\n    plt.xlabel(\"Culmen Length (mm)\")\n    plt.ylabel(\"Flipper Length (mm)\", rotation=90)\n    plt.title(f\"eps={dbscan.eps:.2f}, min_samples={dbscan.min_samples}\")\n    plt.grid()\n    plt.gca().set_axisbelow(True)\n\nplt.figure(figsize=(4.5, 3.2))\nplot_dbscan(dbscan, X, size=100)\nplt.show()\n\n\n\n\n\n\n\n\nAgain, we can see that the clustering algorithm is easily able to separate Gentoo from the rest, but Chinstrap and Adélie are similar enough that it can’t discern between them. DBSCAN also has an interesting secondary use in that it can spot anomalies, marked in the plots with red “x”s.\nLet’s try replacing Flipper Length with Culmen Depth and seeing what we can produce:\n\nX = np.empty((342, 2))\nX[:, 0] = df['culmen_length_mm']\nX[:, 1] = df['culmen_depth_mm']\n\ndbscan = DBSCAN(eps=0.84, min_samples=4)\ndbscan.fit(X)\n\ndef plot_dbscan(dbscan, X, size):\n    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n    core_mask[dbscan.core_sample_indices_] = True\n    anomalies_mask = dbscan.labels_ == -1\n    non_core_mask = ~(core_mask | anomalies_mask)\n\n    cores = dbscan.components_\n    anomalies = X[anomalies_mask]\n    non_cores = X[non_core_mask]\n    \n    plt.scatter(cores[:, 0], cores[:, 1],\n                c=dbscan.labels_[core_mask], marker='o', s=size, cmap=\"Paired\")\n    plt.scatter(cores[:, 0], cores[:, 1], marker='*', s=20,\n                c=dbscan.labels_[core_mask])\n    plt.scatter(anomalies[:, 0], anomalies[:, 1],\n                c=\"r\", marker=\"x\", s=100)\n    plt.scatter(non_cores[:, 0], non_cores[:, 1],\n                c=dbscan.labels_[non_core_mask], marker=\".\")\n    plt.xlabel(\"Culmen Length (mm)\")\n    plt.ylabel(\"Culmen Depth (mm)\", rotation=90)\n    plt.title(f\"eps={dbscan.eps:.2f}, min_samples={dbscan.min_samples}\")\n    plt.grid()\n    plt.gca().set_axisbelow(True)\n    \n\nplt.figure(figsize=(9, 3.2))\n\nplt.subplot(121)\nX1 = df['culmen_length_mm']\nX2 = df['culmen_depth_mm']\nplt.plot(X1[y==0], X2[y==0], \"yo\", label=\"Chinstrap\")\nplt.plot(X1[y==1], X2[y==1], \"bs\", label=\"Gentoo\")\nplt.plot(X1[y==2], X2[y==2], \"g^\", label=\"Adélie\")\nplt.xlabel(\"Culmen Length (mm)\")\nplt.ylabel(\"Culmen Depth (mm)\")\nplt.grid()\nplt.legend()\n\nplt.subplot(122)\nplot_dbscan(dbscan, X, size=100)\nplt.show()\n\n\n\n\n\n\n\n\nFinally, our model finds the three clusters pretty well! Unfortunately, the data contains a lot of outliers and causes the model to have a hard time discerning which features belong to which cluster, but these figures still give us hints as to how many possible classifications there might be.\nHowever, this is the perfect stopping point to lead us into the next and final blog post: Anomaly Detection."
  },
  {
    "objectID": "posts/Anomalies/index.html",
    "href": "posts/Anomalies/index.html",
    "title": "Anomaly Detection",
    "section": "",
    "text": "We have now arrived at the fifth and final blog post (as of now). This last one will be shorter, but we will use most of what we’ve learned so far to do anomaly detection. Let’s get started."
  },
  {
    "objectID": "posts/Anomalies/index.html#the-dataset",
    "href": "posts/Anomalies/index.html#the-dataset",
    "title": "Anomaly Detection",
    "section": "The Dataset",
    "text": "The Dataset\nLet’s jump right into the dataset. It was downloaded from Hugging Face and contains data about the yearly measurements of ocean temperature in different regions of the world.\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndf = pd.read_csv('temperature.csv', encoding='windows-1252')\ndf.head()\n\n\n\n\n\n\n\n\nEntity\nCode\nYear\nGlobal average temperature anomaly relative to 1961-1990\nUpper bound (95% confidence interval) of the annual temperature anomaly\nLower bound (95% confidence interval) of the annual temperature anomaly\n\n\n\n\n0\nGlobal\nNaN\n1850\n-0.417659\n-0.246115\n-0.589203\n\n\n1\nGlobal\nNaN\n1851\n-0.233350\n-0.054832\n-0.411868\n\n\n2\nGlobal\nNaN\n1852\n-0.229399\n-0.049416\n-0.409382\n\n\n3\nGlobal\nNaN\n1853\n-0.270354\n-0.110700\n-0.430009\n\n\n4\nGlobal\nNaN\n1854\n-0.291630\n-0.150436\n-0.432824\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 519 entries, 0 to 518\nData columns (total 6 columns):\n #   Column                                                                   Non-Null Count  Dtype  \n---  ------                                                                   --------------  -----  \n 0   Entity                                                                   519 non-null    object \n 1   Code                                                                     0 non-null      float64\n 2   Year                                                                     519 non-null    int64  \n 3   Global average temperature anomaly relative to 1961-1990                 519 non-null    float64\n 4   Upper bound (95% confidence interval) of the annual temperature anomaly  519 non-null    float64\n 5   Lower bound (95% confidence interval) of the annual temperature anomaly  519 non-null    float64\ndtypes: float64(4), int64(1), object(1)\nmemory usage: 24.5+ KB\n\n\n\ndf['Entity'].value_counts()\n\nEntity\nGlobal                 173\nNorthern Hemisphere    173\nSouthern Hemisphere    173\nName: count, dtype: int64\n\n\nLet’s do a little exploring first:\n\nX = df['Year']\nupper = df['Upper bound (95% confidence interval) of the annual temperature anomaly']\nlower = df['Lower bound (95% confidence interval) of the annual temperature anomaly']\naverage = df['Global average temperature anomaly relative to 1961-1990']\n\nplt.figure(figsize=(4.5, 12))\nplt.subplot(311)\nplt.plot(X[df['Entity'] == \"Global\"], average[df['Entity'] == \"Global\"], \"y\", label=\"Average\", alpha=0.5)\nplt.plot(X[df['Entity'] == \"Global\"], upper[df['Entity'] == \"Global\"], \"b\", label=\"Upper Bound\", alpha=0.5)\nplt.plot(X[df['Entity'] == \"Global\"], lower[df['Entity'] == \"Global\"], \"g\", label=\"LowerUpper Bound\", alpha=0.5)\nplt.title('Global')\nplt.legend()\nplt.grid()\nplt.subplot(312)\nplt.plot(X[df['Entity'] == \"Northern Hemisphere\"], average[df['Entity'] == \"Northern Hemisphere\"], \"y\", label=\"Average\", alpha=0.5)\nplt.plot(X[df['Entity'] == \"Northern Hemisphere\"], upper[df['Entity'] == \"Northern Hemisphere\"], \"b\", label=\"Upper Bound\", alpha=0.5)\nplt.plot(X[df['Entity'] == \"Northern Hemisphere\"], lower[df['Entity'] == \"Northern Hemisphere\"], \"g\", label=\"LowerUpper Bound\", alpha=0.5)\nplt.title('Northern Hemisphere')\nplt.legend()\nplt.grid()\nplt.subplot(313)\nplt.plot(X[df['Entity'] == \"Southern Hemisphere\"], average[df['Entity'] == \"Southern Hemisphere\"], \"y\", label=\"Average\", alpha=0.5)\nplt.plot(X[df['Entity'] == \"Southern Hemisphere\"], upper[df['Entity'] == \"Southern Hemisphere\"], \"b\", label=\"Upper Bound\", alpha=0.5)\nplt.plot(X[df['Entity'] == \"Southern Hemisphere\"], lower[df['Entity'] == \"Southern Hemisphere\"], \"g\", label=\"LowerUpper Bound\", alpha=0.5)\nplt.title('Southern Hemisphere')\nplt.grid()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nWe can see obvious trends, but it is clear that there are some extremely large jumps and dips for certain years. It definitely seems like the Southern Hemisphere has the most action. Let’s plot the upper bound versus the lower bound this time:\n\nX = df['Lower bound (95% confidence interval) of the annual temperature anomaly']\ny = df['Upper bound (95% confidence interval) of the annual temperature anomaly']\n\nplt.figure(figsize=(4.5, 12))\nplt.subplot(311)\nplt.plot(X[df['Entity'] == \"Global\"], y[df['Entity'] == \"Global\"], \"o\", alpha=0.5)\nplt.title('Global')\nplt.grid()\nplt.subplot(312)\nplt.plot(X[df['Entity'] == \"Northern Hemisphere\"], y[df['Entity'] == \"Northern Hemisphere\"], \"o\", alpha=0.5)\nplt.title('Northern Hemisphere')\nplt.grid()\nplt.subplot(313)\nplt.plot(X[df['Entity'] == \"Southern Hemisphere\"], y[df['Entity'] == \"Southern Hemisphere\"], \"o\", alpha=0.5)\nplt.title('Southern Hemisphere')\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nSure enough, the Southern Hemisphere seems to have quite a few years where the upper and lower bounds are wildly different from what they usually are. Let’s use DBSCAN to see if we can find clusters and anomalies:\n\nfrom sklearn.cluster import DBSCAN\nimport numpy as np\n\nX = np.empty((173, 2))\nX1 = df['Lower bound (95% confidence interval) of the annual temperature anomaly']\nX2 = df['Upper bound (95% confidence interval) of the annual temperature anomaly']\nX[:, 0] = X1[df['Entity'] == \"Southern Hemisphere\"]\nX[:, 1] = X2[df['Entity'] == \"Southern Hemisphere\"]\n\ndbscan = DBSCAN(eps=0.1, min_samples=6)\ndbscan.fit(X)\n\ndef plot_dbscan(dbscan, X, size):\n    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n    core_mask[dbscan.core_sample_indices_] = True\n    anomalies_mask = dbscan.labels_ == -1\n    non_core_mask = ~(core_mask | anomalies_mask)\n\n    cores = dbscan.components_\n    anomalies = X[anomalies_mask]\n    non_cores = X[non_core_mask]\n    \n    plt.scatter(cores[:, 0], cores[:, 1],\n                c=dbscan.labels_[core_mask], marker='o', s=size, cmap=\"Paired\")\n    plt.scatter(cores[:, 0], cores[:, 1], marker='*', s=20,\n                c=dbscan.labels_[core_mask])\n    plt.scatter(anomalies[:, 0], anomalies[:, 1],\n                c=\"r\", marker=\"x\", s=100)\n    plt.scatter(non_cores[:, 0], non_cores[:, 1],\n                c=dbscan.labels_[non_core_mask], marker=\".\")\n    plt.grid()\n    plt.gca().set_axisbelow(True)\n\nplot_dbscan(dbscan, X, size=100)\nplt.show()\n\n\n\n\n\n\n\n\nAnd there we have it! We were easily able to classify the most obviously out-of-place data points as anomalies using DBSCAN. At this point, the data scientist could remove the corresponding data to keep any trained models accurate, or leave it in and inform users of the data set that these exist."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! My name is Samuel Huber. I am a graduate student attending Virginia Tech for my Master’s degree in Computer Science with a specificied track toward machine learning and artificial intelligence. I graduated summa cum laude with a Bachelor’s degree in Aerospace Engineering and I am interested in all things science and technology related.\nAs of now, this blog will contain five posts about different machine learning topics. These posts should be read in order, top to bottom, and are written as a sort of pocket-sized “learning journey” for introducing new topics to interested individuals (such as yourself). Hopefully these posts will aid in your understanding of machine learning, or at least pique your interest."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Samuel Huber’s Blog",
    "section": "",
    "text": "Probability Theory and Random Variables\n\n\n\n\n\nThings are about to get Bayesian…\n\n\n\n\n\n\nDec 3, 2023\n\n\nSamuel Huber\n\n\n\n\n\n\n  \n\n\n\n\nLinear and Nonlinear Regression\n\n\n\n\n\nRegressive: An insurance company you can’t rely on.\n\n\n\n\n\n\nDec 3, 2023\n\n\nSamuel Huber\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\nThe information in this post is classified.\n\n\n\n\n\n\nDec 3, 2023\n\n\nSamuel Huber\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\nYou might get a headache…\n\n\n\n\n\n\nDec 3, 2023\n\n\nSamuel Huber\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly Detection\n\n\n\n\n\nWhat’s an ‘omaly’, and why are we detecting it?\n\n\n\n\n\n\nDec 3, 2023\n\n\nSamuel Huber\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "In the last post, we discussed the basics of Linear Regression and showed an example of how it could be used to estimate a given value based on input parameters. In this section, we’ll talk about a similar concept known as Classification.\nA model that is used for classification uses input features to predict a class. The class is a selection from a discrete set of classes. For example, a classification model could be trained for identifying whether an online store review is positive or negative. Another example is classifying a person’s facial expression as happy, sad, bored, scared, and so on. Classification can be used for many unique and useful purposes, and luckily, the metrics to determine the performance of a classification model are very simple.\nHowever, there is one large difference between what we have seen so far for regression and what we will see for classification. In the last post, we looked at Linear Regression and Nonlinear Regression, and as it turns out, these are the main forms that are used. There are a few others (Ridge Regression, Lasso Regression, etc.), but they are very nuanced and have subtle differences. Classification, on the other hand, has many different algorithms that all have their own positives and negatives. Therefore, this post will look less at the mathematical theory and more at the applications and usage of each of the popular classification model types."
  },
  {
    "objectID": "posts/Classification/index.html#the-dataset",
    "href": "posts/Classification/index.html#the-dataset",
    "title": "Classification",
    "section": "The Dataset",
    "text": "The Dataset\nThe simplest type of classification is, unsurprisingly, Binary Classification. However, as we will discover later, classification for multiple output classes is very similar to binary classification.\nBefore we get too deep into the theory though, let’s talk about our dataset. We will be using the “Breast Cancer Wisconsin” dataset, which contains measurements of breast cancer tumors and a classification based on if they’re malignant or benign. The data was obtained using digitized imagery from a process known as fine needle aspiratation. More details about the data collection can be found in the following paper:\n\nStreet, W. Nick, William H. Wolberg, and Olvi L. Mangasarian. “Nuclear feature extraction for breast tumor diagnosis.” Biomedical image processing and biomedical visualization. Vol. 1905. SPIE, 1993.\n\nwhich can be accessed here.\nInterestingly, the paper itself also contains a classification model that performs very well. What is even more impressive is that this was accomplished in 1992!\nThe dataset is obtained from Kaggle, and contains information such as the average radius, area, smoothness, symmetry, and a few other unique values. The page also contains a link to the UCI Machine Learning Repository, which shows graphs of the performance of different classification models, all of which surpass \\(90\\%\\) accuracy.\nJust like the previous et’s take a look at the data:\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndf = pd.read_csv('cancer.csv', encoding='windows-1252')\ndf.head()\n\n\n\n\n\n\n\n\nid\ndiagnosis\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\n...\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\nUnnamed: 32\n\n\n\n\n0\n842302\nM\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n...\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\nNaN\n\n\n1\n842517\nM\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n...\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\nNaN\n\n\n2\n84300903\nM\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n...\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\nNaN\n\n\n3\n84348301\nM\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n...\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\nNaN\n\n\n4\n84358402\nM\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n...\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\nNaN\n\n\n\n\n5 rows × 33 columns\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 569 entries, 0 to 568\nData columns (total 33 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   id                       569 non-null    int64  \n 1   diagnosis                569 non-null    object \n 2   radius_mean              569 non-null    float64\n 3   texture_mean             569 non-null    float64\n 4   perimeter_mean           569 non-null    float64\n 5   area_mean                569 non-null    float64\n 6   smoothness_mean          569 non-null    float64\n 7   compactness_mean         569 non-null    float64\n 8   concavity_mean           569 non-null    float64\n 9   concave points_mean      569 non-null    float64\n 10  symmetry_mean            569 non-null    float64\n 11  fractal_dimension_mean   569 non-null    float64\n 12  radius_se                569 non-null    float64\n 13  texture_se               569 non-null    float64\n 14  perimeter_se             569 non-null    float64\n 15  area_se                  569 non-null    float64\n 16  smoothness_se            569 non-null    float64\n 17  compactness_se           569 non-null    float64\n 18  concavity_se             569 non-null    float64\n 19  concave points_se        569 non-null    float64\n 20  symmetry_se              569 non-null    float64\n 21  fractal_dimension_se     569 non-null    float64\n 22  radius_worst             569 non-null    float64\n 23  texture_worst            569 non-null    float64\n 24  perimeter_worst          569 non-null    float64\n 25  area_worst               569 non-null    float64\n 26  smoothness_worst         569 non-null    float64\n 27  compactness_worst        569 non-null    float64\n 28  concavity_worst          569 non-null    float64\n 29  concave points_worst     569 non-null    float64\n 30  symmetry_worst           569 non-null    float64\n 31  fractal_dimension_worst  569 non-null    float64\n 32  Unnamed: 32              0 non-null      float64\ndtypes: float64(31), int64(1), object(1)\nmemory usage: 146.8+ KB\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nid\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\nsymmetry_mean\n...\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\nUnnamed: 32\n\n\n\n\ncount\n5.690000e+02\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n...\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n0.0\n\n\nmean\n3.037183e+07\n14.127292\n19.289649\n91.969033\n654.889104\n0.096360\n0.104341\n0.088799\n0.048919\n0.181162\n...\n25.677223\n107.261213\n880.583128\n0.132369\n0.254265\n0.272188\n0.114606\n0.290076\n0.083946\nNaN\n\n\nstd\n1.250206e+08\n3.524049\n4.301036\n24.298981\n351.914129\n0.014064\n0.052813\n0.079720\n0.038803\n0.027414\n...\n6.146258\n33.602542\n569.356993\n0.022832\n0.157336\n0.208624\n0.065732\n0.061867\n0.018061\nNaN\n\n\nmin\n8.670000e+03\n6.981000\n9.710000\n43.790000\n143.500000\n0.052630\n0.019380\n0.000000\n0.000000\n0.106000\n...\n12.020000\n50.410000\n185.200000\n0.071170\n0.027290\n0.000000\n0.000000\n0.156500\n0.055040\nNaN\n\n\n25%\n8.692180e+05\n11.700000\n16.170000\n75.170000\n420.300000\n0.086370\n0.064920\n0.029560\n0.020310\n0.161900\n...\n21.080000\n84.110000\n515.300000\n0.116600\n0.147200\n0.114500\n0.064930\n0.250400\n0.071460\nNaN\n\n\n50%\n9.060240e+05\n13.370000\n18.840000\n86.240000\n551.100000\n0.095870\n0.092630\n0.061540\n0.033500\n0.179200\n...\n25.410000\n97.660000\n686.500000\n0.131300\n0.211900\n0.226700\n0.099930\n0.282200\n0.080040\nNaN\n\n\n75%\n8.813129e+06\n15.780000\n21.800000\n104.100000\n782.700000\n0.105300\n0.130400\n0.130700\n0.074000\n0.195700\n...\n29.720000\n125.400000\n1084.000000\n0.146000\n0.339100\n0.382900\n0.161400\n0.317900\n0.092080\nNaN\n\n\nmax\n9.113205e+08\n28.110000\n39.280000\n188.500000\n2501.000000\n0.163400\n0.345400\n0.426800\n0.201200\n0.304000\n...\n49.540000\n251.200000\n4254.000000\n0.222600\n1.058000\n1.252000\n0.291000\n0.663800\n0.207500\nNaN\n\n\n\n\n8 rows × 32 columns\n\n\n\nNow that we’ve seen some of the data, let’s move on to our first model."
  },
  {
    "objectID": "posts/Classification/index.html#logistic-regression",
    "href": "posts/Classification/index.html#logistic-regression",
    "title": "Classification",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLogistic Regression has a very strong relationship to Linear Regression. In fact, it actually uses the Linear Regression equation in its own equation! To get a feel for how it works, let’s plot some data.\nFirst, we have to convert the output labels into numbers so that we can actually plot them. We will assign a value of 1 to malignant tumors and 0 to benign tumors.\n\narr = [1 if element == \"M\" else 0 for element in df['diagnosis']]\ndf.insert(33, \"diagnosis_number\", arr)\n\nNow, let’s plot the diagnoses against the mean radius:\n\nscatter = df.plot.scatter(x='radius_mean', y='diagnosis_number', alpha=0.5)\n\n\n\n\n\n\n\n\nWe can see that tumors below ~\\(12\\) millimeters are guaranteed to be benign, while tumors above ~\\(18\\) are guaranteed to be malignant. The gray area is in between these two numbers.\nNow, imagine trying to fit a Linear Regression line to this. Of course, it doesn’t make much sense to do this, since our data clearly does not have a linear relationship. But just for fun, let’s imagine that we did. This line would clearly pass through both groupings of values, with each end of the line somewhere near the most dense areas of the data. However, since we’re not trying to make predictions on an output value but rather an output class, we need to modify this line to vary only between \\(0\\) and \\(1\\). The best function to do this with is the sigmoid function:\n\\[f(x) = \\frac{1}{1 + e^{-x}}\\]\nSince we’re modifying the line itself, our the variable in the exponential of our sigmoid is going to be replaced with the equation for the Linear Regression line. If you don’t recall, that was:\n\\[f(x) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3 + \\ldots\\]\nPlacing this into the exponential:\n\\[f(x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3 + \\ldots)}}\\]\nThat’s it! It’s as simple as modifying a Linear Regression line. Of course, there is a lot more theory behind Logistic Regression, but this is essentially all it takes to understand what is happening.\nBefore we get into using it, though, we need to talk about one thing that is very important for classification. In order to measure how well our model is performing, we need to separate our data. The reason for this is that if you train you model on all of the data you have, then you can only test it on data that the model has already been trained on. This means you won’t be able to effectively measure how well your model generalizes to new data. To combot this, the dataset is almost always split into two different sets: the training set, and the test set. The most common ratio to use for split is \\(80/20\\), so we’ll try that out:\n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nX = np.asarray(df['radius_mean']).reshape(-1, 1)\ny = np.asarray(df['diagnosis_number'])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=15)\n\nclf = LogisticRegression(random_state=15).fit(X_train, y_train)\nprint(clf.score(X_train, y_train))\nprint(clf.score(X_test, y_test))\n\n0.8879120879120879\n0.8421052631578947\n\n\nNotice that we trained our model on the training set, and we get around \\(88.8\\%\\) accuracy when we compare it to the training set itself. However, when we compare it to the test set, we only get around \\(84\\%\\). This is why it’s important to have different sets for training and testing. Either way, getting more than 4 out of 5 guesses right isn’t bad at all.\nNow, let’s do a visualization of the Linear Regression line and Logistic Regression curve:\n\nb0 = clf.intercept_[0]\nb1 = clf.coef_[0][0]\n\nplt.scatter(df['radius_mean'], df['diagnosis_number'], alpha=0.5)\nxaxis = np.linspace(14.7, 15.6, 100).reshape(100, 1)\nyaxis = [(element * b1) + b0 for element in xaxis]\nplt.plot(xaxis, yaxis, \"red\")\nplt.show()\n\n\n\n\n\n\n\n\nThere’s the Linear Regression line. Let’s see what it looks like after transforming it:\n\nplt.scatter(df['radius_mean'], df['diagnosis_number'], alpha=0.5)\nxaxis = np.linspace(min(df['radius_mean']), max(df['radius_mean']), 100).reshape(100, 1)\nyaxis = [1 / (1 + np.exp(-1 * ((element * b1) + b0))) for element in xaxis]\nplt.plot(xaxis, yaxis, \"green\")\nplt.show()\n\n\n\n\n\n\n\n\nVery nice! We have a smooth variation from \\(0\\) to \\(1\\), whereas the Linear Regression would continue forever in either direction.\nLet’s try making a prediction!\n\nmeasurement_1 = 12\nmeasurement_2 = 18\nclass1 = clf.predict(np.array([measurement_1]).reshape(1, -1))[0]\nclass2 = clf.predict(np.array([measurement_2]).reshape(1, -1))[0]\n\nprint(\"12 millimeters:\", \"Malignant\" if class1 == 1 else \"Benign\")\nprint(\"18 millimeters:\", \"Malignant\" if class2 == 1 else \"Benign\")\n\n12 millimeters: Benign\n18 millimeters: Malignant\n\n\nIt’s definitely working. But that raises a question: “why not just use linear regression and have the decision be based on the midpoint of the line?” That is a very good question. You could, for example, find the \\(x\\) value that corresponds to \\(y = 0.5\\) on the line and classify any measurement that is greather than that \\(x\\) value as a \\(1\\), and \\(0\\) for any values less than that \\(x\\) value. The thing this misses is the probability. Since Logistic Regression curves vary between \\(0\\) and \\(1\\), the probability of the measurement belonging to a class is given by the curve itself. Here’s an example:\n\nprint(\"Measurement 1:\", measurement_1, \"millimeters\")\nmeasurement1_prob = clf.predict_proba(np.array([measurement_1]).reshape(1, -1))\nmeasurement1_class1_prob = measurement1_prob[0][0]\nmeasurement1_class2_prob = measurement1_prob[0][1]\nprint(\"Probability of being benign:\", round(measurement1_class1_prob * 100, 2), \"%\")\nprint(\"Probability of being malignant:\", round(measurement1_class2_prob * 100, 2), \"%\\n\")\n\nprint(\"Measurement 2:\", measurement_2, \"millimeters\")\nmeasurement2_prob = clf.predict_proba(np.array([measurement_2]).reshape(1, -1))\nmeasurement2_class1_prob = measurement2_prob[0][0]\nmeasurement2_class2_prob = measurement2_prob[0][1]\nprint(\"Probability of being benign:\", round(measurement2_class1_prob * 100, 2), \"%\")\nprint(\"Probability of being malignant:\", round(measurement2_class2_prob * 100, 2), \"%\\n\")\n\nMeasurement 1: 12 millimeters\nProbability of being benign: 95.17 %\nProbability of being malignant: 4.83 %\n\nMeasurement 2: 18 millimeters\nProbability of being benign: 2.57 %\nProbability of being malignant: 97.43 %\n\n\n\nSuper neat, right? Let’s try to find the point that is the hardest to classify:\n\\[\\begin{equation}\n\\begin{split}\n\\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x)}} = 0.5 \\longrightarrow 1 + e^{-(\\beta_0 + \\beta_1x)} = 2 \\longrightarrow \\ln(e^{-(\\beta_0 + \\beta_1x)}) &= \\ln(1)\\\\\n-(\\beta_0 + \\beta_1x) &= 0\\\\\nx &= -\\frac{\\beta_0}{\\beta_1}\n\\end{split}\n\\end{equation}\\]\n\nmeasurement_3 = -b0/b1\nmeasurement_3\n\n14.702869626376605\n\n\nLooking back up at the sigmoidal graph, this definitely seems to be close to the center point. Let’s check out the probabilities:\n\nprint(\"Measurement 3:\", round(measurement_3, 2), \"millimeters\")\nmeasurement3_prob = clf.predict_proba(np.array([measurement_3]).reshape(1, -1))\nmeasurement3_class1_prob = measurement3_prob[0][0]\nmeasurement3_class2_prob = measurement3_prob[0][1]\nprint(\"Probability of being benign:\", round(measurement3_class1_prob * 100, 2), \"%\")\nprint(\"Probability of being malignant:\", round(measurement3_class2_prob * 100, 2), \"%\\n\")\n\nMeasurement 3: 14.7 millimeters\nProbability of being benign: 50.0 %\nProbability of being malignant: 50.0 %\n\n\n\nSure enough, it’s a \\(50/50\\) shot. Just for fun, let’s check the classification that LogisticRegression decides:\n\nprint(\"Malignant\" if clf.predict(np.array([measurement_3]).reshape(1, -1))[0] == 1 else \"Benign\")\n\nBenign"
  },
  {
    "objectID": "posts/Classification/index.html#confusion-matrix",
    "href": "posts/Classification/index.html#confusion-matrix",
    "title": "Classification",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\nNow, let’s move away from the guesswork and check what metrics we have to guage our model’s classification performance. If we had more insight than just raw percentages, we might be able to tune our model for our specific purpose.\nWe’re going to look at something called the confusion matrix:\n\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ny_train_pred = cross_val_predict(clf, X_train, y_train, cv=3)\ncm = confusion_matrix(y_train, y_train_pred)\n\nplt.rc('font', size=9)\nConfusionMatrixDisplay.from_predictions(y_train, y_train_pred)\nplt.show()\n\n\n\n\n\n\n\n\nThe vertical axis represents actual benign or malignant classes, while the horizontal axis represents the classification that our model made. You can see that our model correctly classified \\(263\\) benign tumors and \\(141\\) malignant tumors, but accidentally flagged \\(19\\) benign tumors as malignant, and let \\(32\\) malignant tumors fly under the radar as benign.\nThe bottom right value represents True Positives (TP), while the top left value represents True Negatives (TN). Conversely, the top right value represents False Positives (FP) and the bottom left value represents False Negatives (FN). Here’s another way to look at it:\n\\[ \\begin{bmatrix}\n\\mathrm{True~Negatives} & \\mathrm{False~Positives}\\\\\n\\mathrm{False~Negatives} & \\mathrm{True~Positives}\n\\end{bmatrix} \\]\nThis matrix gives us a good idea of what kind of mistakes our model is making. Now, let’s look at the two main numeric metrics that are used to rate the performance of our model."
  },
  {
    "objectID": "posts/Classification/index.html#precision-and-recall",
    "href": "posts/Classification/index.html#precision-and-recall",
    "title": "Classification",
    "section": "Precision and Recall",
    "text": "Precision and Recall\nWe’ll start with Precision first. Precision is the ratio of true positives to total positive guesses. In other words, it is a metric that describes how many positive classifications that we were actually right about, relative to how many positives that we said there were. Mathematically:\n\\[\\mathrm{Precision} = \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}}\\]\nWe can calulate this manually using our confusion matrix:\n\ncm[1, 1] / (cm[0, 1] + cm[1, 1])\n\n0.88125\n\n\nOr take it from scikit-learn itself:\n\nfrom sklearn.metrics import precision_score\n\nprecision_score(y_train, y_train_pred)\n\n0.88125\n\n\nWe can also just verify this mathematically:\n\\[\\frac{141}{141 + 19} = \\frac{141}{160} = 0.88125\\]\nThis means that out of all tumors that our model said were malignant, roughly \\(88\\%\\) were actually malignant.\nThe other main metric is Recall. Recall is the ratio of true positives to total positives. In other words, it is a metric that describes how many positive classifications that we were right about, relative to how many positives there actually were. Mathematically:\n\\[\\mathrm{Recall} = \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}}\\]\nWe can calulate this manually using our confusion matrix:\n\ncm[1, 1] / (cm[1, 0] + cm[1, 1])\n\n0.815028901734104\n\n\nOr use scikit-learn’s built-in functions:\n\nfrom sklearn.metrics import recall_score\n\nrecall_score(y_train, y_train_pred)\n\n0.815028901734104\n\n\nAnd of course we can verify this mathematically again:\n\\[\\frac{141}{141 + 32} = \\frac{141}{173} = 0.815028901734104\\]\nThis tells us that our classifier only classified roughly \\(81.5\\%\\) of all malignant tumors correctly.\nTo see the difference between these two metrics, let’s imagine that these images of malignant tumors are known criminals, and we’re guards on the look out. Our precision and recall tell us that our classifier let nearly \\(18.5\\%\\) (\\(1 - \\mathrm{Recall} = 100\\% - 81.5\\%\\)) of all criminals sneak by, and out of all the people we stopped, around \\(12\\%\\) (\\(1 - \\mathrm{Precision} = 100\\% - 88\\%\\)) of them were innocent civilians!\nAs it turns out, for any model that has some error, the precision and recall values are inextricably linked. Looking back at our “civilian vs. criminal” example, let’s think about how we might try to classify civilians from criminals. Assuming we could see all the individuals at once, we may use some identifiable features or behaviors to set a “score” to each person. A person with a higher “score” is more likely to be a criminal, and a lower “score” means that person is probably a civilian. Once we’ve assigned each person a score, we need to set a threshold value above which we will assume all people are criminals, and below which we will assume all people are civilians.\nOnce we have a score for each person and all of those scores are placed in order along a horizontal number line, we can set a vertical dividing line with all scores on the right side containing scores belonging to criminals and the left side containing scores belonging to civilians. Now, assuming our model is imperfect, we will have some civilians on the right side of the line (false positives) and some criminals on the left side of the line (false negatives). If we shift our threshold to the right, this stops classifying so many civilians as criminals, but it also stops classifying so many criminals as criminals. In other words, the number of false positives will go down, but the number of false negatives will go up. Looking at our equations for precision and recall again:\n\\[\\mathrm{Precision} = \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}}\\]\n\\[\\mathrm{Recall} = \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}}\\]\nWe can see that the decrease of false positives will cause precision to increase, but the increase in false negatives will cause the recall to decrease. Theoretically, if we moved the threshold all the way to the right so that no one’s score is high enough to be classified as a criminal, we would theoretically have a precision of \\(100\\%\\)! This is great on paper, but it is only occurring because we’re not classying anyone as a criminal, so we have no room to fail. In other words, “you miss 100% of the shots you don’t take”.\nConversely, if we move the threshold to the left, our number of classified criminals will increase, but we will also be classifying more civilians as criminals. In other words, the number of false negatives will go down, but the number of false positives will go up. We can see from the equations above that this will result in the decrease of precision and the increase of recall. Again, if the threshold reaches the very edge, and we classify all people as criminals, then our recall looks good on paper (\\(100\\%\\)), but only because we’re accusing everyone of being a criminal. In other words, “you will hit 100% of your shots, if you take every shot possible”.\nThis conundrum is known as the Precision-Recall Tradeoff. We can visualize the changing of the precision and recall with the shifting of the threshold using a graph:\n\nfrom sklearn.metrics import precision_recall_curve\n\nthreshold = 0\ny_scores = cross_val_predict(clf, X_train, y_train, cv=3,\n                             method=\"decision_function\")\nprecisions, recalls, thresholds = precision_recall_curve(y_train, y_scores)\n\nplt.figure(figsize=(8, 4))\nplt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\nplt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\nplt.vlines(threshold, 0, 1.0, \"k\", \"dotted\", label=\"threshold\")\n\nidx = (thresholds &gt;= threshold).argmax()  # first index ≥ threshold\nplt.plot(thresholds[idx], precisions[idx], \"bo\")\nplt.plot(thresholds[idx], recalls[idx], \"go\")\nplt.axis([min(y_scores), max(y_scores), -0.01, 1.01])\nplt.grid()\nplt.xlabel(\"Threshold\")\nplt.legend(loc=\"center right\")\nplt.show()\n\n\n\n\n\n\n\n\nIt is up to us as the model designers to create a model that has the properties we desire. In the case of breast cancer classification, is it better to have higher precision, higher recall, or a mix of both? Well, a false negative could be the difference between life and death, whereas a false positive is not as serious. Therefore, it is in our best interest to lower the amount of false negatives as much as possible, even if there’s an increase in false positives. This, in turn, will raise our recall and lower our precision. This means we need to lower our threshold.\nLooking at the left side of the above figure, we can see that the recall settles at \\(1\\) for quite a while, and the precision settles at roughly \\(0.4\\). We could choose a threshold as far to the left as possible, but we can see that shifting it to the right will slightly raise our precision without affecting our recall much. Let’s use a threshold of \\(-5\\) and see what we get:\n\nthreshold = -5\nidx = (thresholds &gt;= threshold).argmax()\n\nprint(\"Recall:\", 100 * recalls[idx], \"%\")\nprint(\"Precision:\", round(100 * precisions[idx], 2), \"%\")\n\nRecall: 100.0 %\nPrecision: 42.2 %\n\n\nSure enough, we find every malignant tumor, but with the drawback that we also accidentally classify nearly \\(60\\%\\) of benign tumors as malignant as well.\nWhen you think about it, this isn’t too bad. Usually, tumors of any size would be cause for concern, and most people would safely operate under the assumption that their tumor is malignant and get it removed as soon as possible. This is the same as shifting the threshold all the way to the left and classifying everyone as a criminal. In this situation, we can actually say that our model has improved things substantially by still catching all malignant tumors, but also allowing more than \\(40\\%\\) of people to not have to get an operation.\n\nThis post will eventually be updated to include:\n\nOther metrics: \\(F_1\\) Score, PR Curves, ROC curves, etc.\nMulticlass Classification\nOther classification methods: Naïve Bayes, K-Nearest Neighbors, Decision Trees, Support Vector Machines, etc."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/Regression/index.html",
    "href": "posts/Regression/index.html",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "If you’re reading this, you should have already finished the first blog post. Now, with Bayes’ Rule in your toolbox, you are ready to tackle a different set of problems.\nThis post will focus much more on actual data and machine learning concepts. Let’s get started!\n\n\nThe Dataset\nThis time we’ll start by introducing the dataset. This dataset contains information about different cars, such as their miles-per-gallon, number of cylinders, weight, and so on. It was downloaded from Kaggle and is simple enough to let us glide through the following concepts with ease. Let’s take a look:\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndf = pd.read_csv('auto-mpg.csv', encoding='windows-1252')\ndf.head()\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nmodel year\norigin\ncar name\n\n\n\n\n0\n18.0\n8\n307.0\n130\n3504\n12.0\n70\n1\nchevrolet chevelle malibu\n\n\n1\n15.0\n8\n350.0\n165\n3693\n11.5\n70\n1\nbuick skylark 320\n\n\n2\n18.0\n8\n318.0\n150\n3436\n11.0\n70\n1\nplymouth satellite\n\n\n3\n16.0\n8\n304.0\n150\n3433\n12.0\n70\n1\namc rebel sst\n\n\n4\n17.0\n8\n302.0\n140\n3449\n10.5\n70\n1\nford torino\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 398 entries, 0 to 397\nData columns (total 9 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   mpg           398 non-null    float64\n 1   cylinders     398 non-null    int64  \n 2   displacement  398 non-null    float64\n 3   horsepower    397 non-null    object \n 4   weight        398 non-null    int64  \n 5   acceleration  398 non-null    float64\n 6   model year    398 non-null    int64  \n 7   origin        398 non-null    int64  \n 8   car name      398 non-null    object \ndtypes: float64(3), int64(4), object(2)\nmemory usage: 28.1+ KB\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nweight\nacceleration\nmodel year\norigin\n\n\n\n\ncount\n398.000000\n398.000000\n398.000000\n398.000000\n398.000000\n398.000000\n398.000000\n\n\nmean\n23.514573\n5.454774\n193.425879\n2970.424623\n15.568090\n76.010050\n1.572864\n\n\nstd\n7.815984\n1.701004\n104.269838\n846.841774\n2.757689\n3.697627\n0.802055\n\n\nmin\n9.000000\n3.000000\n68.000000\n1613.000000\n8.000000\n70.000000\n1.000000\n\n\n25%\n17.500000\n4.000000\n104.250000\n2223.750000\n13.825000\n73.000000\n1.000000\n\n\n50%\n23.000000\n4.000000\n148.500000\n2803.500000\n15.500000\n76.000000\n1.000000\n\n\n75%\n29.000000\n8.000000\n262.000000\n3608.000000\n17.175000\n79.000000\n2.000000\n\n\nmax\n46.600000\n8.000000\n455.000000\n5140.000000\n24.800000\n82.000000\n3.000000\n\n\n\n\n\n\n\nThat covers the basic information again, but this time we’ll do something even cooler. Let’s display a histogram of each of the parameters:\n\ndf.hist(bins=50, figsize=(12, 8))\nplt.show()\n\n\n\n\n\n\n\n\nThis histogram shows the different values for each feature that every data point has. For example, we can see that a large portion of the cars have between \\(15\\) to \\(40\\) miles-per-gallon. This is cool and all, but for this blog post, we’re going to be more concerned with relationships between features. Let’s try something new:\n\nfrom pandas.plotting import scatter_matrix\n\nattributes = [\"mpg\", \"displacement\", \"weight\", \"acceleration\"]\nscatter_matrix(df[attributes], figsize=(12, 8))\nplt.show()\n\n\n\n\n\n\n\n\nNow this is cool, right? Each plot corresponds to a relationship between two features. For example, the bottom-left plot shows how miles-per-gallon might affect acceleration. From looking at that graph, we can see that there’s not much of a relationship. It’s somewhat of a big mess. But now look up and to the right one plot to see the relationship between weight and displacement. There’s clearly a strong relationship, and we are going to attempt to find it.\nLooking at the main diagonal, the plots look completely different. This is because they have the same \\(x\\) and \\(y\\) axes, so their plots really wouldn’t give us any important information. Therefore, the pandas library knows to replace these plots with histograms, although we’ve already seen them from the previous figure.\nLet’s now shift our focus to the graph of weight and displacement:\n\nscatter = df.plot.scatter(x='displacement', y='weight', alpha=0.5)\n\n\n\n\n\n\n\n\nAt this point, we’d like to find some sort of way to model this data. Before we do this, however, we need to look at some math.\n\n\n\nLinear Regression\nRelationships between two variables occur very frequently. For simplicity sake, we will assume that there is one independent variable (\\(x\\)) that affects the output variable (\\(y\\)), therefore called the dependent variable. Mathematically, we write:\n\\[y = f(x),\\]\nwhere \\(f\\) is a function that changes the input in some way to produce the output.\nWe can also use subscripts to represent having more than one input affecting the output, like so:\n\\[y = f(x_0, x_1, x_2, \\ldots)\\]\nEven with a large amount of total variables, determining the relationship between independent variables and dependent variables is as simple as finding \\(f\\). Or so you would think.\nWhen it comes to real-world relationships, there are very few cases where things are this simple. For example, think about a company that is using commercials to try to increase sales of their product. It would be very nice if doubling the amount of advertisement playing at any given time on television caused twice as many purchases, but that is simply not what happens. A small increase in ads may lead to a similar, deterministic increase in sales, but at a certain point people might get fed up with the ads and stop purchasing the product entirely.\nThis observation is even more exaggerated at the individual level. If you assume that all humans behave the same way, then you could easily reason that a change in the output variable will be consistent across all people, but this is also not true. Human beings are individuals, with their own thoughts and opinions and irrationality. At some point, someone may stop purchasing your product just because they don’t feel like it anymore.\nTo remedy this, we assume that there is still a function that can describe the relationship between our variables, but we also add on an error term:\n\\[Y = f(X) + \\epsilon\\]\nThis error term accounts for the other variables that are too complex to account for. It is true that if we knew every single variable, then we wouldn’t need the error term. But alas, this is the way things are.\nIn creating our model, we will try to find an approximate function that will give an approximate output for some input. In theory, this model will have reduced the error as much as possible:\n\\[\\hat{Y} = \\hat{f}(X)\\]\nThe little “hats” on top indicate that these are approximations.\nTo create our model, we need to train it on the available data, usually called the training set. If we could train forever, we would remove all of the error within our model, known as the reducible error. If our approximate function correctly represents the actual relationship between data, then we can come very close to this. However, since each data point is affected by variables that we can’t keep track of, there is some irreducible error that will always exist.\nSo, we have some understanding of the theory, but how do we do it in practice? There are quite a few methods, but the simplest is Linear Regression. Linear Regression essentially makes the assumption that the input(s) has a linear contribution to the output. In other words, doubling the input doubles the output, halving the input halves the output, and so on. A line has a very simple equation:\n\\[y = mx + b,\\]\nwhere \\(m\\) is the slope, which signifies the increase or decrease in output for a change in input, and b represents how vertically shifted the values are from the start. Let’s write it in a more appropriate form:\n\\[\\hat{Y} = \\hat{\\beta}_1X + \\hat{\\beta}_0\\]\nI’ve changed the \\(m\\) and \\(b\\) constants and used subscripts, but the idea is the same. We can also extend this idea to multiple independent variables:\n\\[\\hat{Y} = \\hat{\\beta}_0 + \\sum_{i = 1}^N \\hat{\\beta}_iX_i\\]\nThe hats on these constants also indicate that they are approximations of the true constants (if the data actually has a linear relationship).\nSo the next question is, how do we find the constants? This is where things begin to get a little complicated.\nComputers do this by minimizing some value that corresponds to the error of our model. There are a few good choices, but the most common is known as the Mean Squared Error (MSE):\n\\[\\mathrm{MSE} = \\frac{1}{N}\\sum_{i = 1}^N (Y_i - \\hat{f}(X_i))^2\\]\nThe inside of the parantheses show the difference between the actual measured output value of the data and the currne iteration our model’s output value for the same input. This difference is then squared. After getting this squared difference for every data point, they’re all summed up, and divided by the total number of data points.\nFor simple Linear Regression (one input variable), formulas for these constants can be found directly. First, we plug in the linear equation for \\(\\hat{f}(X)\\):\n\\[\\begin{equation}\n\\begin{split}\n\\frac{1}{N}\\sum_{i = 1}^N (Y_i - \\hat{f}(X_i))^2 &= \\frac{1}{N}\\sum_{i = 1}^N (Y_i - \\hat{\\beta}_1X_i - \\hat{\\beta}_0)^2\\\\\n&= \\frac{1}{N}\\sum_{i = 1}^N (Y_i^2 - 2\\hat{\\beta}_1X_iY_i - 2\\hat{\\beta}_0Y_i + \\hat{\\beta}_1^2X_i^2 + 2\\hat{\\beta}_0\\hat{\\beta}_1X_i + \\hat{\\beta}_0^2)\n\\end{split}\n\\end{equation}\\]\nPretty nasty. Now, we’d like to minimize this to find the coefficient values. Let’s imagine that the error for each coefficient looks something like this:\n\n\n\n\n\n\n\n\n\nOur error shouldn’t have a maximum, since our line could always be worse, but it should definitely have a minimum, so we know that this graph is somewhat representative of how the error will change as we change the coefficients. To mimimize this error, we need to find the coefficient values that will bring the error down into the very bottom of the valley. We can do this by taking the partial derivative with respect to the coefficient values. This is because the derivative represents the rate of change of a function with respect to a changing independent variable. If we find this derivative, and set it equal to zero, we should have reached an inflection point on the graph, since the derivative must switch between negative and positive values to flip the direction of the function.\nHere is a visual representation of the derivative of this curve:\n\n\n\n\n\n\n\n\n\nNotice how the line that represents the derivative of the curve passes through \\(y = 0\\) right at \\(x = 3\\), the exact same spot where the parabolic curve flips directions. Therefore, to minimize the error, we can take the derivative of our error with respect to each of the variables, set them equal to zero, and solve for the coefficients.\nWe’ll start with the easier one (\\(\\hat{\\beta}_0\\)) first:\n\\[\\frac{\\partial}{\\partial \\hat{\\beta}_0}\\mathrm{MSE} = \\frac{1}{N}\\sum_{i = 1}^N (-2Y_i + 2\\hat{\\beta}_1X_i + 2\\hat{\\beta}_0)\\]\nNow that we have the partial derivative, we can set it equal to zero to find the minimum:\n\\[\\newcommand{\\olsi}[1]{\\,\\overline{\\!{#1}}}\\]\n\\[\\begin{equation}\n\\begin{split}\n\\frac{1}{N}\\sum_{i = 1}^N (-2Y_i + 2\\hat{\\beta}_1X_i + 2\\hat{\\beta}_0) &= 0\\\\\n-\\frac{2}{N}\\sum_{i = 1}^NY_i + \\frac{2\\hat{\\beta}_1}{N}\\sum_{i = 1}^NX_i + \\frac{2\\hat{\\beta}_0}{N}\\sum_{i = 1}^N 1 &= 0\\\\\n-\\olsi{Y} + \\hat{\\beta}_1\\olsi{X} + \\hat{\\beta}_0 &= 0\n\\end{split}\n\\end{equation}\\]\nRearranging:\n\\[\\hat{\\beta}_0 = \\olsi{Y} - \\hat{\\beta}_1\\olsi{X},\\]\nwhere \\(\\olsi{X}\\) and \\(\\olsi{Y}\\) are the averages of \\(X\\) and \\(Y\\), respectively.\nNow, let’s do the same thing for \\(\\hat{\\beta}_1\\). Taking the partial derivative:\n\\[\\frac{\\partial}{\\partial \\hat{\\beta}_1}\\mathrm{MSE} = \\frac{1}{N}\\sum_{i = 1}^N (-2X_iY_i + 2\\hat{\\beta}_1X_i^2 + 2\\hat{\\beta}_0X_i)\\]\nAnd setting it equal to zero:\n\\[\\begin{equation}\n\\begin{split}\n\\frac{1}{N}\\sum_{i = 1}^N (-2X_iY_i + 2\\hat{\\beta}_1X_i^2 + 2\\hat{\\beta}_0X_i) &= 0\\\\\n-\\frac{2}{N}\\sum_{i = 1}^N X_iY_i + \\frac{2\\hat{\\beta}_1}{N}\\sum_{i = 1}^NX_i^2 + \\frac{2\\hat{\\beta}_0}{N}\\sum_{i = 1}^NX_i &= 0\\\\\n\\end{split}\n\\end{equation}\\]\nSubstituting our new equation in for \\(\\hat{\\beta}_1\\):\n\\[\\begin{equation}\n\\begin{split}\n-\\frac{2}{N}\\sum_{i = 1}^N X_iY_i + \\frac{2\\hat{\\beta}_1}{N}\\sum_{i = 1}^NX_i^2 + \\frac{2}{N}(\\olsi{Y} - \\hat{\\beta}_1\\olsi{X})\\sum_{i = 1}^NX_i &= 0\\\\\n-\\frac{2}{N}\\sum_{i = 1}^N X_iY_i + \\frac{2\\hat{\\beta}_1}{N}\\sum_{i = 1}^NX_i^2 + \\frac{2}{N}\\sum_{i = 1}^NX_i\\olsi{Y} - \\frac{2\\hat{\\beta}_1}{N}\\sum_{i = 1}^NX_i\\olsi{X} &= 0\\\\\n\\frac{2}{N}\\left(\\sum_{i = 1}^NX_i\\olsi{Y} - \\sum_{i = 1}^N X_iY_i\\right) + \\frac{2\\hat{\\beta}_1}{N}\\left(\\sum_{i = 1}^NX_i^2 - \\sum_{i = 1}^NX_i\\olsi{X}\\right) &= 0\\\\\n\\sum_{i = 1}^N(X_i\\olsi{Y} - X_iY_i) + \\hat{\\beta}_1\\sum_{i = 1}^N(X_i^2 - X_i\\olsi{X}) &= 0\\\\\n-\\sum_{i = 1}^NX_i(Y_i - \\olsi{Y}) + \\hat{\\beta}_1\\sum_{i = 1}^NX_i(X_i - \\olsi{X}) &= 0\n\\end{split}\n\\end{equation}\\]\nRearranging:\n\\[\\begin{equation}\n\\begin{split}\n\\hat{\\beta}_1 &= \\frac{\\sum\\limits_{i = 1}^NX_i(Y_i - \\olsi{Y})}{\\sum\\limits_{i = 1}^NX_i(X_i - \\olsi{X})}\n\\end{split}\n\\end{equation}\\]\nAt this point, we have an equation for \\(\\hat{\\beta}_1\\), but this may not be the form you’ve seen it in before. In order to get to that form, we need to do a unique manipulation. First, we need to note the following relationship:\n\\[\\begin{equation}\n\\begin{split}\n\\sum\\limits_{i = 1}^N (X_i - \\olsi{X}) &= \\left( X_0 -  \\frac{X_0 + X_1 + \\ldots}{N} \\right) + \\left( X_1 -  \\frac{X_0 + X_1 + \\ldots}{N} \\right) + \\ldots\\\\\n&= \\left( X_0 + X_1 + \\ldots \\right) - N\\left( \\frac{X_0 + X_1 + \\ldots}{N} \\right) = 0\n\\end{split}\n\\end{equation}\\]\nThe same exact process can be applied with \\(Y\\) and \\(\\olsi{Y}\\). So, our two new equations are:\n\\[\\begin{equation}\n\\begin{split}\n\\sum\\limits_{i = 1}^N (X_i - \\olsi{X}) &= 0\\\\\n\\sum\\limits_{i = 1}^N (Y_i - \\olsi{Y}) &= 0\n\\end{split}\n\\end{equation}\\]\nSince the left side is equal to zero, we can multiply it by whatever constant we’d like and the relationship would still be maintained. We will multiply both equations by \\(\\olsi{X}\\):\n\\[\\begin{equation}\n\\begin{split}\n\\olsi{X}\\sum\\limits_{i = 1}^N (X_i - \\olsi{X}) &= \\sum\\limits_{i = 1}^N \\olsi{X}(X_i - \\olsi{X}) = 0\\\\\n\\olsi{X}\\sum\\limits_{i = 1}^N (Y_i - \\olsi{Y}) &= \\sum\\limits_{i = 1}^N \\olsi{X}(Y_i - \\olsi{Y}) = 0\n\\end{split}\n\\end{equation}\\]\nThese equations will be used to perform some algebraic wizardry. Since both of these equations are equal to zero, we can subtract them from the numerator and denominator of our equation for \\(\\hat{\\beta}_1\\) without changing anything:\n\\[\\begin{equation}\n\\begin{split}\n\\hat{\\beta}_1 = \\frac{\\sum\\limits_{i = 1}^NX_i(Y_i - \\olsi{Y})}{\\sum\\limits_{i = 1}^NX_i(X_i - \\olsi{X})} &= \\frac{\\sum\\limits_{i = 1}^NX_i(Y_i - \\olsi{Y}) - \\sum\\limits_{i = 1}^N \\olsi{X}(Y_i - \\olsi{Y})}{\\sum\\limits_{i = 1}^NX_i(X_i - \\olsi{X}) - \\sum\\limits_{i = 1}^N \\olsi{X}(X_i - \\olsi{X})}\\\\\n&= \\frac{\\sum\\limits_{i = 1}^N\\left[X_i(Y_i - \\olsi{Y}) - \\olsi{X}(Y_i - \\olsi{Y})\\right]}{\\sum\\limits_{i = 1}^N\\left[X_i(X_i - \\olsi{X}) - \\olsi{X}(X_i - \\olsi{X})\\right]}\\\\\n&= \\frac{\\sum\\limits_{i = 1}^N\\left[(X_i - \\olsi{X})(Y_i - \\olsi{Y})\\right]}{\\sum\\limits_{i = 1}^N\\left[(X_i - \\olsi{X})(X_i - \\olsi{X})\\right]}\\\\\n&= \\frac{\\sum\\limits_{i = 1}^N(X_i - \\olsi{X})(Y_i - \\olsi{Y})}{\\sum\\limits_{i = 1}^N(X_i - \\olsi{X})^2}\n\\end{split}\n\\end{equation}\\]\nFinally! We got a (somewhat) simple looking equation for our second coefficient. There is another representation, but it is more complicated and doesn’t provide many benefits over this one.\nLet’s recap:\n\\[\\hat{\\beta}_0 = \\olsi{Y} - \\hat{\\beta}_1\\olsi{X},\\]\n\\[\\hat{\\beta}_1 = \\frac{\\sum\\limits_{i = 1}^N(X_i - \\olsi{X})(Y_i - \\olsi{Y})}{\\sum\\limits_{i = 1}^N(X_i - \\olsi{X})^2},\\]\nwhere\n\\[\\olsi{X} = \\frac{1}{N}\\sum\\limits_{i = 1}^N X_i\\]\n\\[\\mathrm{and}\\]\n\\[\\olsi{Y} = \\frac{1}{N}\\sum\\limits_{i = 1}^N Y_i\\]\nNow that we have our equations, let’s try applying them.\n\nimport numpy as np\n\nnp.random.seed(42)\nN = 100\nX = 2 * np.random.rand(m, 1)\nY = 4 + 3 * X + np.random.randn(m, 1)\n\nX = [element[0] for element in X]\nY = [element[0] for element in Y]\n\nplt.figure(figsize=(6, 4))\nplt.plot(X, Y, \"b.\")\nplt.xlabel(\"$X$\")\nplt.ylabel(\"$Y$\", rotation=0)\nplt.axis([0, 2, 0, 15])\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nThis is fake data that was generated with the sole purpose of using our new equations to fit a line to it. Let’s try it out!\n\nproductSum = 0\nsquareSum = 0\n\nX_Mean = sum(X) / N\nY_Mean = sum(Y) / N\n\nfor i in range(0, N):\n    productSum += (X[i] - X_Mean) * (Y[i] - Y_Mean)\n    squareSum += (X[i] - X_Mean) * (X[i] - X_Mean)\n    \nb1 = productSum / squareSum\nb0 = Y_Mean - (b1 * X_Mean)\n\nprint(b0)\nprint(b1)\n\n4.215096157546749\n2.770113386438484\n\n\nThese are close to the coefficients that were used to generate the randomized data in the first place (\\(4\\) and \\(3\\)), so we know we’re on the right track. Let’s plot it to see what the graph looks like:\n\nxAxis = np.asarray(range(0, 100))\nyAxis = (xAxis * b1) + b0\n\nplt.figure(figsize=(6, 4))\nplt.plot(X, Y, \"b.\")\nplt.plot(xAxis, yAxis, \"g\")\nplt.xlabel(\"$X$\")\nplt.ylabel(\"$Y$\", rotation=0)\nplt.axis([0, 2, 0, 15])\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nNice! We see that the line described by our coefficients seems to pass as close to all the points as it can.\nYou may wonder, “why did we try to minimize the Mean Squared Error?” Why not just the difference between the true value and estimated value? Well, some true values may be above our estimates, and some may be below. A worst-case scenario would occur when this occurs over and over, and all of the positive values cancel with the negative values, giving us no information. The squaring process removes all negatives.\nYour next question may be, “then why not use the absolute value instead of squaring?” And that is a very good point. In fact, this can be done. The reason it typically isn’t is that the Mean Squared Error has some nice properties with certain distributions and have a deeper connection to the data in general. However, the absolute value error is used enough to have its own name: the L-1 Norm. The square root of our Mean Squared Error, (also called the Root Mean Square (RMS)) is called the L-2 Norm. There are technically an infinite amount of other norms, but the L-1 and L-2 norms are the most common ones used in practice.\nThe L-1 Norm is preferred when there are a few outliers in the data. This is because the Mean Squared Error exacerbates the error that outliers have, since everything is squared. Let’s take a look at how much this affects our line by adding a couple outliers:\n\nrandomIndices = [i for (i, _) in enumerate(X) if X[i] &gt; 1.9]\n\nY[randomIndices[0]] = 0.5\nY[randomIndices[1]] = 0.25\n\nproductSum = 0\nsquareSum = 0\n\nX_Mean = sum(X) / N\nY_Mean = sum(Y) / N\n\nfor i in range(0, N):\n    productSum += (X[i] - X_Mean) * (Y[i] - Y_Mean)\n    squareSum += (X[i] - X_Mean) * (X[i] - X_Mean)\n    \nnewB1 = productSum / squareSum\nnewB0 = Y_Mean - (b1 * X_Mean)\n\nnewYAxis = (xAxis * newB1) + newB0\n\nplt.figure(figsize=(6, 4))\nplt.plot(X, Y, \"b.\")\nplt.plot(xAxis, yAxis, \"g\")\nplt.plot(xAxis, newYAxis, \"r\")\nplt.xlabel(\"$X$\")\nplt.ylabel(\"$Y$\", rotation=0)\nplt.axis([0, 2, 0, 15])\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nAs you can see, it took only two outliers to move our new line (red) a substantial amount. In this case, it might be better to use the L-1 norm.\nLet’s try to fit a line to our dataset this time. We’ll use scikit-learn’s LinearRegression class to do it for us:\n\nfrom sklearn.linear_model import LinearRegression\n\ndisp = np.asarray(df['displacement']).reshape(-1, 1)\nweight = np.asarray(df['weight']).reshape(-1, 1)\n\nlin_reg = LinearRegression()\nlin_reg.fit(disp, weight)\nlin_reg.intercept_, lin_reg.coef_\n\n(array([1505.01873083]), array([[7.57605909]]))\n\n\n\nxAxis = range(int(min(disp)), int(max(disp)))\nyAxis = (xAxis * lin_reg.coef_[0]) + lin_reg.intercept_[0]\n\nplt.scatter(df['displacement'], df['weight'], alpha=0.5)\nplt.plot(xAxis, yAxis, \"r\")\nplt.xlabel('displacement')\nplt.ylabel('weight')\nplt.show()\n\n\n\n\nLooks pretty solid! This model can now be used to estimate the weight of a car just by knowing the displacement. Let’s use a specific car as an example and measure how close our prediction was. We will use the Lexus LS 400, which has a \\(\\sim250~\\mathrm{in}^3\\) displacement and a curb weight of \\(\\sim3700~\\mathrm{lbs}\\):\n\ndispTest = 250\nweightTest = (lin_reg.coef_ * dispTest) + lin_reg.intercept_\nweightTest[0][0]\n\n3399.0335038967237\n\n\nNot too bad! With an error of about \\(\\frac{3700 - 3400}{3700} \\approx 8.1\\%\\), we’re doing pretty well, especially considering that our dataset doesn’t contain any Lexus cars:\nOf course, the variation of the weight for each displacement is due to other variables affecting the weight. In this case, if you want to determine how multiple variables are affecting the output, you can use Multiple Linear Regression. Since there is a coefficient for each independent variable, finding the formulas directly is incredibly difficult, and only gets harder as more variables are added. Of course, this is why we invented computers; to do our dirty work for us.\nThis section will be expanded in the future to include Linear Regression using multiple features.\n\n\n\nNonlinear Regression\nLet’s look at another plot from our data:\n\nplt.scatter(df['mpg'], df['displacement'], alpha=0.5)\nplt.xlabel('mpg')\nplt.ylabel('displacement')\nplt.show()\n\n\n\n\nWe can see that fitting a line to this would be pretty difficult. We might have some accurrate approximations for miles-per-gallon values between \\(15\\) and \\(25\\), but the data seems to level out after this point. In this case, we need to find a non-linear model. Luckily, scikit-learn has our back:\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndisp = np.asarray(df['displacement']).reshape(-1, 1)\nmpg = np.asarray(df['mpg']).reshape(-1, 1)\n\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly_features.fit_transform(mpg)\n\nlin_reg = LinearRegression()\nlin_reg.fit(X_poly, disp)\nlin_reg.intercept_, lin_reg.coef_\n\nX_new = np.linspace(10, 45, 100).reshape(100, 1)\nX_new_poly = poly_features.transform(X_new)\ny_new = lin_reg.predict(X_new_poly)\n\nplt.figure(figsize=(6, 4))\nplt.plot(mpg, disp, \"b.\", alpha=0.5)\nplt.plot(X_new, y_new, \"r-\", linewidth=2)\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\", rotation=0)\nplt.grid()\nplt.show()\n\n\n\n\nNot too bad! We shaped this using a polynomial of degree \\(2\\), otherwise known as a parabola. Would it become more accurate if we increased the degree value? Let’s try:\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndisp = np.asarray(df['displacement']).reshape(-1, 1)\nmpg = np.asarray(df['mpg']).reshape(-1, 1)\n\npoly_features = PolynomialFeatures(degree=3, include_bias=False)\nX_poly = poly_features.fit_transform(mpg)\n\nlin_reg = LinearRegression()\nlin_reg.fit(X_poly, disp)\nlin_reg.intercept_, lin_reg.coef_\n\nX_new = np.linspace(10, 45, 100).reshape(100, 1)\nX_new_poly = poly_features.transform(X_new)\ny_new = lin_reg.predict(X_new_poly)\n\nplt.figure(figsize=(6, 4))\nplt.plot(mpg, disp, \"b.\", alpha=0.5)\nplt.plot(X_new, y_new, \"r-\", linewidth=2)\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\", rotation=0)\nplt.grid()\nplt.show()\n\n\n\n\nChanging the degree to \\(3\\) definitely seems to have lowered the curve at the end, which is nice! But why can’t we just keep increasing the degree? Well, as you increase the flexibility that the model has, you begin to do something called overfitting. In simple terms, your model is trying to match too closely to the data in your training set. This makes your training set error look really low, which appears to be a good thing, but the problem is that any predictions made will be less accurate. Essentially, the model is finding patterns that aren’t really there, and is trying to match them as closely as possible. Another way of saying the same thing is to say that the model has high variance.\nUnderfitting occurs when the model doesn’t fit the training data particulary well, but generalizes to new data well (only because it’s so simple). You can also say that this model has high bias.\nIn machine learning, bias and variance are inescapable. Raising your bias will lower your variance and result in underfitting. Raising your variance will lower your bias and result in overfitting. This is known as the bias-variance tradeoff. The question of finding a good model is the same as asking for a model that minimizes both bias and variance as much as possible."
  }
]